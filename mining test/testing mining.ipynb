{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ded043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "part1= open(\"miccai 2021 part 6 page 3 of 4.doc\")\n",
    "\n",
    "\n",
    "#opening the html document (copy pasted and saved as a .doc file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d421cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(part1, 'html.parser')\n",
    "#parsing the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4232301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#playing around with getting the titles, authors and doi's - this function returns all the parts of the html with the doi\n",
    "#which also contains the title\n",
    "\n",
    "def has_doi(href):\n",
    "    return href and re.compile(\"chapter/\").search(href)\n",
    "\n",
    "list_of_doi = soup.find_all(href=has_doi)\n",
    "\n",
    "#line I wanted to find:\n",
    " #<a data-track=\"click\" data-track-action=\"ToC link to content page\" data-track-label=\"link\" href=\"/chapter/10.1007/978-3-642-33415-3_1\">Reliable Assessment of Perfusivity and Diffusivity from Diffusion Imaging of the Body</a>,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8de1c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the titles and the doi's from above function\n",
    "\n",
    "titles = []\n",
    "doi_str = []\n",
    "\n",
    "for element in list_of_doi:\n",
    "    titles.append(element.get_text())\n",
    "    string = str(element)\n",
    "    first_substring = '/chapter'\n",
    "    second_substring ='\">'\n",
    "    doi_str.append(string[(string.find(first_substring)):string.find(second_substring)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b815faff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reliable Assessment of Perfusivity and Diffusivity from Diffusion Imaging of the Body',\n",
       " 'Multi-organ Abdominal CT Segmentation Using Hierarchically Weighted Subject-Specific Atlases',\n",
       " 'Radiation-Free Drill Guidance in Interlocking of Intramedullary Nails',\n",
       " 'Developing Essential Rigid-Flexible Outer Sheath to Enable Novel Multi-piercing Surgery',\n",
       " 'Surgical Gesture Classification from Video Data',\n",
       " 'Remote Ultrasound Palpation for Robotic Interventions Using Absolute Elastography',\n",
       " 'Modeling and Real-Time Simulation of a Vascularized Liver Tissue',\n",
       " 'Efficient Optic Cup Detection from Intra-image Learning with Retinal Structure Priors',\n",
       " 'Population-Based Design of Mandibular Plates Based on Bone Quality and Morphology',\n",
       " 'Thoracic Abnormality Detection with Data Adaptive Structure Estimation',\n",
       " 'Domain Transfer Learning for MCI Conversion Prediction',\n",
       " 'Simulation of Pneumoperitoneum for Laparoscopic Surgery Planning',\n",
       " 'Incremental Kernel Ridge Regression for the Prediction of Soft Tissue Deformations',\n",
       " 'Fuzzy Multi-class Statistical Modeling for Efficient Total Lesion Metabolic Activity Estimation from Realistic PET Images',\n",
       " 'Structure and Context in Prostatic Gland Segmentation and Classification',\n",
       " 'Quantitative Characterization of Trabecular Bone Micro-architecture Using Tensor Scale and Multi-Detector CT Imaging',\n",
       " 'Genetic, Structural and Functional Imaging Biomarkers for Early Detection of Conversion from MCI to AD',\n",
       " 'Robust MR Spine Detection Using Hierarchical Learning and Local Articulated Model',\n",
       " 'Spatiotemporal Reconstruction of the Breathing Function']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a054d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now want to find authors - YES!\n",
    "authors = []\n",
    "\n",
    "authors = soup.find_all(\"li\", class_=\"c-author-list__item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8d048d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only the author names\n",
    "authors_str = []\n",
    "for element in authors:\n",
    "    string = str(element)\n",
    "    first_substring = 'item\">'\n",
    "    second_substring ='</li>'\n",
    "    authors_str.append(string[(string.find(first_substring)+6):string.find(second_substring)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2c7a2082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M. Freiman, S. D. Voss, R. V. Mulkern, J. M. Perez-Rossello, M. J. Callahan, Simon K. Warfield',\n",
       " 'Robin Wolz, Chengwen Chu, Kazunari Misawa, Kensaku Mori, Daniel Rueckert',\n",
       " 'Benoit Diotte, Pascal Fallavollita, Lejing Wang, Simon Weidert, Peter-Helmut Thaller, Ekkehard Euler et al.',\n",
       " 'Siyang Zuo, Takeshi Ohdaira, Kenta Kuwana, Yoshihiro Nagao, Satoshi Ieiri, Makoto Hashizume et al.',\n",
       " 'BenjamÃ\\xadn BÃ©jar Haro, Luca Zappella, RenÃ© Vidal',\n",
       " 'Caitlin Schneider, Ali Baghani, Robert Rohling, Septimiu Salcudean',\n",
       " 'Igor PeterlÃ\\xadk, Christian Duriez, StÃ©phane Cotin',\n",
       " 'Yanwu Xu, Jiang Liu, Stephen Lin, Dong Xu, Carol Y. Cheung, Tin Aung et al.',\n",
       " 'Habib Bousleiman, Christof Seiler, Tateyuki Iizuka, Lutz-Peter Nolte, Mauricio Reyes',\n",
       " 'Yang Song, Weidong Cai, Yun Zhou, Dagan Feng',\n",
       " 'Bo Cheng, Daoqiang Zhang, Dinggang Shen',\n",
       " 'J. Bano, A. Hostettler, S. A. Nicolau, S. Cotin, C. Doignon, H. S. Wu et al.',\n",
       " 'Binbin Pan, James J. Xia, Peng Yuan, Jaime Gateno, Horace H. S. Ip, Qizhen He et al.',\n",
       " 'Jose George, Kathleen Vunckx, Elke Van de Casteele, Sabine Tejpar, Christophe M. Deroose, Johan Nuyts et al.',\n",
       " 'Kien Nguyen, Anindya Sarkar, Anil K. Jain',\n",
       " 'Yinxiao Liu, Punam K. Saha, Ziyue Xu',\n",
       " 'Nikhil Singh, Angela Y. Wang, Preethi Sankaranarayanan, P. Thomas Fletcher, Sarang Joshi',\n",
       " 'Yiqiang Zhan, Dewan Maneesh, Martin Harder, Xiang Sean Zhou',\n",
       " 'D. Duong, D. Shastri, P. Tsiamyrtzis, I. Pavlidis']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a40262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next up is page numbers\n",
    "#line to find:\n",
    "#<div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 1-9</span>\n",
    "\n",
    "page_numbers = []\n",
    "\n",
    "page_numbers= soup.find_all('div', class_ = \"c-meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63bf0741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['402-411',\n",
       " '412-420',\n",
       " '421-429',\n",
       " '430-440',\n",
       " '441-450',\n",
       " '451-460',\n",
       " '461-470',\n",
       " '471-481',\n",
       " '485-494',\n",
       " '495-505',\n",
       " '506-515',\n",
       " '516-525',\n",
       " '526-536',\n",
       " '537-546',\n",
       " '547-556',\n",
       " '557-566',\n",
       " '567-576',\n",
       " '579-589']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_numbers_str = []\n",
    "\n",
    "for element in page_numbers:\n",
    "    string = element.get_text()[6:-1] #removes the white spaces and \"Pages \"\n",
    "    both = string.split(\"-\")\n",
    "    if int(both[1])-int(both[0]) > 1: #checks that the article spans more than one page\n",
    "        page_numbers_str.append(string)\n",
    "\n",
    "page_numbers_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec723d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <h3 class=\"c-card__title\" data-test=\"back-matter\">Back Matter</h3>\n",
    "            \n",
    "           # <div class=\"c-meta\"><span class=\"c-meta__item u-display-inline-block\" data-test=\"page-number\">Pages 623-626</span>\n",
    "back_matter = []\n",
    "\n",
    "\n",
    "back_matter = soup.find_all(\">Back Matter<\")\n",
    "\n",
    "back_matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbbb492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pages 402-411',\n",
       " 'Pages 412-420',\n",
       " 'Pages 421-429',\n",
       " 'Pages 430-440',\n",
       " 'Pages 441-450',\n",
       " 'Pages 451-460',\n",
       " 'Pages 461-470',\n",
       " 'Pages 471-481',\n",
       " 'Pages 483-483',\n",
       " 'Pages 485-494',\n",
       " 'Pages 495-505',\n",
       " 'Pages 506-515',\n",
       " 'Pages 516-525',\n",
       " 'Pages 526-536',\n",
       " 'Pages 537-546',\n",
       " 'Pages 547-556',\n",
       " 'Pages 557-566',\n",
       " 'Pages 567-576',\n",
       " 'Pages 577-577',\n",
       " 'Pages 579-589']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#strips it back to simply the pages\n",
    "page_numbers_str = []\n",
    "\n",
    "for element in page_numbers:\n",
    "    string = str(element)\n",
    "    first_substring = 'Pages'\n",
    "    second_substring ='</span>'\n",
    "    string = string[(string.find(first_substring)):string.find(second_substring)]\n",
    "    if len(string) > 0: #ensures I don't add an empty string to the list\n",
    "        page_numbers_str.append(string)\n",
    "page_numbers_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d40fca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "#a check that I've found the same amount of authors, titles, dois and page numbers\n",
    "if len(page_numbers_str) == len(authors_str) == len(titles) == len(doi_str):\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"You have an error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7b1480d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now want to combine everything in a pandas dataframe\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b20c7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to add the year (probably a better way to do this, but should be fine)\n",
    "year_of_pub = []\n",
    "for element in titles:\n",
    "    year_of_pub.append(2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4bd717c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {'Title': titles,\n",
    "        'Authors': authors_str,\n",
    "        'Page numbers' : page_numbers_str,\n",
    "        'DOI': doi_str,\n",
    "        'Year of publication' : year_of_pub\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('miccai_2021_part1.csv')\n",
    "#not sure why it seems to put \"\" around only the authors in the csv file, but good enough to start! \n",
    "#Now want to combine this into a more pretty script that I can then feed all the pages into it, will need to do some \n",
    "#combining at the end, adding everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing back matters in 2021 version\n",
    "#first need to create a hard coded dictionary of the pages in question\n",
    "\n",
    "back_matter_dic = {}\n",
    "\n",
    "\n",
    "#part 1: 743-746\n",
    "#part 2: 659-662\n",
    "#part 3: 645-648\n",
    "#part 4: 679-682\n",
    "#part 5: 835-839\n",
    "#part 6: 623-626\n",
    "#part 7: 797-801\n",
    "#part 8: 701-704\n",
    "\n",
    "#these are all at the end - so if the 2012 also has it, could fix it by simply removing the last entry!\n",
    "#or doing a check for year and then removing the last entry for 2021 if not\n",
    "# 2012 has a back matter, but there are no page numbers shown on the website, so that's why it \n",
    "#misses the issue, so will do the latter version!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
