Paper id (increasing array from 0);What is the article's page numbers in the conference paper?;What is the article's title?;Who are the authors? (Last name only);Which year is the article from? (Conference publication year);Is the article about classification (n/reasoning or y/quote);Which part of body or disease is the article about? (own words);What is the aim/task of the article? (quote);What is the justification for this task? (quote);How many citations does the article have?;When are the citations from? (range, oldest to newest);List of citations, years published;Which method is used for classification? (quote);Which performance measures are used? (quote);Does the article use segmentation as preprocessing? (n/reasoning or y/quote);Does the used dataset have a title? (n/reasoning or y/quote);What is the size of the dataset (input quote);What type is the dataset, public, private, other? (quote/reasoning);Is the survey/method of how the dataset was obtained accessible? (no/reasoning or y/quote);Does the dataset have any mention of demographics of patients/images included? (n/reasoning or y/quote);Does the article mention the intent for collecting the dataset, the task intended? (n/reasoning or y/quote);Does the article disclose any affiliations? (n/reasoning or y/quote);(Menlo ethics 1/4) Does the article include anything about respect for persons (informed consent, voluntary participation) participating in the dataset? (n/reasoning or y/quote);(Menlo ethics 2/4) Does the article have any mention of benefience, minimising risk/maximising benefit of work? (n/reasoning or y/quote);(Menlo ethics 3/4) Does the article have any mention of justice (equal treatment, fair selection of subjects)? (n/reasoning or y/quote) ;(Menlo ethics 4/4) Does the article mention any respect for law/public interest (transparency in methods/results, accountability for actions)? (n/reasoning or y/quote);Are there any other comments/interesting aspects?
0;34-41;Surgical Gesture Classification from Video Data;Haro, Zappella, Vidal;2012;"y/""we propose and evaluate three approaches to surgical gesture classification from video.""";Surgical gestures;!!!;"""Most of the prior work on surgical gesture recognition (see, e.g., [4-6]) uses hidden Markov models (HMMs) to analyze kinematic data stored by the robot (…) Overall, our main conclusion is that methods based on video data perform equally well as methods based n kinematic data for a typical surgical training setup.""";26;1999-2012;2002, 2001, 2008, 2005, 2009, 2011, 2009, 2006, 2012, 2010, 2012, 2011, 2003, 2009, 2002, 2000, 1999, 2005, 1008, 2009, 2009, 2009, 2001;"""In the first one, we model each video clip from each surgical gesture as the output of a linear dynamical system (LDS) and use metrics in the space of LDSs to classify new video clips. In the second one, we use spatio-temporal features extracted from each video clip to learn a dictionary of spatio-temporal words and use a bag-of-features (BoF) approach to classify new video clips. In the third approach, we use multiple kernel learning to combine the LDS and BoF approaches.""";"""The performance is measured as the percentage of correctly identified surgemes averaged over all tests and repetitions for each setup""";"n/There is no mention of segmentation, but the article does write: ""We assume that each video is segmented into video surgemes"", so the segmentation has been done prior to the work in this article. Later they write: ""the data was manually segmented based on the surgeme's definition of [3].""";"y/""For our tests we used the California dataset [3].""";"""The dataset consists of three different tasks: suturing (SU, 39 trials), needle passing (NP, 26 trials) and knot tying (KT, 36 trials). Each task is performed by 8 surgeons with different skill levels.""";"Private/There is no mention of a public dataset, and the article writes ""The authors thank Intuitive Surgical and Carol Reiley for providing the dataset""";n/Upon searching for the dataset nothing has been found, other than the article referenced;"n/no mention of demographics, though the article does specify that ""Each task is performed by 8 surgeons with different skill levels""";n/the article contains no mention of intent of data collection;"y/""This work was funded by NSF grants 0931805 and 0941362, and by the Talentia Fellowships Programme of the Andalusian Regional Ministry of Economy, Innovation and Science""";n/ the article contains no information about the surgeons in the dataset or the patients they were presumably operating on;n/the focus of the article is to promote a new method, mode of analysis, and does not mention any risk or benefit other than the usefulness of this new method;n/there is no mention of this;n/the article has no mention of this;n
1;74-81;Thoracic Abnormality Detection with Data Adaptive Structure Estimation;Song, Cai, Zhou, Feng;2012;y;Lungs;Detecting lung tumors and abnormal lymph nodes ;Improving lung cancer staging, especially complex cases - also created a method that they say can be generalised to other areas;!!;;;binary linear-kernel SVM (but describe a whole lot more prior to this);They have a section called structure estimation, which may be this, but not sure!;n, I think?;clinical PET-CT datasets from lung cancer patients from Royal Prince Alfred Hospital, Sydney;50 sets of images;not mentioned, but think private?;n;;not mentioned;No mention;n;;;;
2;115-123;Structure and Context in Prostatic Gland Segmentation and Classification;Nguyen, Sarkar, Jain;2012;y;Prostate;Classifying prostate cancer;A novel gland segmentation and classification scheme, better than previou;!!;;;linear SVM;10 fold cross validation, report the average classification accuracy;y;no title;48 images;not mentioned;n;;not mentioned;No mention;n;;;;
3;132-140;Genetic, Structural and Functional Imaging Biomarkers for Early Detection of Conversion from MCI to AD;Singh, Wang, Sankaranarayanan, Fletcher and Joshi;2012;y;Alzheimers;Predicting conversion of individuals with Mild Cognitive Impairment (MCI) to Alzheimers Disease (AD);building on technological advancement, filling a 'need' for a unified framework to aid diagnosis of AD;!!;;;partial least squares? ;test subjects were later evaluated manually for AD, results were measured against these results;y;images from Alzheimers Disease Neuroimaging Initiaitve database;242 individuals;seems public, or at least accessible to researchers (but website is not currently reachable);;;;Funded by several NSF and NIH grants and the Alzheimer's Disease Neuroimaging Initiaitve;n;;;;
4;141-148;Robust MR Spine Detection Using Hierarchial Learning and Local Articulated Model;Zhan, Maneesh, Harder, Zhou;2012;y;Spine;Creating a clinically acceptable auto-spine detection system, localizing and labeling vertebrae and discs;A robust, novel method for 3D - building on previous models that work better in 2D;!!;;;hierarchial learning and local articulated model;Results were evaluated and labeled by radiologists as perfect, acceptable and rejected;n;no title;"405 images were used from ""different clinical sites""";not mentioned, but think private?;n;;not mentioned;No mention;n;;;;
5;165-172;Detectioni of Spontaneous Vesicle Release at Individual Synapses Using Multiple Wavelets in a CWT-Based Algorithm;Sokoll, Tönnies, Heine;2012;?;Brain;Detection of spontaneous activity at individual synapses in microscopy images;Neurobiologists find this useful;!!;;;continual multiple wavelets based algorithm;ground truth eval (which failed) and against real data and synthetic data;;no title;rat neurons were used, 123 synapse signals from four images;not mentioned, no organised dataset it would appear;n;;not mentioned;No mention;n;;;;This is the most convoluted article so far, understood very little, and seemed like their method wasn't exactly good?
6;206-213;Classification of Ambigious Nerve Fiber Orientations in 3D Polarized Light Imaging;Kleiner, Axer, Grässel, Reckfort, Pietrzyk, Amunts, Dickscheid;2012;y;Brain;Mapping fiber tracts in the brain (connections between nerve cells?) and the sign of the fiber inclination angle;Improving accuracy, building on current work;!!;;;binary labeling problem solved by Markov random field theory;evaluation section I do not understand;n;no title;2 used, one synthetic, created by the researchers, the other was from three post mortem brains;not mentioned, no organised dataset it would appear;n;;not mentioned;No mention;n;;;;
7;298-306;Oriented Pattern Analysis for Streak Detection in Dermoscopy Images;Sadeghi, Lee, McLean, Lui, Atkins;2012;y;Skin (cancer);Classifying dermoscoy images into streaks Absent or Present;To meet demand for computer aided early diagnosis of deadly melanoma;!!;;;Laplacian of Gaussian finds linear structures, then Averaged Squared Gradient Flow algorithm determins the orientation ;ten fold cross validation;y;no title;300 dermoscopy images, 250 randomly chosen from 2 atlases and 50 from experts' archives with permission;so partially private and public, I think the ones from the atlas could be found (there is a link to a physical book/cdrom);n;;not mentioned;Funded by Canadian NSERC, CIHR-skin reseach training center and a grant from the Canadian Health Reseach Project;n;;;;
8;315-322;Intrinsic Melanin and Hemoglobin Colour Components for Skin Lesion Malignancy Detection;Madooei, Drew, Sadeghi, Atkins;2012;y;Skin (cancer);Malignant vs benign skin lesion classification;an extension of previous approaches;!!;;;Logistic classifier;10 fold cross validation, f-measure, AUC, precision, recall;y (I think its preprocessing at least);no title;500 images are mentioned, a long with a dataset from a different article;not mentioned;n;;not mentioned;No mention;n;;;;
9;331-339;Apoptosis Detection for Adherent Cell Populations in Time-Lapse Phase-Constrast Microscopy images;Huh, Ker, Su, Kanade;2012;y;Myoblastic stem cells;Detecting apoptosis, programmed cell death;critical for furthering understanding of biology, normal tissue development and disease progression;!!;;;linear Support Vector Machine (SVM);precision, recall;n;no title;3 cell populations, 540 images;not mentioned;n;;not mentioned;No mention;n;;;;Actually includes pseudocode for proposed algorithm
10;348-356;Learning to Detect Cells Using Non-overlapping Extremal Regions;Artea, Lempitsky, Noble, Zisserman;2012;y;Cell detection across different modalities;Cell detection in microscopy images;achieving state of the art performance, and being non specific to modality;!!;;;SVM;based on position of region centroids, precision, recall, f1 score;n, but technically the method does segmentation as classification;N/A;3 different datasets, ICPR 2010 Histopathology Image Contest and two without specifics;not mentioned;n;;not mentioned, and still 3 different ones;Acknowledgements to 4 people who provided cell datasets. Financial support from the RCUK Centre for Doctoral Training in Healthcare and an additonal ERC grant;n;;;;
11;373-380;Detecting and Tracking Motion of Myxococcus xanthus Bacteria in Swarms;Liu, Harvey, Wang, Alber, Cehn;2012;n?;Bacteria on agar plates;Detecting and tracking motion of bacteria;to shed light on collective motion of cells;!!;;;bipartite graph matching scheme;;y;no title;4 images sequences with 384 cells;created by the researchers, so private?;N/A;;to do this study;Grants by NSF and NIH (acknowleged on the first page, a first so far!);n;;;;
12;504-511;Combining CRF and Multi-hypothesis Detection for Accurate Lesion Segmentation in Breast Sonogras;Hao, Wang, Seong, Lee, Ren, Kim;2012;y - but no, see other comments;Breast (cancer);Lesion segmentation for breast ultrasound images;;!!;;;;;;;;;;;;;;;;;Would appear to be using classification as a step in making segmentation better, so not segmentation as a preprosessing step, but detection and segmentation in one
13;609-616;Automatic Detection and Classification of Teeth in CT Data;Duy, Lamecker, Kainmuller, Zachow;2012;y;Dentistry, teeth;Tooth detection and classification (existing teeth vs missing teeth);to improve accuracy of drill guides ;!!;;;histogram analysis and svm;10 cross fold validation;y;no title;43 clinical CT datasets;private I guess?;n;;not mentioned;No mention;n;;;;
14;58-65;Efficient Optic Cup Detection from Intra-image Learining with Retinal Structure Priors;Xu, Liu, Lin, Xu, Cheung, Aung, Wong;2012;y;Eyes;Glaucoma detection (optic nerve degeneration);To facilitate widespread testing in a more accessible manner;!!;;;linear SVM;non overlap ratio, relative absolute area difference and absolute CDR error;y;ORIGA light;168 glaucoma and 482 normal images;public, available upon request;possible upon request;;not mentioned;Funded by Singapore A*STAR SERC Grant, mentioned in title;n;;;;
15;82-90;Domain Transfer Learning for MCI Conversion Prediction;Cheng, Zhang, Shen;2012;y;Alzheimers;Identifying mild cognitive impairment (MCI) converters from MCI non-converters;Early diagnosis improves possible delay of disease;!!;;;transfer learning method, Domain Transfer Support Vector Machine;10 cross fold validation, AUC, accuracy, sensitivity, specificity;y;images from Alzheimers Disease Neuroimaging Initiaitve database;202 subjects;seems public, or at least accessible to researchers, based on previous research, but this article doesn't include a reference or link;;;not mentioned;Partially funded by severeal NIH grants, a NSFC grant and a CQKJ ;n;;;;Mentions the lack of MCI subjects available for training as an issue (size of dataset not previously brought up in any of the other articles as an issue)
16;99-106;Incremental Kernel Ridge Regression for the Prediction of Soft Tissue Deformations;Pan, Xia, Yuan, Gateno, Ip, He, Lee, Chow, Zhou;2012;?;Head, deformities to head and facial appearance;Predicting patients facial deformation (following a surgery I think);soft tissue change simulation still a challenge, that this paper seeks to meet/solve;!!;;;incremental kernel ridge regression model;leave-one-out cross validation;y;no title;11 patients;private I guess?;n;;dataset generated for this article as far as I understand;Partially funded by TMHRI scholar award and a NIH/NIDCR grant;n;;;;Not sure this is classification!
17;3-12;DeepStationing: Thoracic Lymph Node Station Parsing in CT Scans Using Anatomical Context Encoding and Key Organ Auto-Search;Dazhou Guo  , Xianghua Ye , Jia Ge, Xing Di, Le Lu, Lingyun Huang, Guotong Xie, Jing Xiao, Zhongjie Lu, Ling Peng, Senxiang Yan, Dakai Jin;2021;"n/ ""better searched-organ delineation can help get superior LNS segmentation performance.""";Chest, lymph nodes;..;"""Assessment of involved LNs [1, 21] and accurate labeling their corresponding stations are essential for the treatment selection and planning. """;21;1992-2021;2002, 2005, 2012, 2020, 2017, 2020, 2019, 2019, 2020, 2018, 2016, 2011, 2014, 1992, 2009, 2014, 2021,2019, 2019, 2020, 2020;"""We adopt the nnU-Net [6] with DSC+CE losses as our backbone for all experiments due to its high accuracy on many medical image segmentation tasks.""";"""We report the segmentation performance using DSC in percentage, Hausdor distance (HD) and average surface distance (ASD) in mm.""";;;;;;;;;;;;;Found it hard to figure out if it was segmentation or if the parsing of organs meant it was classification. Landed on the former though due to their concluding remarks
18;13-22;Hepatocellular Carcinoma Segmentation from Digital Subtraction Angiography Videos Using Learnable Temporal Difference;Wenting Jiang, Yicheng Jiang, Lu Zhang, Changmiao Wang, Xiaoguang Han, Shuixing Zhang, Xiang Wan, Shuguang Cui;2021;"n/"" In this paper, we raise the problem of HCC segmentation in DSA videos, and build our own DSA dataset. We also propose a novel segmentation network called DSA-LTDNet, including a segmentation sub-network, a temporal difference learning (TDL) module and a liver region segmentation (LRS) sub-network for providing additional guidance. """;;..;;;;;;;;;;;;;;;;;;;
19;23-32;CA-Net: Leveraging Contextual Features for Lung Cancer Prediction;Mingzhou Liu, Fandong Zhang, Xinwei Sun, Yizhou Yu, Yizhou Wang;2021;"y/""we propose a Context Attention Network (CA-Net) which extracts both nodule’s and contextual features and then effectively fuses them during malignancy/benignity classification.""";Lungs (cancer);..;"""Early diagnosis of lung cancer, as commonly adopted clinically to improve survival rate, contains an important step of predicting malignancy/benignity for each nodule.""";22;1991-2020;2017, 2017, 2017, 2016, 2014, 2016, 2018, 2020, 2019, 2020, 2018, 2007, 2020, 2004, 2017, 2016, 2017, 2019, 2009, 2017, 2017, 1991;"""To leverage these contextual features into lung cancer diagnosis, we propose a novel Context Attention Network (CA-Net), which extracts both nodule’s and contextual features and effectively fuses them during nodule malignancy classification.""";"""We adopt the following metrics for evaluation: the Accuracy (Acc) under the threshold of 0.5, the Area Under the Curve (AUC) and the Log Loss""";"n/ ""The whole pipeline of our method, namely Context Attention Network (CA-Net), is illustrated in Fig. 2. As shown, it is the sequential of three stages: (i) Nodule Detection that detects all nodules from the CT image;"" - so nodule detection not segmentation";"y/ ""we evaluate it on the Data Science Bowl (DSB) 2017, a dataset for lung cancer prediction in Kaggle’s competition """;"""There are 1397, 198, and 506 patients in the training, validation, and test set, respectively.""";"other/was public, but following the reference to the kaggle site it says: ""due to data set usage restrictions, the data for this competition is no longer available for download.""";"n/On kaggle they write: ""The images in this dataset come from many sources and will vary in quality. """;n/only a patient id is included;n/ there is no mention of the reason for the dataset collected in the article;"y/""This work was supported by MOST-2018AAA0102004, NSFC-61625201, and the Beijing Municipal Science and Technology Planning Project (Grant No. Z201100005620008)""";n/no mention of patients from dataset;"n/only mention is how this solution outperforms others, so in that sense is better: ""Experimentally, our CA-Net outperforms the 1st place by a noticeable margin on Kaggle DSB2017 dataset.""";n/ no mention;n/ no mention;
20;33-42;Semi-supervised Learning for Bone Mineral Density Estimation in Hip X-Ray Images;Kang Zheng, Yirui Wang, Xiao-Yun Zhou, Fakai Wang, Le Lu, Chihung Lin, Lingyun Huang, Guotong Xie, Jing Xiao, Chang-Fu Kuo, Shun Mia;2021;"n/""we formulate the BMD estimation from plain hip X-ray images as a regression problem""";;..;;;;;;;;;;;;;;;;;;;
21;43-52;DAE-GCN: Identifying Disease-Related Features for Disease Prediction;Churan Wang, Xinwei Sun, Fandong Zhang, Yizhou Yu, Yizhou Wang;2021;"y/""Specifically, we explicitly separate the encoded features into disease-related features and others.""";Mammography, but general method that can be used for other forms of cancer;..;"""Learning disease-related representations plays a critical role in image-based cancer diagnosis, due to its trustworthy, interpretable and good generalization power. """;14;1996-2020;2013, 1996, 2018, 2019, 2020, 2020, 2019, 2016, 2018, 2017, 2019, 2016, 2013, 2012;"""Graph Convolutional Network Learning""";"""Area Under the Curve (AUC) is used as evaluation metrics in image-wise.""";n/ the article does not mention segmention as a step;"n/ the article uses 4 datasets: ""We consider both the public dataset DDSM [2] and three in-house datasets: Inhouse1, Inhouse2 and Inhouse3""";The article does not mention any size;other/ 3 private, 1 public;"n/ I can find the public dataset and they write http://www.eng.usf.edu/cvprg/Mammography/Database.html  ""The primary purpose of the database is to facilitate sound research in the development of computer algorithms to aid in screening. Secondary purposes of the database may include the development of algorithms to aid in the diagnosis and the development of teaching or training aids.""";y/ age of patients with each case, and the site contains a breakdown of the race of the women included (percentage overview);n/no mention of this;"y/""This work was supported by MOST-2018AAA0102004, NSFC-61625201 and ZheJiang Province Key Research & Development Program (No. 2020C03073).""";n/no mention;n/ no mention;n/no mention;n/no mention;The fact that no size is mentioned at all is odd - I can look up the size of the public set…
22;53-63;Enhanced Breast Lesion Classification via Knowledge Guided Cross-Modal and Semantic Data Augmentation;Kun Chen, Yuanfan Guo, Canqian Yang, Yi Xu, Rui Zhang, Chunxiao Li et al.;2021;"y/""Further experiments prove that our framework is able to synthesize virtual US and SWE images with US images only, significantly enhancing the breast lesion classification network based on conventional data augmentation methods and outperforming conventional GANs without knowledge guidance.""";Breast (cancer);..;"""Considering the shortage of radiologists and SWE devices in rural hospitals, an automated deep learning based breast lesion classification system has great potential to help treatment planning for radiologists with US only. However, without a sufficient number of annotated US-SWE image pairs, deep learning based classification networks are highly likely to suffer from the over-fitting problem. """;29;2003-2021;2015, 2012, 2021, 2010, 2020, 2014, 2019, 2020, 2016, 2015, 2017, 2020, 2020, 2015, 2016, 2019, 2019, 2008, 2019, 2015, 2003, 2006, 2017, 2021, 2019, 2004, 2016, 2018, 2017;"""The framework mainly consists of: (1) two baseline classification networks $$C_U$$ and $$C_D$$ pre-trained on US images and US-SWE image pairs respectively to provide knowledge guidance for semantic inverter and modal translater, (2) a knowledge distillation [10] guided modal translater $$G_M$$ to synthesize SWE images based on US images, (3) a semantic inverter $$G_S$$ to synthesize US images with opposite category and (4) a student classification network $$C_S$$ conditioned on real and virtual US images and corresponding virtual SWE images, which serves as our final breast lesion classification network.""";"""AUC, accuracy, recall, precision, and F1-score""";"n/ it mentions ""conventional data augmentation (translation, flipping, scaling and shearing) is applied, then the images are resized to $$256 \times 256$$ pixels before fed into models."" but not segmentation";"n/the article mentions collecting the data themselves: ""From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. """;"""a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. """;private/there is no mention of the dataset being publically available;;"n/the article contains no mention, though does descibe the equipment used: ""The Super Linear SL-15-4 probe of ultrafast ultrasound device Aixplorer (Super Sonic Imagine, Aix-en-Provence, France) was used for imaging data collection. """;y/ for this article;"y /""This work was supported in part by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), 111 project (BP0719010), Shanghai Science and Technology Committee (18DZ2270700) and Shanghai Jiao Tong University Science and Technology Innovation Special Fund (ZH2018ZDA17).""";n/no mention;no/no mention;no/no mention;n/no mention;
23;64-74;Multiple Meta-model Quantifying for Medical Visual Question Answering;Tuong Do, Binh X. Nguyen, Erman Tjiputra, Minh Tran, Quang D. Tran, Anh Nguyen;2021;n? /is about meta learning - is this classification;;..;;;;;;;;;;;;;;;;;;;Is meta learning classification? Think not
24;75-84;mfTrans-Net: Quantitative Measurement of Hepatocellular Carcinoma via Multi-Function Transformer Regression Network;Jianfeng Zhao, Xiaojiao Xiao, Dengwang Li, Jaron Chong, Zahra Kassam, Bo Chen et al.;2021;"n/""In this study, we propose a multi-function Transformer regression network (mfTrans-Net) for HCC quantitative measurement.""";;..;;;;;;;;;;;;;;;;;;;
25;85-95;You only Learn Once: Universal Anatomical Landmark Detection;Heqin Zhu, Qingsong Yao, Li Xiao, S. Kevin Zhou;2021;"n/""we investigate the idea of “You Only Learn Once (YOLO)” and develop a universal anatomical landmark detection model to realize multiple landmark detection tasks with end-to-end training based on mixed datasets. """;;..;;;;;;;;;;;;;;;;;;;Is landmark detection classification? Think not
26;96-106;A Coherent Cooperative Learning Framework Based on Transfer Learning for Unsupervised Cross-Domain Classification;Xinxin Shan, Ying Wen, Qingli Li, Yue Lu, Haibin Cai;2021;"y/"" we propose a coherent cooperative learning framework based on transfer learning for unsupervised cross-domain classification. """;Pneumonia;..;""" current methods take knowledge transfer model and classification model as two separate training stages, which inadequately considers and utilizes the intrinsic information interaction between modules. """;35;2010-2020;2019, 2020, 2017, 2020, 2020, 2016, 2020, 2017, 2014, 2017, 2016, 2020, 2010, 2018, 2019, 2019, 2020, 2018, 2020, 2015, 2020, 2020, 2018, 2020, 2017, 2020, 2020, 2020, 2014, 2016, 2020, 2019, 2019, 2017;"""a coherent cooperative learning (CCL) framework based on transfer learning for unsupervised cross-domain classification, which is constructed by a proposed Wasserstein CycleGAN (WCycleGAN) for image translation and two classifiers for prediction. First, by training the WCycleGAN with the original images from the source domain and the target domain, we obtain a class-balanced dataset used to fine-tune two classifiers that are convolutional neural networks (CNNs) pre-trained on ImageNet. """;"""accuracy, precision, recall and F1 score.""";n/ there is no mention of segmentation;"y/but simply calls the databases by names: ""Table 1.The number of images and examples of (a) Chest X-Ray database; (b) Single lesion database; (c) Multiple lesions database. Their data distributions vary due to different imaging equipment/standards.""";Size is found in table 1, combined 51553 images;other/one is definitely public, on kaggle, one opens a chinese(?) website, and looks like there is a price, the final is private I believe;n/nothing is described in the article, kaggle also doesn't describe anything;n/nothing in the article, and nothing in the kaggle ;n/no mention of this;"y/""This work was supported in part by 2030 National Key Research and Development Program of China (2018AAA0100500), the National Nature Science Foundation of China (no. 61773166), Projects of International Cooperation of Shanghai Municipal Science and Technology Committee (14DZ2260800), the Fundamental Research Funds for the Central Universities, and the ECNU Academic Innovation Promotion Program for Excellent Doctoral Students (YBNLTS2021-040).""";n/no mention;n/no mention;n/no mention;n/no mention;
27;107-116;Towards a Non-invasive Diagnosis of Portal Hypertension Based on an Eulerian CFD Model with Diffuse Boundary Conditions;Lixin Ren, Shang Wan, Yi Wei, Xiaowei He, Bin Song, Enhua Wu;2021;"n/ ""In this paper we propose an Eulerian computational fluid dynamics (CFD) model to facilitate hemodynamics analysis. To enable consistent simulation results with different boundary conditions, a diffuse boundary handling technique was proposed to impose smooth boundary conditions for both the pressure and velocity fields. We also propose a computational workflow for quantifying patient-specific hemodynamics in portal vein systems non-invasively. """;;..;;;;;;;;;;;;;;;;;;;
28;117-127;A Segmentation-Assisted Model for Universal Lesion Detection with Partial Labels;Fei Lyu, Baoyao Yang, Andy J. Ma, Pong C. Yuen;2021;"y?/""we propose a novel segmentation-assisted model, where an additional semantic segmentation branch with superpixel-guided selective loss is introduced to assist the conventional detection branch. The segmentation branch and the detection branch help each other to find unlabeled lesions with a mutual-mining strategy, and then the mined suspicious lesions are ignored for fine-tuning to reduce their negative impacts.""";universal lesion detector;..;"""Developing a Universal Lesion Detector (ULD) that can detect various types of lesions from the whole body is of great importance for early diagnosis and timely treatment. Recently, deep neural networks have been applied for the ULD task, and existing methods assume that all the training samples are well-annotated. However, the partial label problem is unavoidable when curating large-scale datasets, where only a part of instances are annotated in each image. """;22;2009-2020;2019, 2018, 2019, 2020, 2018, 2019, 2019, 2020, 2019, 2019, 2017, 2019, 2019, 2020, 2020, 2017, 2018, 2009, 2019, 2019, 2020, 2012;"""The detection branch is based on Mask R-CNN [7], which is shown in red. We employ the SLIC algorithm [1] to produce superpixels, and add an additional branch with superpixel-guided selective loss to predict the pixel-wise segmentation map for the whole image, which is highlighted in green. These two branches can generate two different sets of lesion detection results. After going through the unlabeled lesion mutual-mining module, the mined suspicious lesions from one branch are fed back to the other branch for fine-tuning.""";""" They also introduce a new pseudo 3D (P3D) evaluation metric, which can serve a better measurement of 3D detection performance than current practices. The detected 2D bounding boxes are first stacked to generate 3D bounding boxes, and a predicted 3D bounding box is regarded as correct if any 2D bounding box within the 3D bounding box volume has an intersection over union (IoU) with a ground-truth box which is larger than 0.5. We study sensitivities at operating points varying from false positive (FP) rates of 0.125 to 8 per volume and the average precision for evaluating the detection performance.""";"y/""In this paper, we propose a segmentation-assisted model to address the partial label problem in the universal lesion detection task. Inspired by the advances in Co-teaching [6], we propose to add a peer network for effectively mining unlabeled lesions, and reducing their negative impacts on model training. Specifically, we add an additional semantic segmentation branch as the peer network, which can leverage the spatial context and provide complementary information for the detection task [3, 10]. To handle the segmentation task with partial labels, we propose a superpixel-guided selective loss where only the pixels within the selective masks contribute to the loss and the selective masks are generated based on superpixels [1].""";"y/ ""DeepLesion""";"""Cai et al. [2] completely annotate and publicly release 1915 of the DeepLesion subvolumes. Of these, 844 subvolumes are selected from the original training set for supporting lesion harvesting. The other 1,071 subvolumes are selected from the original testing set for evaluation""";public/ released by Cai et al. ;n/ there is no mention of anything;n/ the article mentions nothing;n/no mention of this;"y/""This work was supported by the Health and Medical Research Fund Project under Grant 07180216.""";n/no mention;n/no mention;n/no mention;n/no mention;Not sure if this is actually classification? They are finding unlabeled lesions and then relabeling them as far as I understand, but does that then qualify?
29;128-140;Constrained Contrastive Distribution Learning for Unsupervised Anomaly Detection and Localisation in Medical Images;Yu Tian, Guansong Pang, Fengbei Liu, Yuanhong Chen, Seon Ho Shin, Johan W. Verjans et al.;2021;;;;;;;;;;;;;;;;;;;;;;
30;141-152;Conditional Training with Bounding Map for Universal Lesion Detection;Han Li, Long Chen, Hu Han, Ying Chi, S. Kevin Zhou;2021;;;;;;;;;;;;;;;;;;;;;;
31;153-162;Focusing on Clinically Interpretable Features: Selective Attention Regularization for Liver Biopsy Image Classification;Chong Yin, Siqi Liu, Rui Shao, Pong C. Yuen;2021;;;;;;;;;;;;;;;;;;;;;;
32;163-173;Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification;Xiaohan Xing, Yuenan Hou, Hang Li, Yixuan Yuan, Hongsheng Li, Max Q.-H. Meng;2021;;;;;;;;;;;;;;;;;;;;;;
33;174-184;Tensor-Based Multi-index Representation Learning for Major Depression Disorder Detection with Resting-State fMRI;Dongren Yao, Erkun Yang, Hao Guan, Jing Sui, Zhizhong Zhang, Mingxia Liu;2021;;;;;;;;;;;;;;;;;;;;;;
34;185-194;Region Ensemble Network for MCI Conversion Prediction with a Relation Regularized Loss;Yuan-Xing Zhao, Yan-Ming Zhang, Ming Song, Cheng-Lin Liu;2021;;;;;;;;;;;;;;;;;;;;;;
