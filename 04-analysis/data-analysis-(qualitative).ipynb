{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ebd9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0704e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in file\n",
    "df = pd.read_csv('annotations_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ab8cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering down to true articles from 2012 or 2021\n",
    "df_true = df[df['Is the article accurately labelled as classification?'] == 'Yes']\n",
    "df = df_true[df_true['Which year is the article from?'] == 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb2e95f",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c4cd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles mentioning demographics:  13\n",
      "173\n",
      "The UK Biobank [27] offers a large-scale and multi-modal repository of health-related data from the UK. From this, we obtained records of over 84, 000 subjects with 174, 465 fundus images from both eyes and multiple visits per participant. Male and female subjects constituted $$46\\%$$ and $$54\\%$$ of the data, respectively. As a substantial fraction of the images were not gradable due to image quality issues (artefacts, high contrast, or oversaturation), we used the EyeQual networks [6] to filter out poor images. 47, 939 images ($$47\\%$$ male, $$53\\%$$ female) passed the quality check by the EyeQual ensemble. We partitioned them into the training, validation and test sets with $$75\\%$$, $$10\\%$$ and $$15\\%$$ of subjects, respectively, making sure that all images from each subject were allocated to the same set.\n",
      "\n",
      "Additionally, we obtained 29 fundus images from patients (11 male, 18 female, all older than 47 years) at the University Eye Hospital with permission of the Institutional Ethics Board. We used these additional images as an independent test set.\n",
      "\n",
      "Gender - but as this is the binary classification they are testing this is necessary!\n",
      "---\n",
      "321\n",
      "by the diagnosis time of EC, marriage, gender, and race (and more which they do not specify)\n",
      "The clinical dataset was collected using a standardized questionnaire, including general, diagnostic, treatment, and hematological data. There are 7 numerical and 27 categorical variables. Categorical variables were further processed to one-hot encoding matrices for each patient (n = 74). \n",
      "---\n",
      "315\n",
      "There are 20 non-imaging clinical parameters following into four categories. The first type is general information of age (62.69 ± 7.37) and gender (83.12% Male). The second type includes histories of smoking (60.00% Yes), drinking (57.79% Yes), hypertension (24.6% Yes), diabetes (7.8% Yes), heart disease (5.79% Yes), cerebral infarction (2.26% Yes), and gastritis (17.18% Yes). Third type pathologic features include preoperative tumor markers of CEA (3.06 ± 1.97), NSE (13.94 ± 4.45), CA19–9 (13.27 ± 9.82), CA125 (11.90 ± 5.92), CA72–4 (2.63 ± 4.45), CA15–3 (15.64 ± 8.66), Cyfra21–1 (3.26 ± 2.20), and SCC (1.55 ± 1.03). Last category is hematological parameters of neutrophil count (4.29 ± 2.00), lymphocyte count (1.83 ± 0.67), and platelet count (252.90 ± 67.48).\n",
      "---\n",
      "314\n",
      "But this makes sense as the task is a multi-modality solution! Needs more information than just a scan... \n",
      "\n",
      "In the clinical screening process (Fig. 1), patients’ demographic information (e.g., age and gender) is captured in electronic medical records (EMR). \n",
      "\n",
      "We consider two longitudinal CTs (TP0 for previous, TP1 for current) as the complete data for image modality. The non-image modality includes the following 14 risk factors: age, sex, education, body mass index, race, quit smoke time, smoke status, pack-year, chronic obstructive pulmonary disease, personal cancer history, family lung cancer history, nodule size, spiculation, upper lobe of nodule.\n",
      "---\n",
      "280\n",
      " Since this research did not impact clinical care, patient consent was waived. To keep patient’s Protected Health Information (PHI) secure, all patient-identifiable information associated with the images has been removed. Several DICOM attributes that are important for evaluating the spine conditions like patient’s age and sex were retained.\n",
      "---\n",
      "106\n",
      " For one dataset yes: For each scan the self-reported gender and age was noted (192/202 male/female, age 0–84 years) (but also the one where they want to find out gender, so necessary)\n",
      "---\n",
      "318\n",
      "Tabular data comprises 9 variables: age, gender, education, ApoE4, cerebrospinal fluid biomarkers A$$\\beta _{42}$$, P-tau181 and T-tau, and two summary measures derived from 18F-fluorodeoxyglucose and florbetapir PET scans. \n",
      "---\n",
      "269\n",
      "Article contains a table covering the gender, age, education year, first period, on medication and illness time of subjects included!\n",
      "---\n",
      "325\n",
      "This dataset contains 860 h&e stained diagnostic histopathology slides and clinical information from 389 patients. The clinical data include patient age, gender, ethnicity, race, vital status, days to last follow up and days to death information. \n",
      "\n",
      "The clinical features used in the building of SSCNN are numerical age, and categorical gender, ethnicity and race. \n",
      "---\n",
      "145\n",
      "This is all they write: \n",
      "Detailed demographic information of the datasets is provided in the supplementary material.\n",
      "---\n",
      "69\n",
      "Maybe at least:\n",
      " The details of the datasets are in Appendix A in the supplementary material. All these datasets will be publicly available sooner.\n",
      "---\n",
      "102\n",
      "age and sex:\n",
      "The dataset included N = 140 subjects with the Pre-AD group (93 subjects/330 records) and the Pro-AD group (47 subjects/170 records). The mean (std) of ages and sex ratio (Male:Famale) in Pre-AD group and Pro-AD group are 74.02(6.72)/(185:145) and 74.87(6.92)/(95:75), respectively\n",
      "---\n",
      "308\n",
      "Our first dataset is ChestX-ray14 [27], a public chest X-ray dataset with 108,948 images from 32,717 patients showing 14 pathological classes as well as a normal class. From this large dataset, we extract 43,322 posteroanterior (PA) views of adult patients (over 18) and split them into male (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figa_HTML.gif ) and female (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figb_HTML.gif ) partitions\n",
      "\n",
      "For the second: The scans are of volunteers at 18–24 weeks gestation (Ethics: anonymous during review), in a fetal cardiology clinic, where patients are referred to from primary screening and secondary care sites\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#column 1: id\n",
    "#column 19: demographics y/n\n",
    "#column 22: quote\n",
    "\n",
    "demo = df[df.iloc[:,19] == 'Yes']\n",
    "demo = demo.iloc[:,[1,20]]\n",
    "demo.reset_index()\n",
    "print('Number of articles mentioning demographics: ', len(demo))\n",
    "\n",
    "\n",
    "for index in range(len(demo)):\n",
    "    print(demo.iloc[index, 0])\n",
    "    print(demo.iloc[index, 1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37acc932",
   "metadata": {},
   "source": [
    "## Dataset collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9e0de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles mentioning dataset collection:  14\n",
      "78\n",
      "For this article (though plucked from an existing dataset\n",
      "---\n",
      "82\n",
      "For this article, from existing dataset\n",
      "---\n",
      "124\n",
      "For the purpose of this article\n",
      "---\n",
      "173\n",
      "They expanded their dataset with data specifically for this article: \n",
      "\n",
      "Additionally, we obtained 29 fundus images from patients (11 male, 18 female, all older than 47 years) at the University Eye Hospital with permission of the Institutional Ethics Board. We used these additional images as an independent test set.\n",
      "---\n",
      "258\n",
      "From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. \n",
      "---\n",
      "278\n",
      "For this article: To this end, we collected a big-data (BD) cohort consisting of $$3\\,790$$ patients, $$312\\,848$$ US images, and $$36\\,602$$ US studies from the PACS of Anonymized\n",
      "---\n",
      "280\n",
      "For this article\n",
      "---\n",
      "284\n",
      "For the article\n",
      "---\n",
      "291\n",
      "For one of the three it was created for this article specifically: \n",
      "The proposed method was extensively evaluated on three imbalanced medical image datasets, Skin7 [5], OCTMNIST [27], and X-ray6 (Table 1). Specially, X-ray6 contains six diseases of X-ray images (Atelectasis, Cardiomegaly, Emphysema, Hernia, Mass, Effusion), where the six classes were selected from the original 14-class dataset ChestX-ray14 [24] by removing those classes of images which may contain multiple or ambiguous diseases in single images. \n",
      "---\n",
      "294\n",
      "I think for this article:\n",
      "We evaluate our method on a dataset of 439 subjects, including 93, 98, and 248 HCC cases of grading LR-3, LR-4, and LR-5, respectively. All data were obtained using either a 1.5 T (Magnetom Aera, Siemens Healthcare, Erlangen, Germany) or a 3.0 T (uMR 770, United Imaging Healthcare, Shanghai, China) MR scanner with Gd-EOB-DTPA as contrast agent in same hospital and labeled by three radiologists with experience over 5 years. \n",
      "---\n",
      "297\n",
      "The final dataset is created for this article, the others no:\n",
      "Xray6 is a subset of ChestXray14 dataset [23], containing six diseases of X-ray images (Atelectasis, Cardiomegaly, Emphysema, Hernia, Mass, Effusion). Based on the smallest class (i.e., Hernia) which has only 110 images, the same number of images were randomly sampled from every other class, forming the small-sample Xray6 dataset. \n",
      "---\n",
      "309\n",
      "For this article\n",
      "---\n",
      "315\n",
      "For this article\n",
      "---\n",
      "316\n",
      "For this article: (generated from other previous datasets)\n",
      "176 patients (see patient selection in Fig. S2) with Gd-T1w and T2w-FLAIR scans from the TCGA-GBM [23] and TCGA-LGG [24] studies were obtained from TCIA [25] and annotated by 7 radiologists to delineate the enhancing lesion and edema region.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#column 1: id\n",
    "#column 21: dataset collection y/n\n",
    "#column 22: quote\n",
    "\n",
    "\n",
    "demo = df[df.iloc[:,21] == 'Yes']\n",
    "demo = demo.iloc[:,[1,22]]\n",
    "demo.reset_index()\n",
    "print('Number of articles mentioning dataset collection: ', len(demo))\n",
    "\n",
    "\n",
    "for index in range(len(demo)):\n",
    "    print(demo.iloc[index, 0])\n",
    "    print(demo.iloc[index, 1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47da679",
   "metadata": {},
   "source": [
    "## Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5514e424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "Deep neural networks (DNNs) are able to predict a person’s gender from retinal fundus images with high accuracy, even though this task is usually considered hardly possible by ophthalmologists. Therefore, it has been an open question which features allow reliable discrimination between male and female fundus images. To study this question, we used a particular DNN architecture called BagNet, which extracts local features from small image patches and then averages the class evidence across all patches. The BagNet performed on par with the more sophisticated Inception-v3 model, showing that the gender information can be read out from local features alone. \n",
      "\n",
      "In recent years, deep neural networks (DNNs) have achieved physician-level accuracy in various image-based medical tasks, e.g. in radiology [21], dermatology [10], pathology [15] and ophthalmology [7, 12]. Moreover, in some cases DNNs have been shown to have good performance in tasks that are not straightforward for physicians: for example, they can accurately predict the gender from retinal images [25]. As this task is typically not clinically relevant, ophthalmologists are not explicitly trained for it. \n",
      "---\n",
      "74\n",
      "Traditional supervised learning with deep neural networks requires a tremendous amount of labelled data to converge to a good solution. For 3D medical images, it is often impractical to build a large homogeneous annotated dataset for a specific pathology. Self-supervised methods offer a new way to learn a representation of the images in an unsupervised manner with a neural network. In particular, contrastive learning has shown great promises by (almost) matching the performance of fully-supervised CNN on vision tasks. Nonetheless, this method does not take advantage of available meta-data, such as participant’s age, viewed as prior knowledge. Here, we propose to leverage continuous proxy metadata, in the contrastive learning framework, by introducing a new loss called y-Aware InfoNCE loss. Specifically, we improve the positive sampling during pre-training by adding more positive examples with similar proxy meta-data with the anchor, assuming they share similar discriminative semantic features. With our method, a 3D CNN model pre-trained on $$10^4$$ multi-site healthy brain MRI scans can extract relevant features for three classification tasks: schizophrenia, bipolar diagnosis and Alzheimer’s detection. When fine-tuned, it also outperforms 3D CNN trained from scratch on these tasks, as well as state-of-the-art self-supervised methods. Our code is made publicly available here.\n",
      "\n",
      "Traditional supervised learning with deep neural networks requires a tremendous amount of labelled data to converge to a good solution. For 3D medical images, it is often impractical to build a large homogeneous annotated dataset for a specific pathology. Self-supervised methods offer a new way to learn a representation of the images in an unsupervised manner with a neural network. In particular, contrastive learning has shown great promises by (almost) matching the performance of fully-supervised CNN on vision tasks. Nonetheless, this method does not take advantage of available meta-data, such as participant’s age, viewed as prior knowledge. Here, we propose to leverage continuous proxy metadata, in the contrastive learning framework, by introducing a new loss called y-Aware InfoNCE loss. Specifically, we improve the positive sampling during pre-training by adding more positive examples with similar proxy meta-data with the anchor, assuming they share similar discriminative semantic features. With our method, a 3D CNN model pre-trained on $$10^4$$ multi-site healthy brain MRI scans can extract relevant features for three classification tasks: schizophrenia, bipolar diagnosis and Alzheimer’s detection. When fine-tuned, it also outperforms 3D CNN trained from scratch on these tasks, as well as state-of-the-art self-supervised methods. Our code is made publicly available here.\n",
      "---\n",
      "122\n",
      "Intracranial hemorrhage (ICH) is a life-threatening emergency with high rates of mortality and morbidity. Rapid and accurate detection of ICH is crucial for patients to get a timely treatment. In order to achieve the automatic diagnosis of ICH, most deep learning models rely on huge amounts of slice labels for training. Unfortunately, the manual annotation of CT slices by radiologists is time-consuming and costly. To diagnose ICH, in this work, we propose to use an attention-based multiple instance learning (Att-MIL) approach implemented through the combination of an attention-based convolutional neural network (Att-CNN) and a variational Gaussian process for multiple instance learning (VGPMIL). \n",
      "\n",
      "Acute intracranial hemorrhage (ICH) has always been a life-threatening event that causes high mortality and morbidity rate [13]. Rapid and early detection of ICH is essential because nearly $$30\\%$$ of the life loss happens in the first 24 h [18]. In order to prompt the optimal treatment to patients in short time, computer-aided diagnosis (CAD) is being designed to establish a better triaging protocol.\n",
      "\n",
      "Recently, deep learning (DL) algorithms have been proposed for the diagnosis of ICH. \n",
      "---\n",
      "321\n",
      "We propose a novel method of combining thoracic Computerized Tomography (CT) scans and clinical tabular data to improve the prediction of EF risks in EC patients. \n",
      "\n",
      "Radiotherapy plays a vital role in treating patients with esophageal cancer (EC), whereas potential complications such as esophageal fistula (EF) can be devastating and even life-threatening. Therefore, predicting EF risks prior to radiotherapies for EC patients is crucial for their clinical treatment and quality of life.\n",
      "\n",
      "Esophageal cancer (EC) is the 6th most common cause of cancer-related death. To treat patients with unresectable locally advanced esophageal squamous cell carcinoma (SCC), chemotherapy and/or radiotherapy have demonstrated effectiveness and received considerable attention [1, 2]. Unfortunately, esophageal fistula (EF) is one of the complications resulting from these treatments [3]. Around 4.8–22.1% of the EC patients developed EF due to chemoradiotherapy [5]; this drastically reduces life expectancy to a rate of two to three months.\n",
      "---\n",
      "316\n",
      "Here, we predict the overall survival (OS) of glioma patients from diverse multimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to combine information from multiparametric MRI exams, biopsy-based modalities (such as H&E slide images and/or DNA sequencing), and clinical variables into a comprehensive multimodal risk score. \n",
      "\n",
      "Clinical decision-making in oncology involves multimodal data such as radiology scans, molecular profiling, histopathology slides, and clinical factors. Despite the importance of these modalities individually, no deep learning framework to date has combined them all to predict patient prognosis.\n",
      "\n",
      "Glioma is an intuitive candidate for deep learning-based multimodal biomarkers owing to the presence of well-characterized prognostic information across modalities [5], as well as its severity\n",
      "\n",
      "---\n",
      "315\n",
      "Lymph node metastasis (LNM) is the most critical prognosis factor in esophageal squamous cell carcinoma (ESCC). Effective and adaptive integration of preoperative CT images and multi-sourced non-imaging clinical factors is a challenging issue. In this work, we propose a graph-based reasoning model to learn new representations from multi-categorical clinical parameters for LNM prediction\n",
      "\n",
      "Esophageal cancer (EC) is the seventh most common cancer leading to death in the United States [1]. The overall 5-year survival rate between 2010 and 2016 is 19.9%. [2] Diagnosed at early stage 1, EC patients’ chance of surviving 5 years can be increased to 47.1%. [2] An independent factor in EC prognosis is lymph node metastasis (LNM), which is most common in the significant histological EC subtype, esophageal squamous cell carcinoma (ESCC). [3] Thus, predicting LNM preoperatively is critical in clinical treatment decisions and planning for radiotherapy and surgery.\n",
      "---\n",
      "314\n",
      " Motivated by partial bidirectional generative adversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method that imputes one modality combining the conditional knowledge from another modality. \n",
      "\n",
      "Data from multi-modality provide complementary information in clinical prediction, but missing data in clinical cohorts limits the number of subjects in multi-modal learning context. Multi-modal missing imputation is challenging with existing methods when 1) the missing data span across heterogeneous modalities (e.g., image vs. non-image); or 2) one modality is largely missing.\n",
      "\n",
      "To our knowledge, we are the first to impute missing data by modeling joint distribution of image and non-image data with adversarial training; \n",
      "\n",
      "Lung cancer has the highest cancer death rate [1] and early diagnosis with low-dose computed tomography (CT) can reduce the risk of dying from lung cancer by 20% [2, 3]. Risk factors (e.g., age and nodule size) are widely used in machine learning and established prediction models [4–7]. With deep learning techniques, CT image features can be automatically extracted at the nodule-level [8], scan-level [9], or patient-level with longitudinal scans [10]. Previous studies demonstrated that CT image features and risk factors provide complementary information, which is combined to improve lung cancer risk estimation [11].\n",
      "---\n",
      "309\n",
      "The emergence of novel pathogens and zoonotic diseases like the SARS-CoV-2 have underlined the need for developing novel diagnosis and intervention pipelines that can learn rapidly from small amounts of labeled data. \n",
      "This is particularly a difficult task given that closely related pathogens can share more than $$90\\%$$ of their genome structure. In this work, we address these challenges by proposing MG-Net, a self-supervised representation learning framework that leverages multi-modal context using pseudo-imaging data derived from clinical metagenome sequences. We show that the proposed framework can learn robust representations from unlabeled data that can be used for downstream tasks such as metagenome sequence classification with limited access to labeled data. Extensive experiments show that the learned features outperform current baseline metagenome representations, given only 1000 samples per class.\n",
      "\n",
      "This is particularly a difficult task given that closely related pathogens can share more than $$90\\%$$ of their genome structure. In this work, we address these challenges by proposing MG-Net, a self-supervised representation learning framework that leverages multi-modal context using pseudo-imaging data derived from clinical metagenome sequences. We show that the proposed framework can learn robust representations from unlabeled data that can be used for downstream tasks such as metagenome sequence classification with limited access to labeled data. Extensive experiments show that the learned features outperform current baseline metagenome representations, given only 1000 samples per class.\n",
      "\n",
      "Advances in DNA sequencing technologies [21, 22] have made possible the determination of whole-genome sequences of simple unicellular (e.g., bacteria) and complex multicellular (e.g., human) organisms at a cheaper, faster, and larger scale. \n",
      "---\n",
      "304\n",
      "In this paper, a classifier is trained to detect embryo development stage with learning strategies added to explicitly address challenges of this task.\n",
      "\n",
      "In Vitro Fertilization (IVF) treatment is increasingly chosen by couples suffering from infertility as a means to conceive. Time-lapse imaging technology has enabled continuous monitoring of embryos in vitro and time-based development metrics for assessing embryo quality prior to transfer. Timing at which embryos reach certain development stages provides valuable information about their potential to become a positive pregnancy. Automating development stage detection of day 4–5 embryos remains difficult due to small variation between stages. \n",
      "\n",
      "With infertility rates rising, more than 100,000 embryo transfers for IVF treatment are performed annually in the US \n",
      "---\n",
      "301\n",
      "Our work has four highlights. First, we propose to formulate the brain midline delineation as a 3D hemisphere segmentation task, which recognizes the midline location via enriched anatomical features. Second, we employ a distance-weighted map for midline aware loss. Third, we introduce rectificative learning for the model to handle various head poses. Finally, considering the complexity of hematomas distribution in human brain, we build a classification model to automatically identify the situation when hematoma breaks into brain ventricles and formulate a midline correction strategy to locally adjust the midline according to the location and boundary of hematomas. To our best knowledge, it is the first study focusing on delineating the brain midline on 3D CT images of hematoma patients and handling the situation of ventricle break-in. \n",
      "\n",
      "Brain midline delineates the boundary between the two cerebral hemispheres of the human brain, which plays a significant role in guiding intracranial hemorrhage surgery. Large midline shift caused by hematomas remains an inherent challenge for delineation. However, most previous methods only handle normal brains and delineate the brain midline on 2D CT images. In this study, we propose a novel hemisphere-segmented framework (HSF) for generating smooth 3D brain midline especially when large hematoma shifts the midline. \n",
      "\n",
      "To our best knowledge, it is the first study focusing on delineating the brain midline on 3D CT images of hematoma patients and handling the situation of ventricle break-in. \n",
      "---\n",
      "294\n",
      "we propose a two-stage method for automatically predicting HCC grades according to multiphasic magnetic resonance imaging (MRI). \n",
      "\n",
      "Liver cancer is the third leading cause of cancer death in the world, where the hepatocellular carcinoma (HCC) is the most common case in primary liver cancer. In general diagnosis, accurate prediction of HCC grades is of great help to the subsequent treatment to improve the survival rate. Rather than to straightly predict HCC grades from images, it will be more interpretable in clinic to first predict the symptoms and then obtain the HCC grades from the Liver Imaging Reporting and Data System (LI-RADS). Accordingly, we propose a two-stage method for automatically predicting HCC grades according to multiphasic magnetic resonance imaging (MRI). \n",
      "---\n",
      "284\n",
      "Detailed analysis of seizure semiology, the symptoms and signs which occur during a seizure, is critical for management of epilepsy patients. Inter-rater reliability using qualitative visual analysis is often poor for semiological features. Therefore, automatic and quantitative analysis of video-recorded seizures is needed for objective assessment. We present GESTURES, a novel architecture combining convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn deep representations of arbitrarily long videos of epileptic seizures.\n",
      "\n",
      "Detailed analysis of seizure semiology, the symptoms and signs which occur during a seizure, is critical for management of epilepsy patients. Inter-rater reliability using qualitative visual analysis is often poor for semiological features. Therefore, automatic and quantitative analysis of video-recorded seizures is needed for objective assessment. We present GESTURES, a novel architecture combining convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn deep representations of arbitrarily long videos of epileptic seizures.\n",
      "---\n",
      "280\n",
      "This work aims at developing and evaluating a deep learning-based framework, named VinDr-SpineXR, for the classification and localization of abnormalities from spine X-rays\n",
      "\n",
      "Radiographs are used as the most important imaging tool for identifying spine anomalies in clinical practice. The evaluation of spinal bone lesions, however, is a challenging task for radiologists\n",
      "---\n",
      "278\n",
      "we introduce auto-decoded deep latent embeddings (ADDLE), which explicitly models the tendencies of each rater using an auto-decoder framework. \n",
      "\n",
      "Depending on the application, radiological diagnoses can be associated with high inter- and intra-rater variabilities. Most computer-aided diagnosis (CAD) solutions treat such data as incontrovertible, exposing learning algorithms to considerable and possibly contradictory label noise and biases. Thus, managing subjectivity in labels is a fundamental problem in medical imaging analysis.\n",
      "---\n",
      "277\n",
      "In this work, we investigate the feasibility of using a single-phase non-contrast CT scan, a cheaper, simpler, and safer substituent, to detect resectable pancreatic mass and classify the detection as pancreatic ductal adenocarcinoma (PDAC) or other abnormalities (nonPDAC) or normal pancreas.\n",
      "\n",
      "Pancreatic cancer is a relatively uncommon but most deadly cancer. Screening the general asymptomatic population is not recommended due to the risk that a significant number of false positive individuals may undergo unnecessary imaging tests (e.g., multi-phase contrast-enhanced CT scans) and follow-ups, adding health care costs greatly and no clear patient benefits\n",
      "---\n",
      "258\n",
      "To enhance the diagnosis with only US available, in this work, we propose a knowledge-guided data augmentation framework, which consists of a modal translater and a semantic inverter, achieving cross-modal and semantic data augmentation simultaneously. \n",
      "\n",
      "Breast cancer, the most commonly diagnosed cancer, is the fifth leading cause of cancer death all over the world [24].\n",
      "---\n",
      "324\n",
      "In this paper, we present an end-to-end autoencoder-based multiple instance neural network (AMINN) for the prediction of survival outcomes in multifocal CRLM patients using radiomic features extracted from contrast-enhanced MRIs.\n",
      "\n",
      "Colorectal cancer is one of the most common and lethal cancers and colorectal cancer liver metastases (CRLM) is the major cause of death in patients with colorectal cancer. Multifocality occurs frequently in CRLM, but is relatively unexplored in CRLM outcome prediction. Most existing clinical and imaging biomarkers do not take the imaging features of all multifocal lesions into account. \n",
      "---\n",
      "331\n",
      "COVID-19 image analysis has mostly focused on diagnostic tasks using single timepoint scans acquired upon disease presentation or admission. We present a deep learning-based approach to predict lung infiltrate progression from serial chest radiographs (CXRs) of COVID-19 patients.\n",
      "\n",
      "Coronavirus disease 2019 (COVID-19) remains at the forefront of threats to public health. As a result, there continues to be a critical need to further understand the progression of the disease process.\n",
      "Despite the many studies analyzing the use of CXRs in COVID-19, machine learning applications have been limited to diagnostic tasks including differentiating COVID-19 from viral pneumonia and predicting clinical outcomes such as mortality and mechanical ventilation requirement \n",
      "---\n",
      "170\n",
      "In order to ensure that a radiomics-based machine learning model will robustly generalize to new, unseen data (which may harbor significant variations compared to the discovery cohort), radiomic features are often screened for stability via test/retest or cross-site evaluation. However, as stability screening is often conducted independent of the feature selection process, the resulting feature set may not be simultaneously optimized for discriminability, stability, as well as sparsity. In this work, we present a novel radiomic feature selection approach termed SPARse sTable lAsso (SPARTA), uniquely developed to identify a highly discriminative and sparse set of features which are also stable to acquisition or institution variations\n",
      "\n",
      "In radiomics [1] applications, feature selection strategies such as maximum relevance minimum redundancy (mRMR) [2], least absolute shrinkage and selection operator (LASSO) [3], or Wilcoxon rank-sum testing (WLCX) [4], are widely used to find a relatively parsimonious (i.e. sparse) subset of features, which in combination are able to accurately distinguish between any classes of interest (i.e. are discriminatory). Critically, the ultimate validation of these radiomics models is in a multi-institutional setting where unseen datasets may suffer from significant variations compared to the original discovery cohort. Towards this, it has become critical to determine whether selected radiomic features are stable or reproducible within an institution (intra-site), between institutions (inter-site), as well as in repeated test/re-test evaluation [5]. The underlying hypothesis here is that stable radiomic features which are also discriminable will result in better model generalizability on new, unseen data.\n",
      "---\n",
      "82\n",
      "Most deep-learning based magnetic resonance image (MRI) analysis methods require numerous amounts of labelling work manually done by specialists, which is laborious and time-consuming. In this paper, we aim to develop a hybrid-supervised model generation strategy, called SpineGEM, which can economically generate a high-performing deep learning model for the classification of multiple pathologies of lumbar degeneration disease (LDD).\n",
      "\n",
      "Most deep-learning based magnetic resonance image (MRI) analysis methods require numerous amounts of labelling work manually done by specialists, which is laborious and time-consuming. In this paper, we aim to develop a hybrid-supervised model generation strategy, called SpineGEM, which can economically generate a high-performing deep learning model for the classification of multiple pathologies of lumbar degeneration disease (LDD).\n",
      "\n",
      "\n",
      "Magnetic resonance imaging (MRI) is widely used in Orthopaedics for the assessment of spine pathologies [1, 2]. Automated MRI analyses can have great clinical significance, by improving the efficiency and consistency of diagnosis [3]. For pathology classifications, the accuracy generated by a deep learning model can be comparable with human specialists [4, 5]. However, numerous amounts of manual labels are required for the training process of the model, which is expensive, laborious, and time-consuming.\n",
      "---\n",
      "106\n",
      "Abstract\n",
      "The task of 3D shape classification is closely related to finding a good representation of the shapes. In this study, we focus on surface representations of complex anatomies and on how such representations can be utilized for super- and unsupervised classification. We present a novel Implicit Neural Distance Representation based on unsigned distance fields (UDFs). The UDFs can be embedded into a low-dimensional latent space, which is optimized using only the shape itself. We demonstrate that this self-optimized latent space holds important global shape information useful for reconstructing the anatomies, but also that unsupervised clustering of the latent vectors successfully separates different anatomies (left atrium, left/right ear-canals and human faces). Finally, we show how the representation can be used to do gender classification of human face geometries, which is a notoriously hard problem.\n",
      "\n",
      "Being able to describe and classify complex anatomical shapes are tasks inevitably linked to finding a good representation of the shape. In the human body, a variety of complex anatomies exists and often such anatomies are represented as a 3D mesh surface either as an isosurface in an image volume or directly from a 3D surface scanner. Representing such 3D meshes in a geometric deep learning framework that can extract global shape characteristics and use them for clustering or classification is not straightforward. \n",
      "---\n",
      "112\n",
      "Abstract\n",
      "Ulcerative colitis (UC) classification, which is an important task for endoscopic diagnosis, involves two main difficulties. First, endoscopic images with the annotation about UC (positive or negative) are usually limited. Second, they show a large variability in their appearance due to the location in the colon. Especially, the second difficulty prevents us from using existing semi-supervised learning techniques, which are the common remedy for the first difficulty. In this paper, we propose a practical semi-supervised learning method for UC classification by newly exploiting two additional features, the location in a colon (e.g., left colon) and image capturing order, both of which are often attached to individual images in endoscopic image sequences. The proposed method can extract the essential information of UC classification efficiently by a disentanglement process with those features. Experimental results demonstrate that the proposed method outperforms several existing semi-supervised learning methods in the classification task, even with a small number of annotated images.\n",
      "\n",
      "Abstract\n",
      "Ulcerative colitis (UC) classification, which is an important task for endoscopic diagnosis, involves two main difficulties. First, endoscopic images with the annotation about UC (positive or negative) are usually limited. Second, they show a large variability in their appearance due to the location in the colon. Especially, the second difficulty prevents us from using existing semi-supervised learning techniques, which are the common remedy for the first difficulty. In this paper, we propose a practical semi-supervised learning method for UC classification by newly exploiting two additional features, the location in a colon (e.g., left colon) and image capturing order, both of which are often attached to individual images in endoscopic image sequences. The proposed method can extract the essential information of UC classification efficiently by a disentanglement process with those features. Experimental results demonstrate that the proposed method outperforms several existing semi-supervised learning methods in the classification task, even with a small number of annotated images.\n",
      "---\n",
      "118\n",
      "Abstract\n",
      "Chest X-ray (CXR) is the most typical diagnostic X-ray examination for screening various thoracic diseases. Automatically localizing lesions from CXR is promising for alleviating radiologists’ reading burden. However, CXR datasets are often with massive image-level annotations and scarce lesion-level annotations, and more often, without annotations. Thus far, unifying different supervision granularities to develop thoracic disease detection algorithms has not been comprehensively addressed. In this paper, we present OXnet, the first deep omni-supervised thoracic disease detection network to our best knowledge that uses as much available supervision as possible for CXR diagnosis. We first introduce supervised learning via a one-stage detection model. Then, we inject a global classification head to the detection model and propose dual attention alignment to guide the global gradient to the local detection branch, which enables learning lesion detection from image-level annotations. We also impose intra-class compactness and inter-class separability with global prototype alignment to further enhance the global information learning. Moreover, we leverage a soft focal loss to distill the soft pseudo-labels of unlabeled data generated by a teacher model. Extensive experiments on a large-scale chest X-ray dataset show the proposed OXnet outperforms competitive methods with significant margins. Further, we investigate omni-supervision under various annotation granularities and corroborate OXnet is a promising choice to mitigate the plight of annotation shortage for medical image diagnosis (Code is available at https://​github.​com/​LLYXC/​OXnet.).\n",
      "\n",
      "Modern object detection algorithms often require a large amount of supervision signals. However, annotating abundant medical images for disease detection is infeasible due to the high dependence of expert knowledge and expense of human labor. Consequently, many medical datasets are weakly labeled or, more frequently, unlabeled [22]. This situation especially exists for chest X-rays (CXR), which is the most commonly performed diagnostic X-ray examination. Apart from massive unlabeled data, CXR datasets often have image-level annotations that can be easily obtained by text mining from numerous radiological reports [9, 27], while lesion-level annotations (e.g., bounding boxes) scarcely exist [7, 28]. Therefore, efficiently leveraging available annotations to develop thoracic disease detection algorithms has significant practical value.\n",
      "---\n",
      "124\n",
      "Abstract\n",
      "Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician’s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines.\n",
      "\n",
      "A growing challenge in medical imaging is the need for more qualified experts to read an increasing volume of medical images, which has led to interpretation delays and reduced quality of healthcare [25]. Deep learning models in radiology [5], dermatology [6], and other areas [7] can increase physician throughput to alleviate this challenge. However, a major bottleneck to developing such models is the need for large labeled datasets [7].\n",
      "---\n",
      "291\n",
      "Intelligent diagnosis is often biased toward common diseases due to data imbalance between common and rare diseases. Such bias may still exist even after applying re-balancing strategies during model training. To further alleviate the bias, we propose a novel method which works not in the training but in the inference phase.\n",
      "\n",
      "Intelligent diagnosis is often biased toward common diseases due to data imbalance between common and rare diseases. Such bias may still exist even after applying re-balancing strategies during model training. To further alleviate the bias, we propose a novel method which works not in the training but in the inference phase. For any test input data, based on the difference between the temperature-tuned classifier output and a target probability distribution derived from the inverse frequency of different diseases, the input data can be slightly perturbed in a way similar to adversarial learning.\n",
      "---\n",
      "267\n",
      "we propose a selective attention regularization module (SAttenReg) to mimic the diagnosis process of pathologists.\n",
      "\n",
      "Nonalcoholic fatty liver disease (NAFLD) is the most common cause of liver disease worldwide [21, 22]. It is estimated that the prevalence of NAFLD is between 25% and 45%, which has become an important public health concern [4, 20]. \n",
      "---\n",
      "322\n",
      "In this paper, we propose a Hybrid Aggregation Network (HANet) to adaptively aggregate information from multiple WSIs of one patient for survival analysis.\n",
      "\n",
      "Understanding of prognosis and mortality is crucial for evaluating the treatment plans for patients. Recent developments of digital pathology and deep learning bring the possibility of predicting survival time using histopathology whole slide images (WSIs). However, most prevalent methods usually rely on a small set of patches sampled from a WSI and are unable to directly learn from an entire WSI. We argue that a small patch set cannot fully represent patients’ survival risks due to the heterogeneity of tumors; moreover, multiple WSIs from one patient need to be evaluated together.\n",
      "\n",
      "Survival analysis is becoming a popular field in healthcare research. The purpose of survival analysis is to examine how specified factors (e.g. smoking, age, treatment, etc.) affect the occurrence probability of a particular event (e.g. death, recurrence of the disease, etc.) at a certain time point. Clinicians can exploit survival analysis to evaluate the significance of prognostic variables and subsequently make an early decision among treatment options.\n",
      "\n",
      "We present a novel Hybrid Aggregation Network (HANet)\n",
      "---\n",
      "320\n",
      "we propose a novel semi-supervised approach named GKD based on the knowledge distillation. We train a teacher component that employs the label-propagation algorithm besides a deep neural network to benefit from the graph and non-graph modalities only in the training phase. The teacher component embeds all the available information into the soft pseudo-labels. The soft pseudo-labels are then used to train a deep student network for disease prediction of unseen test data for which the graph modality is unavailable. We perform our experiments on two public datasets for diagnosing Autism spectrum disorder, and Alzheimer’s disease, along with a thorough analysis on synthetic multi-modal datasets. According to these experiments, GKD outperforms the previous graph-based deep learning methods in terms of accuracy, AUC, and Macro F1\n",
      "\n",
      "The increased amount of multi-modal medical data has opened the opportunities to simultaneously process various modalities such as imaging and non-imaging data to gain a comprehensive insight into the disease prediction domain. Recent studies using Graph Convolutional Networks (GCNs) provide novel semi-supervised approaches for integrating heterogeneous modalities while investigating the patients’ associations for disease prediction. However, when the meta-data used for graph construction is not available at inference time (e.g., coming from a distinct population), the conventional methods exhibit poor performance.\n",
      "---\n",
      "318\n",
      "Prior work on diagnosing Alzheimer’s disease from magnetic resonance images of the brain established that convolutional neural networks (CNNs) can leverage the high-dimensional image information for classifying patients. However, little research focused on how these models can utilize the usually low-dimensional tabular information, such as patient demographics or laboratory measurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a general-purpose module for CNNs that dynamically rescales and shifts the feature maps of a convolutional layer, conditional on a patient’s tabular clinical information. We show that DAFT is highly effective in combining 3D image and tabular information for diagnosis and time-to-dementia prediction, where it outperforms competing CNNs with a mean balanced accuracy of 0.622 and mean c-index of 0.748, respectively. \n",
      "\n",
      "In recent years, deep convolutional neural networks (CNNs) have become the standard for classification of Alzheimer’s disease (AD) from magnetic resonance images (MRI) of the brain (see e.g. [4, 29] for an overview). CNNs excel at extracting high-level information about the neuroanatomy from MRI. However, brain MRI only offers a partial view on the underlying changes causing cognitive decline. Therefore, clinicians and researchers often rely on tabular data such as patient demographics, family history, or laboratory measurements from cerebrospinal fluid for diagnosis. In contrast to image information, tabular data is typically low-dimensional and individual variables capture rich clinical knowledge.\n",
      "---\n",
      "313\n",
      ". In this work, we propose a novel deep curriculum learning method that utilizes radiomics information as a source of additional knowledge to guide training using customized curriculums. Specifically, we define a new measure, termed radiomics score, to capture the difficulty of classifying a set of samples. We use the radiomics score to enable a newly designed curriculum-based training scheme. In this scheme, the loss function component is weighted and initialized by the corresponding radiomics score of each sample, and furthermore, the weights are continuously updated in the course of training based on our customized curriculums to enable curriculum learning. We implement and evaluate our methods on a typical computer-aided diagnosis of breast cancer.\n",
      "\n",
      "The traditional way of training Convolutional Neural Networks (CNNs) makes use of images as the sole source of data. In medical imaging applications, additional information or clinical knowledge are often available along with the data, such as pre-assessment made by clinicians, auxiliary tasks in relation to a target task, qualitative clinical experience, etc. These sources of information can be useful for a target task but are mostly ignored in the current practice of data-driven deep learning modeling.\n",
      "---\n",
      "312\n",
      "This work explores alternatives on how to augment the training data for colon carcinoma metastasis detection when there is limited or no representation of the target domain. Through an exhaustive study of cross-validated experiments with limited training data availability, we evaluate both an inter-organ approach utilizing already available data for other tissues, and an intra-organ approach, utilizing the primary tumor. Both these approaches result in little to no extra annotation effort. Our results show that these data augmentation strategies can be an efficient way of increasing accuracy on metastasis detection, but fore-most increase robustness.\n",
      "\n",
      "The scarcity of labeled data is a major bottleneck for developing accurate and robust deep learning-based models for histopathology applications.\n",
      "\n",
      "Colon cancer is the third most common cancer type in the world, where the majority of the cases are classified as adenocarcinoma\n",
      "---\n",
      "311\n",
      "Linear Prediction Residual for Efficient Diagnosis of Parkinson’s Disease from Gait\n",
      "\n",
      "Parkinson’s Disease (PD) is a chronic and progressive neurological disorder that results in rigidity, tremors and postural instability. There is no definite medical test to diagnose PD and diagnosis is mostly a clinical exercise. Although guidelines exist, about 10–30% of the patients are wrongly diagnosed with PD. Hence, there is a need for an accurate, unbiased and fast method for diagnosis. In this study, we propose LPGNet, a fast and accurate method to diagnose PD from gait. LPGNet uses Linear Prediction Residuals (LPR) to extract discriminating patterns from gait recordings and then uses a 1D convolution neural network with depth-wise separable convolutions to perform diagnosis. \n",
      "---\n",
      "310\n",
      "we propose a Multimodal Multitask Deep Learning (MMDL) approach for CBIR on radiology images.\n",
      "\n",
      "Content-based image retrieval (CBIR) is of increasing interest for clinical applications spanning differential diagnosis, prognostication, and indexing of electronic radiology databases. However, meaningful CBIR for radiology applications requires capabilities to address the semantic gap and assess similarity based on fine-grained image features. We observe that images in radiology databases are often accompanied by free-text radiologist reports containing rich semantic information. \n",
      "---\n",
      "306\n",
      "The performance of anatomy site recognition is critical for computer-aided diagnosis systems such as the quality evaluation of endoscopic examinations and the automatic generating of electronic medical records. To achieve an accurate recognition model, it requires extensive training samples and precise annotations from human experts, especially for deep learning based methods. However, due to the similar appearance of gastrointestinal (GI) anatomy sites, it is hard to annotate accurately and is expensive to acquire such high quality dataset at a large scale. Therefore, to balance the cost-performance trade-offs, in this work we propose an effective annotation refinement approach which leverages a small amount of trust data that is accurately labelled by experts to further improve the training performance on a large amount of noisy label data. \n",
      "\n",
      "The performance of anatomy site recognition is critical for computer-aided diagnosis systems such as the quality evaluation of endoscopic examinations and the automatic generating of electronic medical records. To achieve an accurate recognition model, it requires extensive training samples and precise annotations from human experts, especially for deep learning based methods. However, due to the similar appearance of gastrointestinal (GI) anatomy sites, it is hard to annotate accurately and is expensive to acquire such high quality dataset at a large scale. Therefore, to balance the cost-performance trade-offs, in this work we propose an effective annotation refinement approach which leverages a small amount of trust data that is accurately labelled by experts to further improve the training performance on a large amount of noisy label data. \n",
      "---\n",
      "302\n",
      "To this end, we propose in this work a novel hybrid approach to rare disease classification, featuring two key novelties targeted at the above drawbacks.\n",
      "\n",
      "Rare diseases are characterized by low prevalence and are often chronically debilitating or life-threatening. Imaging-based classification of rare diseases is challenging due to the severe shortage in training examples. Few-shot learning (FSL) methods tackle this challenge by extracting generalizable prior knowledge from a large base dataset of common diseases and normal controls, and transferring the knowledge to rare diseases. Yet, most existing methods require the base dataset to be labeled and do not make full use of the precious examples of the rare diseases.\n",
      "\n",
      "Rare diseases are a significant public health issue and a challenge to healthcare. On aggregate, the number of people suffering from rare diseases worldwide is estimated over 400 million, and there are about 5000–7000 rare diseases—with 250 new ones appearing each year [27]. Patients with rare diseases face delayed diagnosis: 10% of patients spent 5–30 years to reach a final diagnosis. Besides, many rare diseases can be misdiagnosed. \n",
      "---\n",
      "300\n",
      "we propose a novel framework that adaptively calibrates the site-specific features into site-invariant features via a novel modulation mechanism. \n",
      "\n",
      "In general, it is expected that large amounts of functional magnetic resonance imaging (fMRI) would be helpful to deduce statistically meaningful biomarkers or to build generalized predictive models for brain disease diagnosis. However, the site-variation inherent in rs-fMRI hampers the researchers to use the entire samples collected from multiple sites because it involves the unfavorable heterogeneity in data distribution, thus negatively impact on identifying biomarkers and making a diagnostic decision. To alleviate this challenging multi-site problem, we propose a novel framework that adaptively calibrates the site-specific features into site-invariant features via a novel modulation mechanism.\n",
      "---\n",
      "298\n",
      "In this paper, we propose the collaborative diagnosis-synthesis framework (CDSF) for joint missing neuroimage imputation and multi-modal diagnosis of neurodegenerative disorders. \n",
      "\n",
      "The missing data issue is a common problem in multi-modal neuroimage (e.g., MRI and PET) based diagnosis of neurodegenerative disorders.\n",
      "---\n",
      "290\n",
      "we propose a Bayesian generative model for continual learning built on a fixed pre-trained feature extractor. In this model, knowledge of each old class can be compactly represented by a collection of statistical distributions, e.g. with Gaussian mixture models, and naturally kept from forgetting in continual learning.\n",
      "\n",
      "Current deep learning models are characterised by catastrophic forgetting of old knowledge when learning new classes. This poses a challenge in intelligent diagnosis systems where initially only training data of a limited number of diseases are available.  In this case, updating the intelligent system with data of new diseases would inevitably downgrade its performance on previously learned diseases. Inspired by the process of learning new knowledge in human brains,\n",
      "---\n",
      "288\n",
      "we propose a deep learning-based approach to simultaneously estimate spine centerline and spinal curvature with shared convolutional backbone\n",
      "\n",
      "Spinal curvature estimation plays an important role in adolescent idiopathic scoliosis (AIS) evaluation and treatment. \n",
      "---\n",
      "287\n",
      "we develop DeepOPG that breaks the summarization process into functional segmentation and tooth localization, the latter of which is further refined by a novel dental coherence module. We also leverage weak supervision labels to improve detection results in a reinforcement learning scenario.\n",
      "\n",
      "Clinical finding summaries from an orthopantomogram, or a dental panoramic radiograph, have significant potential to improve patient communication and speed up clinical judgments. While orthopantomogram is a first-line tool for dental examinations, no existing work has explored the summarization of findings from it. \n",
      "---\n",
      "286\n",
      "A holistic understanding of dual-view transformation (DVT) is an enabling technique for computer-aided diagnosis (CAD) of breast lesion in mammogram, e.g., micro-calcification ($$\\mu $$C) or mass matching, dual-view feature extraction etc. Learning a complete DVT usually relies on a dense supervision which indicates a corresponding tissue in one view for each tissue in another. Since such dense supervision is infeasible to obtain in practical, a sparse supervision of some traceable lesion tissues across two views is thus an alternative but will lead to a defective DVT, limiting the performance of existing CAD systems dramatically. To address this problem, our solution is simple but very effective, i.e., densifying the existing sparse supervision by synthesizing lesions across two views\n",
      "\n",
      "Since such dense supervision is infeasible to obtain in practical, a sparse supervision of some traceable lesion tissues across two views is thus an alternative but will lead to a defective DVT, limiting the performance of existing CAD systems dramatically. \n",
      "---\n",
      "283\n",
      "we propose a novel mechanism for sampling training data based on the popular MixUp regularization technique, which we refer to as Balanced-MixUp. \n",
      "\n",
      "Highly imbalanced datasets are ubiquitous in medical image classification problems. In such problems, it is often the case that rare classes associated to less prevalent diseases are severely under-represented in labeled databases, typically resulting in poor performance of machine learning algorithms due to overfitting in the learning process.\n",
      "---\n",
      "276\n",
      "this paper tackles the problem of learning a model from the source data for which can directly generalize to an unseen target domain for SCD prediction.\n",
      "\n",
      "It is highly desired to predict the progress of SCD for possible intervention of AD-related cognitive decline. Many neuroimaging-based methods have been developed for AD diagnosis, but there are few studies devoted to automated progress prediction of SCD due to the limited number of SCD subjects. \n",
      "---\n",
      "271\n",
      " Since normal human airways share an anatomical structure, we design a graph prototype whose structure follows the normal airway anatomy. Then, we learn the prototype and a graph neural network from a weakly-supervised airway dataset, i.e., only the holistic label is available, indicating if the airway has anomaly or not, but which bronchus node has the anomaly is unknown. During inference, the graph neural network predicts the anomaly score at both the holistic level and node-level of an airway. \n",
      "\n",
      "Detecting the airway anomaly can be an essential part to aid the lung disease diagnosis. \n",
      "---\n",
      "270\n",
      "we propose a region ensemble model using a divide and conquer strategy to capture the disease’s finer representation\n",
      "\n",
      "Despite many recent advances, computer-aided mild cognitive impairment (MCI) conversion prediction is still a very challenging task due to: 1) the abnormal areas are subtle compared to the size of the whole brain, 2) the features’ dimension is much larger than the number of samples. \n",
      "---\n",
      "269\n",
      ", we propose a tensor-based multi-index representation learning (TMRL) framework for fMRI-based MDD detection\n",
      "\n",
      "Major depression disorder (MDD) is one of the most prevalent disabling disorder, characterized by depressed mood, loss of interest or pleasure in nearly all activities. This mental illness has a high mortality rate due to the suicidal behavior of MDD patients, while the high cost of treatment troubles patients, their family members, and society [1, 2]. Even though many efforts have been made in clinical neuroscience and psychiatric research, the unknown etiology and pathological mechanism still prevent us from fully understanding the disease.\n",
      "---\n",
      "268\n",
      "we propose a novel Categorical Relation-preserving Contrastive Knowledge Distillation (CRCKD) algorithm, which takes the commonly used mean-teacher model as the supervisor. \n",
      "\n",
      "The amount of medical images for training deep classification models is typically very scarce, making these deep models prone to overfit the training data. Studies showed that knowledge distillation (KD), especially the mean-teacher framework which is more robust to perturbations, can help mitigate the over-fitting effect. \n",
      "---\n",
      "266\n",
      "In this paper we propose a BM-based conditional training for two-stage ULD, which can (i) reduce positive vs. negative anchor imbalance via a BM-based conditioning (BMC) mechanism for anchor sampling instead of traditional IoU-based rule; and (ii) adaptively compute size-adaptive BM (ABM) M. de Bruijne et al. (eds.)Medical Image Computing and Computer Assisted Intervention – MICCAI 2021Image Processing, Computer Vision, Pattern Recognition, and Graphics12905\n",
      "https://doi.org/10.1007/978-3-030-87240-3_14\n",
      "Conditional Training with Bounding Map for Universal Lesion Detection\n",
      "Han Li1, 2  , Long Chen2, 3  , Hu Han2  , Ying Chi4   and S. Kevin Zhou1, 2  \n",
      "(1)\n",
      "Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), School of Biomedical Engineering & Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China\n",
      "(2)\n",
      "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China\n",
      "(3)\n",
      "School of Electronic, Electrical and Communication Engineering, University of the Chinese Academy of Science, Beijing, China\n",
      "(4)\n",
      "Healthcare Intelligence, AIC, DAMO Academy, Alibaba Group, Hangzhou, China\n",
      " \n",
      " \n",
      "Han Li\n",
      "Email: han.li@miracle.ict.ac.cn\n",
      " \n",
      "Long Chen\n",
      "Email: long.chen@miracle.ict.ac.cn\n",
      " \n",
      "Hu Han (Corresponding author)\n",
      "Email: hanhu@ict.ac.cn\n",
      " \n",
      "Ying Chi\n",
      "Email: xinyi.cy@alibaba-inc.com\n",
      "Abstract\n",
      "Universal Lesion Detection (ULD) in computed tomography plays an essential role in computer-aided diagnosis. Promising ULD results have been reported by coarse-to-fine two-stage detection approaches, but such two-stage ULD methods still suffer from issues like imbalance of positive v.s. negative anchors during object proposal and insufficient supervision problem during localization regression and classification of the region of interest (RoI) proposals. While leveraging pseudo segmentation masks such as bounding map (BM) can reduce the above issues to some degree, it is still an open problem to effectively handle the diverse lesion shapes and sizes in ULD. In this paper we propose a BM-based conditional training for two-stage ULD, which can (i) reduce positive vs. negative anchor imbalance via a BM-based conditioning (BMC) mechanism for anchor sampling instead of traditional IoU-based rule; and (ii) adaptively compute size-adaptive BM (ABM) from lesion bounding-box, which is used for improving lesion localization accuracy via ABM-supervised segmentation.\n",
      "\n",
      "we propose a novel training mechanism for ULD to effectively reduce positive vs. negative anchor imbalance via a BM-based conditioning (BMC) mechanism in stage-1 and improve lesion localization accuracy by leveraging a size-adaptive BM (ABM) for supervising the segmentation branch in stage-2. \n",
      "---\n",
      "262\n",
      "In this paper, we propose a coherent cooperative learning framework based on transfer learning for unsupervised cross-domain classification.\n",
      "\n",
      "In the practical application of medical image analysis, due to the different data distributions of source domain and target domain and the lack of the labels of target domain, domain adaptation for unsupervised cross-domain classification attracts widespread attention. However, current methods take knowledge transfer model and classification model as two separate training stages, which inadequately considers and utilizes the intrinsic information interaction between module\n",
      "---\n",
      "255\n",
      "Recently, the contextual features attract increasing attention, due to the complementary information they provide. Clinically, such contextual features refer to the features of nodule’s surrounding structures, such that (together with nodule’s features) they can expose discriminate patterns for the malignant/benign, such as vascular convergence and fissural attachment. To leverage such contextual features, we propose a Context Attention Network (CA-Net) which extracts both nodule’s and contextual features and then effectively fuses them during malignancy/benignity classification. To accurately identify the contextual features that contain structures distorted/attached by the nodule, we take the nodule’s features as a reference via an attention mechanism. Further, we propose a feature fusion module that can adaptively adjust the weights of nodule’s and contextual features across nodules.\n",
      "\n",
      "Recently, the contextual features attract increasing attention, due to the complementary information they provide. Clinically, such contextual features refer to the features of nodule’s surrounding structures, such that (together with nodule’s features) they can expose discriminate patterns for the malignant/benign, such as vascular convergence and fissural attachment. To leverage such contextual features, we propose a Context Attention Network (CA-Net) which extracts both nodule’s and contextual features and then effectively fuses them during malignancy/benignity classification\n",
      "---\n",
      "325\n",
      "Current methods for whole slide image (WSI) histopathology subregion classification and survival prediction rely on phenotype clustering from randomly sampled image tiles or on analyzing key tiles selected by experts from the much larger in size WSIs. These approaches do not capture the whole tissue region present in a histopathology image, also missing the spatial distribution of features that could be critical for good survival predictors. We propose a novel method that extracts a whole slide feature map (WSFM) in the first step and then uses it to train the survival prediction model.\n",
      "\n",
      "With large histopathology images being captured at an increasing rate and the development of deep learning models for analyzing such images, image-based computer-aided diagnosis based on segmentation and classification has rapidly expanded. Histopathology images capture tumor growth and morphology with great details, making them highly suitable for patient risk assessment. However, there is little work on histopathology image-based survival analysis because of the challenges involved. First, in contrast to the natural images where a label is learned for a relatively smaller image (100s or 1000s of pixels) input, in histopathology studies labels are learned from extremely large inputs (WSIs might be $$10^{10}$$ pixels). Thus, patient samples with corresponding labels are usually limited, while a massive number of parameters need to be optimized when using WSIs in deep learning-based approaches. Second, histopathology images have extremely heterogeneous structures and textures, and annotating specific regions to facilitate risk prediction is laborious and often unfeasible\n",
      "\n",
      "\n",
      "(For disease task: We focus on the Glioblastoma multiforme (GBM) brain cancer in our study.)\n",
      "---\n",
      "329\n",
      "In this paper, we propose a novel multi-label chest X-ray classification model that accurately classifies the image finding and also localizes the findings to their correct anatomical regions. \n",
      "\n",
      "Radiologists usually observe anatomical regions of chest X-ray images as well as the overall image before making a decision. However, most existing deep learning models only look at the entire X-ray image for classification, failing to utilize important anatomical information.\n",
      "---\n",
      "138\n",
      "Multi-view medical image analysis often depends on the combination of information from multiple views. However, differences in perspective or other forms of misalignment can make it difficult to combine views effectively, as registration is not always possible. Without registration, views can only be combined at a global feature level, by joining feature vectors after global pooling. We present a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps. We demonstrate this method on multi-view mammography and chest X-ray datasets. On both datasets, we find that a cross-view transformer that links spatial feature maps can outperform a baseline model that joins feature vectors after global pooling.\n",
      "\n",
      "Multi-view medical image analysis often depends on the combination of information from multiple views. However, differences in perspective or other forms of misalignment can make it difficult to combine views effectively, as registration is not always possible. Without registration, views can only be combined at a global feature level, by joining feature vectors after global pooling. We present a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps. We demonstrate this method on multi-view mammography and chest X-ray datasets. On both datasets, we find that a cross-view transformer that links spatial feature maps can outperform a baseline model that joins feature vectors after global pooling.\n",
      "---\n",
      "139\n",
      "\n",
      "Abstract\n",
      "Computational histopathology studies have shown that stain color variations considerably hamper the performance. Stain color variations indicate the slides exhibit greatly different color appearance due to the diversity of chemical stains, staining procedures, and slide scanners. Previous approaches tend to improve model robustness via data augmentation or stain color normalization. However, they still suffer from generalization to new domains with unseen stain colors. In this study, we address the issue of unseen color domain generalization in histopathology images by encouraging the model to adapt varied stain colors. To this end, we propose a novel data augmentation method, stain mix-up, which incorporates the stain colors of unseen domains into training data. Unlike previous mix-up methods employed in computer vision, the proposed method constructs the combination of stain colors without using any label information, hence enabling unsupervised domain generalization. Extensive experiments are conducted and demonstrate that our method is general enough to different tasks and stain methods, including H&E stains for tumor classification and hematological stains for bone marrow cell instance segmentation. The results validate that the proposed stain mix-up can significantly improves the performance on the unseen domains.\n",
      "\n",
      "Computer-aided diagnosis based on histopathology images, such as whole slide images (WSIs) and field of views (FoVs) of tissue sections, gains significant progress owing to the great success of machine learning algorithms in digital pathology. Tissue sections are typically stained with various stains to make tissues visible under the microscope. However, tissue manipulation, staining, and even scanning often result in substantial color appearance variations in histopathology images, and degrade machine learning algorithms due to the domain gap of colors. Thus, it is crucial to take color appearance variations into account when developing machine learning algorithms for histopathology image analysis. Specifically, two strategies are widely used, including 1) augmenting color patterns of training data to enhance model robustness; and 2) normalizing all histopathology images to a single color pattern so that the unfavorable impact of color variations in the subsequent process can be alleviated.\n",
      "---\n",
      "145\n",
      "Also a more general method, that is shown to work for classification and regression in this article \n",
      "This paper leverages a recently proposed normalizing-flow-based method to perform counterfactual inference upon a structural causal model (SCM), in order to achieve harmonization of such data. \n",
      "\n",
      "Deep learning models have shown great promise in medical imaging diagnostics [11] and predictive modeling with applications ranging from segmentation tasks [19] to more complex decision-support functions for phenotyping brain diseases and personalized prognosis. However deep learning models tend to have poor reproducibility across hospitals, scanners, and patient cohorts; these high-dimensional models tend to overfit to specific datasets and generalize poorly across training data [6]. One potential solution to the above problem is to train on very large and diverse databases but this can be prohibitive, because data may change frequently (e.g., new imaging devices are introduced) and gathering training labels for medical images is expensive. More importantly, even if it were possible to train a model on data that covers all possible variations across images, such a model would almost certainly sacrifice accuracy in favor of generalization—it would rely on coarse imaging features that are stable across, say imaging devices and patient populations, and might fail to capture more subtle and informative detail. Methods that can tackle heterogeneity in medical data without sacrificing predictive accuracy are needed, including methods for “data harmonization”, which would allow training a classifier on, say data from one site, and obtaining similar predictive accuracy on data from another site.\n",
      "---\n",
      "159\n",
      "This paper studies a practical yet challenging FL problem, named Federated Semi-supervised Learning (FSSL), which aims to learn a federated model by jointly utilizing the data from both labeled and unlabeled clients (i.e., hospitals). We present a novel approach for this problem, which improves over traditional consistency regularization mechanism with a new inter-client relation matching scheme. \n",
      "\n",
      "Federated learning (FL) has emerged with increasing popularity to collaborate distributed medical institutions for training deep networks. However, despite existing FL algorithms only allow the supervised training setting, most hospitals in realistic usually cannot afford the intricate data labeling due to absence of budget or expertise. This paper studies a practical yet challenging FL problem, named Federated Semi-supervised Learning (FSSL), which aims to learn a federated model by jointly utilizing the data from both labeled and unlabeled clients (i.e., hospitals). \n",
      "---\n",
      "160\n",
      "Skin cancer is one of the most deadly cancers worldwide. Yet, it can be reduced by early detection. Recent deep-learning methods have shown a dermatologist-level performance in skin cancer classification. Yet, this success demands a large amount of centralized data, which is oftentimes not available. Federated learning has been recently introduced to train machine learning models in a privacy-preserved distributed fashion demanding annotated data at the clients, which is usually expensive and not available, especially in the medical field. To this end, we propose $$\\texttt {FedPerl}$$, a semi-supervised federated learning method that utilizes peer learning from social sciences and ensemble averaging from committee machines to build communities and encourage its members to learn from each other such that they produce more accurate pseudo labels. We also propose the peer anonymization (PA) technique as a core component of $$\\texttt {FedPerl}$$. PA preserves privacy and reduces the communication cost while maintaining the performance without additional complexity. \n",
      "\n",
      "Skin cancer is one of the most deadly cancers worldwide. Yet, it can be reduced by early detection. Recent deep-learning methods have shown a dermatologist-level performance in skin cancer classification. Yet, this success demands a large amount of centralized data, which is oftentimes not available. Federated learning has been recently introduced to train machine learning models in a privacy-preserved distributed fashion demanding annotated data at the clients, which is usually expensive and not available, especially in the medical field. To this end, we propose $$\\texttt {FedPerl}$$, a semi-supervised federated learning method that utilizes peer learning from social sciences and ensemble averaging from committee machines to build communities and encourage its members to learn from each other such that they produce more accurate pseudo labels. We also propose the peer anonymization (PA) technique as a core component of $$\\texttt {FedPerl}$$. PA preserves privacy and reduces the communication cost while maintaining the performance without additional complexity. \n",
      "---\n",
      "161\n",
      "Nowadays, deep learning methods with large-scale datasets can produce clinically useful models for computer-aided diagnosis. However, the privacy and ethical concerns are increasingly critical, which make it difficult to collect large quantities of data from multiple institutions. Federated Learning (FL) provides a promising decentralized solution to train model collaboratively by exchanging client models instead of private data. However, the server aggregation of existing FL methods is observed to degrade the model performance in real-world medical FL setting, which is termed as retrogress. To address this problem, we propose a personalized retrogress-resilient framework to produce a superior personalized model for each client. Specifically, we devise a Progressive Fourier Aggregation (PFA) at the server to achieve more stable and effective global knowledge gathering by integrating client models from low-frequency to high-frequency gradually. \n",
      "\n",
      "Recent years have witnessed the superior performance of deep learning techniques in the field of computer-aided diagnosis [2, 6, 27]. By collecting large quantities of data from multiple institutions, tailored deep learning methods achieved great success and have been applied in the clinical practice to alleviate the workload of physicians [5, 12, 13]. However, this is not a sustainable way to develop future intelligent healthcare systems, where the patient privacy and ethical concerns impede constructing centralized datasets with increasing size\n",
      "---\n",
      "164\n",
      "A label-efficient paradigm in computer vision is based on self-supervised contrastive pre-training on unlabeled data followed by fine-tuning with a small number of labels. Making practical use of a federated computing environment in the clinical domain and learning on medical images poses specific challenges. In this work, we propose FedMoCo, a robust federated contrastive learning (FCL) framework, which makes efficient use of decentralized unlabeled medical data. \n",
      "\n",
      "Recent studies in self-supervised learning (SSL) [21] have led to a renaissance of research on contrastive learning (CL) [1]. Self-supervised or unsupervised CL aims to learn transferable representations from unlabeled data. In a CL framework, a model is first pre-trained on unlabeled data in a self-supervised fashion via a contrastive loss, and then fine-tuned on labeled data. Utilizing the state-of-the-art (SOTA) CL frameworks [3, 8, 16, 24], a model trained with only unlabeled data plus a small amount of labeled data can achieve comparable performance with the same model trained with a large amount of labeled data on various downstream tasks.\n",
      "\n",
      "As a data-driven approach, deep learning has fueled many breakthroughs in medical image analysis (MIA).\n",
      "---\n",
      "172\n",
      "Medical image datasets are hard to collect, expensive to label, and often highly imbalanced. The last issue is underestimated, as typical average metrics hardly reveal that the often very important minority classes have a very low accuracy. In this paper, we address this problem by a feature embedding that balances the classes using contrastive learning as an alternative to the common cross-entropy loss.\n",
      "\n",
      "Medical image datasets are hard to collect, expensive to label, and often highly imbalanced. The last issue is underestimated, as typical average metrics hardly reveal that the often very important minority classes have a very low accuracy. In this paper, we address this problem by a feature embedding that balances the classes using contrastive learning as an alternative to the common cross-entropy loss.\n",
      "---\n",
      "174\n",
      "In this paper, we propose a novel Explainable Fuzzy Bag-of-Words (XFBoW) feature extraction model, for the classification of weakly annotated WCE images. A comparative advantage of the proposed model over state-of-the-art feature extractors is that it can provide an explainable classification outcome, even with conventional classification schemes, such as Support Vector Machines. The explanations that can be derived are based on the similarity of the image content with the content of the training images, used for the construction of the model. The feature extraction process relies on data clustering and fuzzy sets. Clustering is used to encode the image content into visual words. These words are subsequently used for the formation of fuzzy sets to enable a linguistic characterization of similarities with the training images. A state-of-the-art Brain Storm Optimization algorithm is used as an optimizer to define the most appropriate number of visual words and fuzzy sets and also the fittest parameters of the classifier, in order to optimally classify the WCE images. The training of XFBoW is performed using only image-level, semantic labels instead of detailed, pixel-level annotations. The proposed method is investigated on real datasets that include a variety of GI abnormalities. The results show that XFBoW outperforms several state-of-the-art methods, while providing the advantage of explainability.\n",
      "\n",
      "In this paper, we propose a novel Explainable Fuzzy Bag-of-Words (XFBoW) feature extraction model, for the classification of weakly annotated WCE images. A comparative advantage of the proposed model over state-of-the-art feature extractors is that it can provide an explainable classification outcome, even with conventional classification schemes, such as Support Vector Machines. The explanations that can be derived are based on the similarity of the image content with the content of the training images, used for the construction of the model. The feature extraction process relies on data clustering and fuzzy sets. Clustering is used to encode the image content into visual words. These words are subsequently used for the formation of fuzzy sets to enable a linguistic characterization of similarities with the training images. A state-of-the-art Brain Storm Optimization algorithm is used as an optimizer to define the most appropriate number of visual words and fuzzy sets and also the fittest parameters of the classifier, in order to optimally classify the WCE images. The training of XFBoW is performed using only image-level, semantic labels instead of detailed, pixel-level annotations. The proposed method is investigated on real datasets that include a variety of GI abnormalities. The results show that XFBoW outperforms several state-of-the-art methods, while providing the advantage of explainability.\n",
      "---\n",
      "180\n",
      "Explainability is a key feature for computer-aided diagnosis systems. This property not only helps doctors understand their decisions, but also allows less experienced practitioners to improve their knowledge. Skin cancer diagnosis is a field where explainability is of critical importance, as lesions of different classes often exhibit confounding characteristics. This work proposes a deep neural network (DNN) for skin cancer diagnosis that provides explainability through content-based image retrieval. We explore several state-of-the-art approaches to improve the feature space learned by the DNN, namely contrastive, distillation, and triplet losses.\n",
      "\n",
      "Explainability is a key feature for computer-aided diagnosis systems. This property not only helps doctors understand their decisions, but also allows less experienced practitioners to improve their knowledge. Skin cancer diagnosis is a field where explainability is of critical importance, as lesions of different classes often exhibit confounding characteristics. This work proposes a deep neural network (DNN) for skin cancer diagnosis that provides explainability through content-based image retrieval. We explore several state-of-the-art approaches to improve the feature space learned by the DNN, namely contrastive, distillation, and triplet losses.\n",
      "---\n",
      "69\n",
      "Abstract\n",
      "Nowadays, there is an urgent requirement of self-supervised learning (SSL) on whole slide pathological images (WSIs) to relieve the demand of finely expert annotations. However, the performance of SSL algorithms on WSIs has long lagged behind their supervised counterparts. To close this gap, in this paper, we fully explore the intrinsic characteristics of WSIs and propose SSLP: Spatial Guided Self-supervised Learning on Pathological Images. We argue the patch-wise spatial proximity is a significant characteristic of WSIs, if properly employed, shall provide abundant supervision for free. Specifically, we explore three semantic invariance from 1) self-invariance: the same patch of different augmented views, 2) intra-invariance: the patches within spatial neighbors and 3) inter-invariance: their corresponding neighbors in the feature space. As a result, our SSLP model achieves $$82.9\\%$$ accuracy and $$85.7\\%$$ AUC on CAMELYON linear classification and $$95.2\\%$$ accuracy fine-tuning on cross-disease classification on NCTCRC, which outperforms previous state-of-the-art algorithm and matches the performance of a supervised counterpart.\n",
      "\n",
      "Abstract\n",
      "Nowadays, there is an urgent requirement of self-supervised learning (SSL) on whole slide pathological images (WSIs) to relieve the demand of finely expert annotations. However, the performance of SSL algorithms on WSIs has long lagged behind their supervised counterparts. To close this gap, in this paper, we fully explore the intrinsic characteristics of WSIs and propose SSLP: Spatial Guided Self-supervised Learning on Pathological Images. We argue the patch-wise spatial proximity is a significant characteristic of WSIs, if properly employed, shall provide abundant supervision for free. Specifically, we explore three semantic invariance from 1) self-invariance: the same patch of different augmented views, 2) intra-invariance: the patches within spatial neighbors and 3) inter-invariance: their corresponding neighbors in the feature space. As a result, our SSLP model achieves $$82.9\\%$$ accuracy and $$85.7\\%$$ AUC on CAMELYON linear classification and $$95.2\\%$$ accuracy fine-tuning on cross-disease classification on NCTCRC, which outperforms previous state-of-the-art algorithm and matches the performance of a supervised counterpart.\n",
      "---\n",
      "72\n",
      "Radiomics can quantify the properties of regions of interest in medical image data. Classically, they account for pre-defined statistics of shape, texture, and other low-level image features. Alternatively, deep learning-based representations are derived from supervised learning but require expensive annotations and often suffer from overfitting and data imbalance issues. In this work, we address the challenge of learning the representation of a 3D medical image for an effective quantification under data imbalance. We propose a self-supervised representation learning framework to learn high-level features of 3D volumes as a complement to existing radiomics features. Specifically, we demonstrate how to learn image representations in a self-supervised fashion using a 3D Siamese network. More importantly, we deal with data imbalance by exploiting two unsupervised strategies: a) sample re-weighting, and b) balancing the composition of training batches. When combining the learned self-supervised feature with traditional radiomics, we show significant improvement in brain tumor classification and lung cancer staging tasks covering MRI and CT imaging modalities. Codes are available in https://​github.​com/​hongweilibran/​imbalanced-SSL.\n",
      "\n",
      "Great advances have been achieved in supervised deep learning, reaching expert-level performance on some considerably challenging applications [11]. However, supervised methods for image classification commonly require relatively large-scale datasets with ground-truth labels which is time- and resource-consuming in the medical field. \n",
      "---\n",
      "73\n",
      "Self-supervised learning provides a possible solution to extract effective visual representations from unlabeled histopathological images. However, existing methods either fail to make good use of domain-specific knowledge, or rely on side information like spatial proximity and magnification. In this paper, we propose CS-CO, a hybrid self-supervised visual representation learning method tailored for histopathological images, which integrates advantages of both generative and discriminative models. The proposed method consists of two self-supervised learning stages: cross-stain prediction (CS) and contrastive learning (CO), both of which are designed based on domain-specific knowledge and do not require side information. A novel data augmentation approach, stain vector perturbation, is specifically proposed to serve contrastive learning. Experimental results on the public dataset NCT-CRC-HE-100K demonstrate the superiority of the proposed method for histopathological image visual representation. Under the common linear evaluation protocol, our method achieves 0.915 eight-class classification accuracy with only 1,000 labeled data, which is about 1.3% higher than the fully-supervised ResNet18 classifier trained with the whole 89,434 labeled training data. Our code is available at https://​github.​com/​easonyang1996/​CS-CO.\n",
      "\n",
      "Self-supervised learning provides a possible solution to extract effective visual representations from unlabeled histopathological images. However, existing methods either fail to make good use of domain-specific knowledge, or rely on side information like spatial proximity and magnification. In this paper, we propose CS-CO, a hybrid self-supervised visual representation learning method tailored for histopathological images, which integrates advantages of both generative and discriminative models. The proposed method consists of two self-supervised learning stages: cross-stain prediction (CS) and contrastive learning (CO), both of which are designed based on domain-specific knowledge and do not require side information. A novel data augmentation approach, stain vector perturbation, is specifically proposed to serve contrastive learning. Experimental results on the public dataset NCT-CRC-HE-100K demonstrate the superiority of the proposed method for histopathological image visual representation. Under the common linear evaluation protocol, our method achieves 0.915 eight-class classification accuracy with only 1,000 labeled data, which is about 1.3% higher than the fully-supervised ResNet18 classifier trained with the whole 89,434 labeled training data. Our code is available at https://​github.​com/​easonyang1996/​CS-CO.\n",
      "---\n",
      "78\n",
      "Abstract\n",
      "Contrastive learning is a key technique of modern self-supervised learning. The broader accessibility of earlier approaches is hindered by the need of heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which accommodate for large-scale negative samples or momentum. The more recent SimSiam approach addresses such key limitations via stop-gradient without momentum encoders. In medical image analysis, multiple instances can be achieved from the same patient or tissue. Inspired by these advances, we propose a simple triplet representation learning (SimTriplet) approach on pathological images. The contribution of the paper is three-fold: (1) The proposed SimTriplet method takes advantage of the multi-view nature of medical images beyond self-augmentation; (2) The method maximizes both intra-sample and inter-sample similarities via triplets from positive pairs, without using negative samples; and (3) The recent mix precision training is employed to advance the training by only using a single GPU with 16 GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58% better performance compared with supervised learning. It also achieved 2.13% better performance compared with SimSiam. Our proposed SimTriplet can achieve decent performance using only 1% labeled data. The code and data are available at https://​github.​com/​hrlblab/​SimTriplet.\n",
      "\n",
      "Abstract\n",
      "Contrastive learning is a key technique of modern self-supervised learning. The broader accessibility of earlier approaches is hindered by the need of heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which accommodate for large-scale negative samples or momentum. The more recent SimSiam approach addresses such key limitations via stop-gradient without momentum encoders. In medical image analysis, multiple instances can be achieved from the same patient or tissue. Inspired by these advances, we propose a simple triplet representation learning (SimTriplet) approach on pathological images. The contribution of the paper is three-fold: (1) The proposed SimTriplet method takes advantage of the multi-view nature of medical images beyond self-augmentation; (2) The method maximizes both intra-sample and inter-sample similarities via triplets from positive pairs, without using negative samples; and (3) The recent mix precision training is employed to advance the training by only using a single GPU with 16 GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58% better performance compared with supervised learning. It also achieved 2.13% better performance compared with SimSiam. Our proposed SimTriplet can achieve decent performance using only 1% labeled data. The code and data are available at https://​github.​com/​hrlblab/​SimTriplet.\n",
      "\n",
      "We extracted image patches from seven melanoma skin cancer \n",
      "---\n",
      "92\n",
      "Tumor classification is important for decision support of precision medicine. Computer-aided diagnosis by convolutional neural networks relies on a large amount of annotated dataset, which is costly sometimes. To solve the poor predictive ability caused by tumor heterogeneity and inadequate labeled image data, a self-supervised learning method combined with radiomics is proposed to learn rich visual representation about tumors without human supervision. A self-supervised pretext task, namely “Radiomics-Deep Feature Correspondence”, is formulated to maximize agreement between radiomics view and deep learning view of the same sample in the latent space. The presented self-supervised model is evaluated on two public medical image datasets of thyroid nodule and kidney tumor and achieves high score on linear evaluations. Furthermore, fine-tuning the pre-trained network leads to a better score than the train-from-scratch models on the tumor classification task and shows label-efficient performance using small training datasets. This shows injecting radiomics prior knowledge about tumors into the representation space can build a more powerful self-supervised method.\n",
      "\n",
      "Deep convolutional neural networks (CNNs) have made major breakthroughs in the past few years, largely driven by increased computing power and massive labeled datasets. Benefitting from the huge advances of deep learning in image classification, computer-aided medical diagnostics has achieved great success [3]. Precise prediction of tumor type can help doctors recognize and interpret the subtle difference between different kinds of medical images. Moreover, it is critical to decision support of personalized cancer treatment for patients.\n",
      "---\n",
      "94\n",
      "Abstract\n",
      "We propose and demonstrate a representation learning approach by maximizing the mutual information between local features of images and text. The goal of this approach is to learn useful image representations by taking advantage of the rich information contained in the free text that describes the findings in the image. Our method trains image and text encoders by encouraging the resulting representations to exhibit high local mutual information. We make use of recent advances in mutual information estimation with neural network discriminators. We argue that the sum of local mutual information is typically a lower bound on the global mutual information. Our experimental results in the downstream image classification tasks demonstrate the advantages of using local features for image-text representation learning.\n",
      "\n",
      "We present a novel approach for image-text representation learning by maximizing the mutual information between local features of the images and the text. In the context of medical imaging, the images could be, for example, radiographs and the text could be radiology reports that capture radiologists’ impressions of the images. A large number of such image-text pairs are generated in the clinical workflow every day [7, 13]. Jointly learning from images and raw text can support a leap in the quality of medical vision models by taking advantage of existing expert descriptions of the images.\n",
      "---\n",
      "102\n",
      "Abstract\n",
      "Given a population longitudinal neuroimaging measurements defined on a brain network, exploiting temporal dependencies within the sequence of data and corresponding latent variables defined on the graph (i.e., network encoding relationships between regions of interest (ROI)) can highly benefit characterizing the brain. Here, it is important to distinguish time-variant (e.g., longitudinal measures) and time-invariant (e.g., gender) components to analyze them individually. For this, we propose an innovative and ground-breaking Disentangled Sequential Graph Autoencoder which leverages the Sequential Variational Autoencoder (SVAE), graph convolution and semi-supervising framework together to learn a latent space composed of time-variant and time-invariant latent variables to characterize disentangled representation of the measurements over the entire ROIs. Incorporating target information in the decoder with a supervised loss let us achieve more effective representation learning towards improved classification. We validate our proposed method on the longitudinal cortical thickness data from Alzheimer’s Disease Neuroimaging Initiative (ADNI) study. Our method outperforms baselines with traditional techniques demonstrating benefits for effective longitudinal data representation for predicting labels and longitudinal data generation.\n",
      "\n",
      "Abstract\n",
      "Given a population longitudinal neuroimaging measurements defined on a brain network, exploiting temporal dependencies within the sequence of data and corresponding latent variables defined on the graph (i.e., network encoding relationships between regions of interest (ROI)) can highly benefit characterizing the brain. Here, it is important to distinguish time-variant (e.g., longitudinal measures) and time-invariant (e.g., gender) components to analyze them individually. For this, we propose an innovative and ground-breaking Disentangled Sequential Graph Autoencoder which leverages the Sequential Variational Autoencoder (SVAE), graph convolution and semi-supervising framework together to learn a latent space composed of time-variant and time-invariant latent variables to characterize disentangled representation of the measurements over the entire ROIs. Incorporating target information in the decoder with a supervised loss let us achieve more effective representation learning towards improved classification. We validate our proposed method on the longitudinal cortical thickness data from Alzheimer’s Disease Neuroimaging Initiative (ADNI) study. Our method outperforms baselines with traditional techniques demonstrating benefits for effective longitudinal data representation for predicting labels and longitudinal data generation.\n",
      "---\n",
      "308\n",
      "Image anomaly detection methods that learn normal appearance from only healthy data have shown promising results recently. We propose an alternative to image reconstruction-based and image embedding-based methods and propose a new self-supervised method to tackle pathological anomaly detection.\n",
      "\n",
      "Doctors such as radiologists and cardiologists, along with allied imaging specialists such as sonographers shoulder the heavy responsibility of making complex diagnoses. Their decisions often determine patient treatment. Unfortunately, diagnostic errors lead to death or disability almost twice as often as any other medical error\n",
      "\n",
      "However, detecting arbitrary irregularities, without having any predefined target classes, remains an unsolved problem.\n",
      "---\n",
      "297\n",
      "a novel data augmentation method is proposed to effectively alleviate the over-fitting issue, not in the input space but in the logit space. \n",
      "\n",
      "Successful application of deep learning often depends on large amount of training data. However in practical medical image analysis, available training data are often limited, often causing over-fitting during model training. In this paper, a novel data augmentation method is proposed to effectively alleviate the over-fitting issue, not in the input space but in the logit space. \n",
      "---\n",
      "266\n",
      "we propose a novel self-supervised representation learning method, called Constrained Contrastive Distribution learning for anomaly detection (CCD), which learns fine-grained feature representations by simultaneously predicting the distribution of augmented data and image contexts using contrastive learning with pretext constraints. \n",
      "\n",
      "UAD has two main advantages over its fully supervised counterpart. Firstly, it is able to directly leverage large datasets available from health screening programs that contain mostly normal image samples, avoiding the costly manual labelling of abnormal samples and the subsequent issues involved in training with extremely class-imbalanced data. Further, UAD approaches can potentially detect and localise any type of lesions that deviate from the normal patterns. One significant challenge faced by UAD methods is how to learn effective low-dimensional image representations to detect and localise subtle abnormalities, generally consisting of small lesions. \n",
      "---\n",
      "257\n",
      "Learning disease-related representations plays a critical role in image-based cancer diagnosis, due to its trustworthy, interpretable and good generalization power. A good representation should not only be disentangled from the disease-irrelevant features, but also incorporate the information of lesion’s attributes (e.g., shape, margin) that are often identified first during cancer diagnosis clinically. To learn such a representation, we propose a Disentangle Auto-Encoder with Graph Convolutional Network (DAE-GCN), which adopts a disentangling mechanism with the guidance of a GCN model in the AE-based framework. \n",
      "\n",
      "For better representation learning, the disentanglement mechanism has been proved to be an effective way [1, 3, 12], since such a mechanism prompts different independent latent units to encode different independent ground truth generation factors that vary in the data [1]. Based on the above, to capture the disease-related features without mixing other irrelevant information, in this paper we propose a Disentangle Auto-Encoder with Graph Convolutional Network (DAE-GCN), which incorporates a disentangling mechanism into an AE framework, equipped with attribution data during training stage (the attributes are not provided during the test). \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#column 1: id\n",
    "#column 6: task\n",
    "#column 7: justification\n",
    "\n",
    "demo = df.iloc[:,[1,6,7]]\n",
    "demo.reset_index()\n",
    "\n",
    "\n",
    "for index in range(len(demo)):\n",
    "    print(demo.iloc[index, 0])\n",
    "    print(demo.iloc[index, 1])\n",
    "    print()\n",
    "    print(demo.iloc[index, 2])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682da28",
   "metadata": {},
   "source": [
    "## Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9509a68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles who acknowledge funding etc:  58\n",
      "69\n",
      "Shanghai Jiao Tong University, Shanghai, China\n",
      "(2)\n",
      "MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China\n",
      "\n",
      "This work was supported in part by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), 111 project (BP0719010), Shanghai Science and Technology Committee (18DZ2270700) and Shanghai Jiao Tong University Science and Technology Innovation Special Fund (ZH2018ZDA17).\n",
      "---\n",
      "72\n",
      "Department of Computer Science, Technical University of Munich, Munich, Germany\n",
      "(2)\n",
      "Department of Quantitative Biomedicine, University of Zurich, Zürich, Switzerland\n",
      "(3)\n",
      "ETH Zurich, Zürich, Switzerland\n",
      "(4)\n",
      "Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China\n",
      "(5)\n",
      "Faculty of Information Technology, Macau University of Science and Technology, Macao, China\n",
      "(6)\n",
      "Klinikum rechts der Isar, Technical University of Munich, Munich, Germany \n",
      "\n",
      "This work was supported by Helmut Horten Foundation. I. E. was supported by the TRABIT network under the EU Marie Sklodowska-Curie program (Grant ID: 765148). S. L. was supported by the Faculty Research Grant (NO. FRG-18-020-FI) at Macau University of Science and Technology. B. W. and B. M. were supported through the DFG, SFB-824, subproject B12. K. C. was supported by Clinical Research Priority Program (CRPP) Grant on Artificial Intelligence in Oncological Imaging Network, University of Zurich.\n",
      "---\n",
      "73\n",
      "\n",
      "Ministry of Education Key Laboratory of Bioinformatics, Bioinformatics Division, Beijing National Research Center for Information Science and Technology, Department of Automation, Tsinghua University, Beijing, 100084, China\n",
      "(2)\n",
      "Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, China\n",
      "(3)\n",
      "Department of Hepatobiliary and Pancreatic Surgery, The Affiliated Hospital of Qingdao University, Qingdao, 266000, Shandong, China\n",
      "\n",
      "This work was partially supported by the National Key Research and Development Program of China (No. 2018YFC0910404), the National Natural Science Foundation of China (Nos. 61873141, 61721003), the Shanghai Municipal Science and Technology Major Project (No. 2017SHZDZX01), the Tsinghua-Fuzhou Institute for Data Technology, the Taishan Scholars Program of Shandong Province (No. 2019010668), and the Shandong Higher Education Young Science and Technology Support Program (No. 2020KJL005).\n",
      "---\n",
      "74\n",
      "(1)\n",
      "NeuroSpin, CEA Saclay, Université Paris-Saclay, Gif-sur-Yvette, France\n",
      "(2)\n",
      "LTCI, Télécom Paris, IPParis, Paris, France\n",
      "(3)\n",
      "Department of Neuropsychology, Johannes-Gutenberg University of Mainz, Mainz, Germany\n",
      "(4)\n",
      "Department of Neurosciences, Fondazione IRCCS, University of Milan, Milan, Italy\n",
      "(5)\n",
      "Université Grenoble Alpes, Inserm U1216, CHU Grenoble Alpe, Grenoble, France\n",
      "(6)\n",
      "Centre for Neuroimaging and Cognitive Genomics (NICOG), Galway, Ireland\n",
      "(7)\n",
      "Department of Neuroscience, University of Geneva, Geneva, Switzerland\n",
      "(8)\n",
      "Department of Psychiatry, Western Psychiatric Institute, University of Pittsburgh, Pittsburgh, USA\n",
      "(9)\n",
      "Department of Psychiatry, UC San Diego, San Diego, CA, USA\n",
      "\n",
      "This work was granted access to the HPC resources of IDRIS under the allocation 2020-AD011011854 made by GENCI.\n",
      "---\n",
      "78\n",
      "\n",
      "\n",
      "Vanderbilt University, Nashville, TN 37215, USA\n",
      "(2)\n",
      "Vanderbilt University Medical Center, Nashville, TN 37215, USA\n",
      "\n",
      "Dr. Wheless is funded by grants from the Skin Cancer Foundation and the Dermatology Foundation.\n",
      "---\n",
      "92\n",
      "LIST, Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, Nanjing, China\n",
      "(2)\n",
      "Centre de Recherche en Information Biomédicale Sino-Français (CRIBs), Rennes, France\n",
      "\n",
      "This research was supported by National Natural Science Foundation under grants (31571001, 61828101). We thank the Big Data Center of Southeast University for providing the GPUs to support the numerical calculations in this paper.\n",
      "---\n",
      "94\n",
      "(1)\n",
      "CSAIL, Massachusetts Institute of Technology, Cambridge, MA, USA\n",
      "(2)\n",
      "MIT Lincoln Laboratory, Lexington, MA, USA\n",
      "(3)\n",
      "Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, USA\n",
      "(4)\n",
      "Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA\n",
      "\n",
      "This work was supported in part by NIH NIBIB NAC P41EB015902, Wistron, IBM Watson, MIT Deshpande Center, MIT J-Clinic, MIT Lincoln Lab, and US Air Force.\n",
      "---\n",
      "102\n",
      "University of Texas at Arlington, Arlington, USA\n",
      "(2)\n",
      "Lawrence Berkeley National Laboratory, Berkeley, USA\n",
      "(3)\n",
      "Pohang University of Science and Technology, Pohang, South Korea\n",
      "(4)\n",
      "University of North Carolina, Chapel Hill, Chapel Hill, USA\n",
      "\n",
      "This work was supported by GAANN Doctoral Fellowships in Computer Science and Engineering at UTA sponsored by the U.S. Department of Education, NSF IIS CRII 1948510, NIH RF1 AG059312, NIH R03 AG070701, and IITP-2019-0-01906 funded by MSIT (AI Graduate School Program at POSTECH).\n",
      "---\n",
      "106\n",
      "\n",
      "Section for Image Computing, Technical University of Denmark, Kgs. Lyngby, Denmark\n",
      "(2)\n",
      "Physense, Department of Information and Communication Technologies, Universitat Pompeu Fabra, Barcelona, Spain\n",
      "(3)\n",
      "Department of Cardiology, Rigshospitalet, University of Copenhagen, Copenhagen, Denmark\n",
      "\n",
      "This work was supported by a PhD grant from the Technical University of Denmark - Department of Applied Mathematics and Computer Science (DTU Compute) and the Spanish Ministry of Science, Innovation and Universities under the Retos I+D Programme (RTI2018-101193-B-I00).\n",
      "---\n",
      "112\n",
      "\n",
      "Kyushu University, Fukuoka City, Japan\n",
      "(2)\n",
      "National Institute of Informatics, Tokyo, Japan\n",
      "(3)\n",
      "Kyoto Second Red Cross Hospital, Kyoto, Japan\n",
      "\n",
      "This work was supported by JSPS KAKENHI Grant Number JP20H04211 and AMED Grant Number JP20lk1010036h0002.\n",
      "---\n",
      "118\n",
      "Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China\n",
      "(2)\n",
      "Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China\n",
      "(3)\n",
      "Imsight AI Research Lab, Shenzhen, China\n",
      "(4)\n",
      "Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Beijing, China\n",
      "\n",
      "This work was supported by Key-Area Research and Development Program of Guangdong Province, China (2020B010165004), Hong Kong Innovation and Technology Fund (Project No. ITS/311/18FP and Project No. ITS/426/17FP.), and National Natural Science Foundation of China with Project No. U1813204.\n",
      "---\n",
      "122\n",
      "Image and Video Processing Laboratory, Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL 60208, USA\n",
      "(2)\n",
      "Depto. Ciencias de la Computacion e I.A., Universidad de Granada, 18071 Granada, Spain\n",
      "\n",
      "This work has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska Curie grant agreement No 860627 (CLARIFY Project) and also from the Spanish Ministry of Science and Innovation under project PID2019-105142RB-C22.\n",
      "---\n",
      "138\n",
      "\n",
      "Data Science Group, Faculty of Science, Radboud University, Nijmegen, The Netherlands\n",
      "\n",
      "The research leading to these results is part of the project “MARBLE”, funded from the EFRO/OP-Oost under grant number PROJ-00887. Some of the experiments were carried out on the Dutch national e-infrastructure with the support of SURF Cooperative.\n",
      "---\n",
      "139\n",
      "aetherAI, Taipei, Taiwan\n",
      "(2)\n",
      "National Yang Ming Chiao Tung University, Hsinchu, Taiwan\n",
      "\n",
      "We thank Wen-Chien Chou M.D.(National Taiwan University Hospital), Ta-Chuan Yu M.D.(National Taiwan University Hospital Yunlin Branch) and Poshing Lee M.D.(Department of Hematopathology, BioReference) for Hema dataset construction. This paper was supported in part by the Ministry of Science and Technology, Taiwan, under Grants MOST 110-2634-F-007-015 and MOST 109-2221-E-009-113-MY3.\n",
      "---\n",
      "145\n",
      "Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, USA\n",
      "(2)\n",
      "Center for Biomedical Image Computing and Analytics (CBICA), Philadelphia, USA\n",
      "(3)\n",
      "General Robotics, Automation, Sensing and Perception Laboratory (GRASP), Philadelphia, USA\n",
      "(4)\n",
      "Department of Radiology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, USA\n",
      "\n",
      "We thank Ben Glocker, Nick Pawlowski and Daniel C. Castro for suggestions. This work was supported by the National Institute on Aging (grant numbers RF1AG054409 and U01AG068057) and the National Institute of Mental Health (grant number R01MH112070). Pratik Chaudhari would like to acknowledge the support of the Amazon Web Services Machine Learning Research Award.\n",
      "---\n",
      "159\n",
      "Department of Computer Science and Engineering, The Chinese University of Hong Kong, Kowloon, Hong Kong SAR, China\n",
      "(2)\n",
      "Department of Computer Science and Engineering, Beihang University, Beijing, China\n",
      "(3)\n",
      "Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China\n",
      "\n",
      "The work described in this paper was supported in parts by the following grants: Key-Area Research and Development Program of Guangdong Province, China (2020B010165004), Hong Kong Innovation and Technology Fund (Project No. GHP/110/19SZ), Foundation of China with Project No. U1813204 and Shenzhen-HK Collaborative Development Zone.\n",
      "---\n",
      "160\n",
      "Computer Aided Medical Procedures, Technical University of Munich, Munich, Germany\n",
      "(2)\n",
      "Helmholtz AI, Helmholtz Zentrum München, Neuherberg, Germany\n",
      "(3)\n",
      "The Whiting School of Engineering, Johns Hopkins University, Baltimore, USA\n",
      "\n",
      "T.B. is financially supported by the German Academic Exchange Service (DAAD).\n",
      "---\n",
      "161\n",
      "Department of Electrical Engineering, City University of Hong Kong, Kowloon, Hong Kong, China\n",
      "\n",
      "This work is supported by Shenzhen-Hong Kong Innovation Circle Category D Project SGDX2019081623300177 (CityU 9240008) and CityU SRG 7005229.\n",
      "---\n",
      "164\n",
      "Department of Computer Science, University of Oxford, Oxford, UK\n",
      "\n",
      "We would like to thank Huawei Technologies Co., Ltd. for providing GPU computing service for this study.\n",
      "---\n",
      "170\n",
      "Case Western Reserve University, Cleveland, OH 44106, USA\n",
      "(2)\n",
      "Louis Stokes Cleveland VA Medical Center, Cleveland, OH 44106, USA\n",
      "\n",
      "Research supported by NCI (1U24CA199374-01, 1R01CA249992-01A1, 1R01CA202752-01A1, 1R01CA208236-01A1, 1R01CA216579-01A1, 1R01CA220581-01A1, 1R01CA257612-01A1, 1U01CA239055-01, 1U01CA248226-01, 1U54CA254566-01, 1F31CA216935-01A1), NHLBI (R01HL15127701A1), NIBIB (1R43EB028736-01), NCRR (1C06RR12463-01), DOD/CDMRP (W81XWH-19-1-0668, W81XWH-15-1-0558, W81XWH-20-1-0851, W81XWH-18-1-0440, W81XWH-20-1-0595, W81XWH-18-1-0404, CA200789), VA (IBX004121A Merit Review Award), the KPMP Glue Grant, the Ohio Third Frontier Technology Validation Fund, the CTSC of Cleveland (UL1TR0002548), the Wallace H. Coulter Foundation Program in the Department of Biomedical Engineering at Case Western Reserve University, as well as sponsored research agreements from Bristol Myers-Squibb, Boehringer-Ingelheim, and Astrazeneca. Content solely responsibility of the authors and does not necessarily represent the official views of the NIH, USDVA, DOD, or the United States Government.\n",
      "---\n",
      "172\n",
      "Department of Computer Science, University of Freiburg, Freiburg im Breisgau, Germany\n",
      "(2)\n",
      "CIBSS – Centre for Integrative Biological Signalling Studies, University of Freiburg, Freiburg im Breisgau, Germany\n",
      "\n",
      "This study was supported by the Excellence Strategy of the German Federal and State Governments, (CIBSS - EXC 2189).\n",
      "---\n",
      "173\n",
      "Institute for Ophthalmic Research, Tübingen, Germany\n",
      "(2)\n",
      "Tübingen AI Center, Tübingen, Germany\n",
      "(3)\n",
      "University Eye Clinic, University of Tübingen, 72076 Tübingen, Germany\n",
      "\n",
      "We thank Wieland Brendel for his support with BagNets. This research was supported by the German Ministry of Science and Education (BMBF, 01GQ1601 and 01IS18039A) and the German Science Foundation (BE5601/4-2 and EXC 2064, project number 390727645). Hanna Faber received research funding from the Junior Clinician Scientist Program of the Faculty of Medicine, Eberhard Karls University of Tübingen, Germany (application number 463–0–0). Additional funding was provided by Novartis AG through a research grant. The funding bodies did not have any influence in the study planning and design. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Indu Ilanchezian.\n",
      "---\n",
      "174\n",
      "University of Thessaly, Papasiopoulou str. 2-4, 35131 Lamia, Greece\n",
      "\n",
      "This work was supported in part by the grant No. 5024 of the Special Account of Research Grants of the University of Thessaly, Greece.\n",
      "---\n",
      "180\n",
      "Institute for Systems and Robotics, Instituto Superior Técnico, Lisbon, Portugal\n",
      "\n",
      "This work was supported by the FCT project and multi-year funding [CEECIND/ 00326/2017] and LARSyS - FCT Plurianual funding 2020–2023; and by a Google Research Award’21. The Titan Xp used in this project were donated by the NVIDIA Corporation.\n",
      "---\n",
      "255\n",
      "(1)\r\n",
      "Department of Computer Science and Technology, Peking University, Beijing, China\r\n",
      "(2)\r\n",
      "Peking University, Beijing, China\r\n",
      "(3)\r\n",
      "Deepwise AI Lab, Beijing, China\r\n",
      "(4)\r\n",
      "University of Hong Kong, Pokfulam, Hong Kong   This work was supported by MOST-2018AAA0102004, NSFC-61625201, and the Beijing Municipal Science and Technology Planning Project (Grant No. Z201100005620008).\n",
      "---\n",
      "257\n",
      "(1)\r\n",
      "Center for Data Science, Peking University, Beijing, China\r\n",
      "(2)\r\n",
      "Peking University, Beijing, China\r\n",
      "(3)\r\n",
      "Department of Computer Science and Technology, Peking University, Beijing, China\r\n",
      "(4)\r\n",
      "Deepwise AI Lab, Beijing, China\r\n",
      "(5)\r\n",
      "The University of Hong Kong, Pokfulam, Hong Kong   This work was supported by MOST-2018AAA0102004, NSFC-61625201 and ZheJiang Province Key Research & Development Program (No. 2020C03073).\n",
      "---\n",
      "258\n",
      "This work was supported in part by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), 111 project (BP0719010), Shanghai Science and Technology Committee (18DZ2270700) and Shanghai Jiao Tong University Science and Technology Innovation Special Fund (ZH2018ZDA17).\n",
      "---\n",
      "262\n",
      "(1)\n",
      "Shanghai Key Laboratory of Multidimensional Information Processing, School of Communication and Electronic Engineering, East China Normal University, Shanghai, China  This work was supported in part by 2030 National Key Research and Development Program of China (2018AAA0100500), the National Nature Science Foundation of China (no. 61773166), Projects of International Cooperation of Shanghai Municipal Science and Technology Committee (14DZ2260800), the Fundamental Research Funds for the Central Universities, and the ECNU Academic Innovation Promotion Program for Excellent Doctoral Students (YBNLTS2021-040).\n",
      "---\n",
      "266\n",
      "(1)\r\n",
      "Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), School of Biomedical Engineering & Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China\r\n",
      "(2)\r\n",
      "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China\r\n",
      "(3)\r\n",
      "School of Electronic, Electrical and Communication Engineering, University of the Chinese Academy of Science, Beijing, China\r\n",
      "(4)\r\n",
      "Healthcare Intelligence, AIC, DAMO Academy, Alibaba Group, Hangzhou, China   This research was supported in part by the Natural Science Foundation of China (grant 61732004), Youth Innovation Promotion Association CAS (grant 2018135) and Alibaba Group through Alibaba Innovative Research Program.\n",
      "---\n",
      "267\n",
      "(1)\n",
      "Department of Computer Science, Hong Kong Baptist University, kowloon, Hong Kong  This work was supported by the Health and Medical Research Fund Project under Grant 07180216. We acknowledge insightful discussion with Anthony W.H. CHAN. We also thank Vincent WS WONG, Grace LH WONG, and Howard H.W. LEUNG from the Chinese University of Hong Kong for help with data preparation.\n",
      "---\n",
      "268\n",
      "(1)\r\n",
      "Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong, China\r\n",
      "(2)\r\n",
      "Department of Information Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong, China\r\n",
      "(3)\r\n",
      "School of Informatics, Xiamen University, Xiamen, China\r\n",
      "(4)\r\n",
      "Department of Electrical Engineering, City University of Hong Kong, Kowloon, Hong Kong, China\r\n",
      "(5)\r\n",
      "Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China  The work described in this paper was supported by National Key R&D program of China with Grant No. 2019YFB1312400, Hong Kong RGC CRF grant C4063-18G, and Hong Kong RGC GRF grant #14211420.\n",
      "---\n",
      "269\n",
      "(1)\r\n",
      "Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, USA\r\n",
      "(2)\r\n",
      "Brainnetome Center & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China\r\n",
      "(3)\r\n",
      "State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing, 100678, China\r\n",
      "(4)\r\n",
      "School of Computer Science and Technology, East China Normal University, Shanghai, 200241, China   This work was finished when D. Yao was visiting the University of North Carolina at Chapel Hill. D. Yao and M. Liu was partly supported by NIH grant (No. AG041721). Z. Zhang was partly supported by the National Key Research and Development Program of China (No. 2016YFD0700100).\n",
      "---\n",
      "270\n",
      "(1)\n",
      "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 100149, China\n",
      "(2)\n",
      "NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China\n",
      "(3)\n",
      "Brainnetome Center, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China\n",
      "(4)\n",
      "CAS Center for Excellence of Brain Science and Intelligence Technology, Beijing, 100190, China   This work has been supported by the National Key Research and Development Program Grant 2018AAA0100400, the National Natural Science Foundation of China (NSFC) grants 61773376, 61836014, 61721004 and 31870984.\n",
      "---\n",
      "276\n",
      "(1)\n",
      "Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, USA\n",
      "(2)\n",
      "School of Biomedical Engineering, Southern Medical University, Guangzhou, 510515, China\n",
      "(3)\n",
      "Department of Geriatric Psychiatry, Shanghai Mental Health Center, Shanghai Jiao Tong University School of Medicine, Shanghai, 200030, China    H. Guan and M. Liu were partly supported by NIH grant (No. AG041721).\n",
      "---\n",
      "277\n",
      "(1)\n",
      "Johns Hopkins University, Baltimore, USA\n",
      "(2)\n",
      "PAII Inc., Bethesda, USA\n",
      "(3)\n",
      "PingAn Technology, Shenzhen, China\n",
      "(4)\n",
      "Changhai Hospital, Shanghai, China   Y. Xia—Work done during an internship at PAII Inc.\n",
      "---\n",
      "283\n",
      "(1)\r\n",
      "Bournemouth University, Poole, UK\r\n",
      "(2)\r\n",
      "University of Adelaide, Adelaide, Australia\r\n",
      "(3)\r\n",
      "BCN Medtech, Department of Information and Communication Technologies, Universitat Pompeu Fabra, Barcelona, Spain\r\n",
      "(4)\r\n",
      "Catalan Institution for Research and Advanced Studies (ICREA), Barcelona, Spain    This work was partially supported by a Marie Skłodowska-Curie Global Fellowship (No. 892297) and by Australian Research Council grants (DP180103232 and FT190100525).\n",
      "---\n",
      "284\n",
      "(1)\n",
      "Department of Medical Physics and Biomedical Engineering, University College London, London, UK\n",
      "(2)\n",
      "Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, UK\n",
      "(3)\n",
      "School of Biomedical Engineering and Imaging Sciences (BMEIS), King’s College London, London, UK\n",
      "(4)\n",
      "Department of Clinical and Experimental Epilepsy, UCL Queen Square Institute of Neurology, London, UK\n",
      "(5)\n",
      "Department of Clinical Neurophysiology, National Hospital for Neurology and Neurosurgery, London, UK\n",
      "\n",
      "This work is supported by the Engineering and Physical Sciences Research Council (EPSRC) [EP/R512400/1]. This work is additionally supported by the EPSRC-funded UCL Centre for Doctoral Training in Intelligent, Integrated Imaging in Healthcare (i4health) [EP/S021930/1] and the Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS, UCL) [203145Z/16/Z]. The data acquisition was supported by the National Institute of Neurological Disorders and Stroke [U01-NS090407].\n",
      "\n",
      "This publication represents, in part, independent research commissioned by the Wellcome Innovator Award [218380/Z/19/Z/]. The views expressed in this publication are those of the authors and not necessarily those of the Wellcome Trust.\n",
      "\n",
      "The weights for the 2D and 3D models were downloaded from TorchVision and https://​github.​com/​moabitcoin/​ig65m-pytorch, respectively.\n",
      "---\n",
      "286\n",
      "(1)\n",
      "School of Electronic Information and Communication, Huazhong University of Science and Technology, Wuhan, China\n",
      "(2)\n",
      "Britton Chance Center for Biomedical Photonics, Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China\n",
      "(3)\n",
      "MoE Key Laboratory for Biomedical Photonics, Collaborative Innovation Center for Biomedical Engineering, School of Engineering Sciences, Huazhong University of Science and Technology, Wuhan, China\n",
      "(4)\n",
      "Hong Kong University of Science and Technology, Hong Kong, China\n",
      "(5)\n",
      "Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China\n",
      "\n",
      "This work was supported by the National Natural Science Foundation of China (6187241762061160490), the project of Wuhan Science and Technology Bureau (2020010601012167), the Open Project of Wuhan National Laboratory for Optoelectronics (2018WNLOKF025), the Fundamental Research Funds for the Central Universities (2021XXJS033).\n",
      "---\n",
      "288\n",
      "(1)\n",
      "Institute of Intelligent Machines, HFIPS, Chinese Academy of Sciences, Hefei, China\n",
      "(2)\n",
      "University of Science and Technology of China, Hefei, China\n",
      "(3)\n",
      "School of Information Engineering, Zhengzhou University, Zhengzhou, China\n",
      "(4)\n",
      "Nullmax, Shanghai, China\n",
      "(5)\n",
      "The First Affiliated Hospital of USTC, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China\n",
      "\n",
      "This work is supported in part by the grant of NSFC (61804100, 61973294, 61806181), KRDP of Anhui Province (201904a05020086) and CAS (GJTD-2018-15).\n",
      "---\n",
      "290\n",
      "(1)\n",
      "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\n",
      "(2)\n",
      "Key Laboratory of Machine Intelligence and Advanced Computing, MOE, China\n",
      "(3)\n",
      "Pazhou Lab, Guangzhou, China\n",
      "\n",
      "This work is supported in part by the National Natural Science Foundation of China (grant No. 62071502, U1811461), the Guangdong Key Research and Development Program (grant No. 2020B1111190001, 2019B020228001), and the Meizhou Science and Technology Program (grant No. 2019A0102005).\n",
      "---\n",
      "291\n",
      "(1)\n",
      "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\n",
      "(2)\n",
      "Key Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, China\n",
      "\n",
      "This work is supported by the National Natural Science Foundation of China (No. 62071502, U1811461), the Guangdong Key Research and Development Program (No. 2020B1111190001, 2019B020228001), and the Meizhou Science and Technology Program (No. 2019A0102005).\n",
      "---\n",
      "294\n",
      "(1)\n",
      "National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, Shaanxi, China\n",
      "(2)\n",
      "Research and Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen, 518057, China\n",
      "(3)\n",
      "School of Biomedical Engineering, ShanghaiTech University, Shanghai, China\n",
      "(4)\n",
      "Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China\n",
      "\n",
      "This work was supported in part by the National Natural Science Foundation of China under Grants 61771397, in part by the CAAI-Huawei MindSpore Open Fund under Grants CAAIXSJLJJ-2020-005B, and in part by the China Postdoctoral Science Foundation under Grants BX2021333.\n",
      "---\n",
      "297\n",
      "(1)\n",
      "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\n",
      "(2)\n",
      "Key Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, China\n",
      "(3)\n",
      "Pazhou Lab, Guangzhou, China\n",
      "\n",
      "This work is supported by the National Natural Science Foundation of China (No. 62071502, U1811461), the Guangdong Key Research and Development Program (No. 2020B1111190001, 2019B020228001), and the Meizhou Science and Technology Program (No. 2019A0102005).\n",
      "---\n",
      "298\n",
      "(1)\n",
      "National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, Shaanxi, China\n",
      "(2)\n",
      "Research and Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen, 518057, China\n",
      "(3)\n",
      "School of Biomedical and Engineering, ShanghaiTech University, Shanghai, 201210, China\n",
      "\n",
      "This work was supported in part by the National Natural Science Foundation of China under Grants 61771397, in part by the CAAI-Huawei MindSpore Open Fund under Grants CAAIXSJLJJ-2020-005B, and in part by the China Postdoctoral Science Foundation under Grants BX2021333.\n",
      "---\n",
      "300\n",
      "(1)\n",
      "Department of Brain and Cognitive Engineering, Korea University, Seoul, Republic of Korea\n",
      "(2)\n",
      "Department of Artificial Intelligence, Korea University, Seoul, Republic of Korea\n",
      "\n",
      "This work was supported by National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2019R1A2C1006543) and partially by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00079, Artificial Intelligence Graduate School Program (Korea University)).\n",
      "---\n",
      "302\n",
      "(1)\n",
      "Xiamen University, Xiamen, China\n",
      "(2)\n",
      "Tencent Jarvis Lab, Shenzhen, China\n",
      "\n",
      "This work was supported by the Fundamental Research Funds for the Central Universities (Grant No. 20720190012), Key-Area Research and Development Program of Guangdong Province, China (No. 2018B010111001), and Scientific  and Technical Innovation 2030 - “New Generation Artificial Intelligence” Project (No. 2020AAA0104100).\n",
      "---\n",
      "308\n",
      "(1)\n",
      "Imperial College London, SW7 2AZ London, UK\n",
      "(2)\n",
      "King’s College London, St Thomas’ Hospital, SE1 7EH London, UK\n",
      "(3)\n",
      "Friedrich–Alexander University Erlangen–Nürnberg, Erlangen, Germany\n",
      "\n",
      "Support from Wellcome Trust IEH Award iFind project [102431] and UK Research and Innovation London Medical Imaging and Artificial Intelligence Centre for Value Based Healthcare. JT was supported by the ICL President’s Scholarship.\n",
      "---\n",
      "309\n",
      "(1)\n",
      "Department of Computer Science, Oklahoma State University, Stillwater, OK, USA\n",
      "(2)\n",
      "Oklahoma Animal Disease Diagnostic Laboratory, College of Veterinary Medicine, Oklahoma State University, Stillwater, OK, USA\n",
      "\n",
      "This research was supported in part by the US Department of Agriculture (USDA) grants AP20VSD and B000C011.\n",
      "\n",
      "We thank Dr. Kitty Cardwell and Dr. Andres Espindola (Institute of Biosecurity and Microbial Forensics, Oklahoma State University) for providing access and assisting with use of the MiFi platform.\n",
      "---\n",
      "310\n",
      "(1)\n",
      "Institute for Infocomm Research, A*STAR, Singapore, Singapore\n",
      "(2)\n",
      "College of Computer Science, Sichuan University, Chengdu, Sichuan, China\n",
      "\n",
      "Research efforts were supported by funding and infrastructure for deep learning and medical imaging research from the Institute for Infocomm Research, Science and Engineering Research Council, A*STAR, Singapore. We thank Victor Getty, Vijay Chandrasekhar and Ivan Ho Mien from the Institute for Infocomm Research, A*STAR for their valuable inputs. We also acknowledge insightful discussions with Jayashree Kalpathy-Cramer at the Massachusetts General Hospital, Boston, USA\n",
      "---\n",
      "311\n",
      "Center for Computational Natural Sciences and Bioinformatics, IIIT Hyderabad, Hyderabad, India\n",
      "\n",
      "This study was supported by funding from IHub-Data and IIIT Hyderabad. We would also like to thank Dr. K Sudarsana Reddy for the discussions we had regarding the theoretical correctness of the method presented.\n",
      "---\n",
      "312\n",
      "(1)\n",
      "Department of Science and Technology, Linkoping University, Linköping, Sweden\n",
      "(2)\n",
      "Center for Medical Image Science and Visualization, Linkoping University, Linköping, Sweden\n",
      "(3)\n",
      "Sectra AB, Linköping, Sweden\n",
      "\n",
      "We would like to thank Martin Lindvall for the interesting discussions and insights into the use of cancer type-specific primary tumor data for lymph node metastasis detection, and Panagiotis Tsirikoglou for the suggestions in results analysis. This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, the strategic research environment ELLIIT, and the VINNOVA grant 2017-02447 for the Analytic Imaging Diagnostics Arena (AIDA).\n",
      "---\n",
      "313\n",
      "Intelligent Systems Program, School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA\n",
      "(2)\n",
      "Department of Radiology, University of Pittsburgh School of Medicine, Pittsburgh, PA, USA\n",
      "(3)\n",
      "Magee-Womens Hospital, University of Pittsburgh Medical Center, Pittsburgh, PA, USA\n",
      "(4)\n",
      "Department of Electrical and Computer Engineering, Swanson School of Engineering, University of Pittsburgh, Pittsburgh, PA, USA\n",
      "(5)\n",
      "Department of Biomedical Informatics and Department of Bioengineering, University of Pittsburgh, Pittsburgh, PA, USA\n",
      "\n",
      "This work was supported by National Institutes of Health grants (1R01CA193603, 3R01CA193603-03S1, and 1R01CA218405), the UPMC Hillman Cancer Center Developmental Pilot Program, and an Amazon Machine Learning Research Award. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation (NSF) grant number ACI-1548562. Specifically, it used the Bridges-2 system, which is supported by NSF award number ACI-1928147, at the Pittsburgh Supercomputing Center (PSC).\n",
      "---\n",
      "314\n",
      "EECS, Vanderbilt University, Nashville, TN 37235, USA\n",
      "(2)\n",
      "Vanderbilt University Medical Center, Nashville, TN 37235, USA\n",
      "\n",
      "This research was supported by NSF CAREER 1452485, R01 EB017230 and R01 CA253923. This study was supported in part by U01 CA196405 to Massion. This project was supported in part by the National Center for Research Resources, Grant UL1 RR024975-01, and is now at the National Center for Advancing Translational Sciences, Grant 2 UL1 TR000445-06. This study was funded in part by the Martineau Innovation Fund Grant through the Vanderbilt-Ingram Cancer Center Thoracic Working Group and NCI Early Detection Research Network 2U01CA152662 to PPM.\n",
      "\n",
      "---\n",
      "318\n",
      "Artificial Intelligence in Medical Imaging (AI-Med), Department of Child and Adolescent Psychiatry, Ludwig-Maximilians-Universität, Munich, Germany\n",
      "\n",
      "This research was supported by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation, and the Federal Ministry of Education and Research in the call for Computational Life Sciences (DeepMentia, 031L0200A).\n",
      "---\n",
      "322\n",
      "aetherAI, Taipei, Taiwan\n",
      "(2)\n",
      "AstraZeneca, London, UK\n",
      "\n",
      "We are grateful to Taiwan’s National Center for High-performance Computing for providing computing resources. We also thank Szu-Hua Chen, M.D. (aetherAI) for valuable advice.\n",
      "---\n",
      "324\n",
      "Department of Medical Biophysics, University of Toronto, Toronto, ON, Canada\n",
      "(2)\n",
      "Department of Medical Imaging, University of Toronto, Toronto, ON, Canada\n",
      "(3)\n",
      "Sunnybrook Research Institute, Toronto, ON, Canada\n",
      "(4)\n",
      "Centre Hospitalier Universitaire de Lyon, Lyon, France\n",
      "\n",
      "\n",
      "The authors would like to thank The Natural Sciences and Engineering Research Council of Canada (NSERC) for funding, and acknowledge the contribution of Drs. Karanicolas, Law and Coburn in helping to create the patient cohort for this study.\n",
      "---\n",
      "329\n",
      "(1)\n",
      "Rensselaer Polytechnic Institute, Troy, NY 12180, USA\n",
      "(2)\n",
      "IBM Research, Almaden Research Center, San Jose, CA 95120, USA\n",
      "(3)\n",
      "Virginia Tech, Blacksburg, VA 24061, USA\n",
      "\n",
      "This work was supported by the Rensselaer-IBM AI ResearchCollaboration, part of the IBM AI Horizons Network.\n",
      "---\n",
      "331\n",
      "\n",
      "Department of Computer Science, Stony Brook University, Stony Brook, NY, USA\n",
      "(2)\n",
      "Department of Biomedical Informatics, Stony Brook University, Stony Brook, NY, USA\n",
      "(3)\n",
      "Department of Radiology, Newark Beth Israel Medical Center, Newark, NJ, USA\n",
      "\n",
      "\n",
      "Reported research was supported by the OVPR and IEDM seed grants, 2020 at Stony Brook University, NIGMS T32GM008444, and NIH 75N92020D00021 (subcontract). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#column 1: id\n",
    "#column 23: affiliations y/n\n",
    "#column 24: who\n",
    "\n",
    "demo = df[df.iloc[:,23] == 'Yes']\n",
    "demo = demo.iloc[:,[1,24]]\n",
    "demo.reset_index()\n",
    "print('Number of articles who acknowledge funding etc: ', len(demo))\n",
    "\n",
    "\n",
    "for index in range(len(demo)):\n",
    "    print(demo.iloc[index, 0])\n",
    "    print(demo.iloc[index, 1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d08b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles who don't acknowledge funding etc:  15\n",
      "82\n",
      "The University of Hong Kong, Pok Fu Lam, Hong Kong\n",
      "(2)\n",
      "VoxelCloud Inc., Gayley Avenue 1085, Los Angeles, CA 90024, USA\n",
      "---\n",
      "124\n",
      "\n",
      "Department of Electrical Engineering, Stanford University, Stanford, USA\n",
      "(2)\n",
      "Institute for Computational and Mathematical Engineering, Stanford University, Stanford, USA\n",
      "(3)\n",
      "Department of Computer Science, Stanford University, Stanford, USA\n",
      "(4)\n",
      "Department of Radiology, Stanford University, Stanford, USA\n",
      "(5)\n",
      "Khoury College of Computer Sciences, Northeastern University, Boston, USA\n",
      "(6)\n",
      "Department of Biomedical Data Science, Stanford University, Stanford, USA\n",
      "---\n",
      "265\n",
      "Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), School of Biomedical Engineering & Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China\n",
      "(2)\n",
      "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China\n",
      "(3)\n",
      "School of Electronic, Electrical and Communication Engineering, University of the Chinese Academy of Science, Beijing, China\n",
      "(4)\n",
      "Healthcare Intelligence, AIC, DAMO Academy, Alibaba Group, Hangzhou, China  Nothing is mentioned\n",
      "---\n",
      "271\n",
      "(1)\n",
      "Stony Brook University, Stony Brook, NY, USA  Nothing is mentioned\n",
      "---\n",
      "278\n",
      "(1)\n",
      "PAII Inc., Bethesda, MD 20817, USA\n",
      "(2)\n",
      "Ruijin Hospital, Shanghai, China\n",
      "(3)\n",
      "Chang Gung Memorial Hospital, Linkou, Taoyuan, Taiwan ROC\n",
      "(4)\n",
      "PingAn Technology, Shenzhen, China  Nothing is mentioned\n",
      "---\n",
      "280\n",
      "(1)\n",
      "Medical Imaging Center, Vingroup Big Data Institute, Hanoi, Vietnam\n",
      "(2)\n",
      "School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam\n",
      "(3)\n",
      "College of Engineering and Computer Science, VinUniversity, Hanoi, Vietnam\n",
      "(4)\n",
      "Department of Mathematics, Yale University, New Heaven, USA  Nothing is mentioned\n",
      "---\n",
      "287\n",
      "(1)\n",
      "MIT CSAIL, Cambridge, USA\n",
      "(2)\n",
      "Chung Shan Medical University, Taichung, Taiwan\n",
      "---\n",
      "301\n",
      "(1)\n",
      "Tencent AI Lab, Shenzhen, China\n",
      "(2)\n",
      "Peking Union Medical College Hospital, Beijing, China\n",
      "---\n",
      "304\n",
      "(1)\n",
      "Simon Fraser University, Burnaby, Canada\n",
      "(2)\n",
      "Pacific Centre for Reproductive Medicine, Burnaby, Canada\n",
      "---\n",
      "306\n",
      "(1)\n",
      "NEC Laboratories, Beijing, China\n",
      "---\n",
      "315\n",
      ")\n",
      "Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia\n",
      "(2)\n",
      "Department of Computer Science and Technology, Heilongjiang University, Harbin, China\n",
      "(3)\n",
      "College of Intelligence and Computing, Tianjin University, Tianjin, China\n",
      "(4)\n",
      "Department of Radiation Oncology, Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences, Jinan, China\n",
      "---\n",
      "316\n",
      "Tempus Labs, Inc., Chicago, IL, USA\n",
      "---\n",
      "320\n",
      "Computer Aided Medical Procedures, Technical University of Munich, Munich, Germany\n",
      "(2)\n",
      "Sharif University of Technology, Tehran, Iran\n",
      "(3)\n",
      "Whiting School of Engineering, Johns Hopkins University, Baltimore, USA\n",
      "---\n",
      "321\n",
      "Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia\n",
      "(2)\n",
      "Department of Radiation Oncology, Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences, Jinan, China\n",
      "(3)\n",
      "College of Intelligence and Computing, Tianjin University, Tianjin, China\n",
      "(4)\n",
      "School of Software Technology, Zhejiang University, Hangzhou, China\n",
      "(5)\n",
      "Department of Computer Science and Technology, Heilongjiang University, Harbin, China\n",
      "---\n",
      "325\n",
      "Department of Computer Science, University of Texas at Dallas, Richardson, TX 75080, USA\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#column 1: id\n",
    "#column 23: affiliations y/n\n",
    "#column 24: who\n",
    "\n",
    "demo = df[df.iloc[:,23] == 'No']\n",
    "demo = demo.iloc[:,[1,24]]\n",
    "demo.reset_index()\n",
    "print(\"Number of articles who don't acknowledge funding etc: \", len(demo))\n",
    "\n",
    "\n",
    "for index in range(len(demo)):\n",
    "    print(demo.iloc[index, 0])\n",
    "    print(demo.iloc[index, 1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257a032",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c739ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles who include respect for persons:  0\n"
     ]
    }
   ],
   "source": [
    "#column 1: id\n",
    "#column 25: respect for persons\n",
    "#column 27: benefience\n",
    "#column 29: justice\n",
    "#column 31: law/public interest\n",
    "\n",
    "demo = df[df.iloc[:,25] == 'Yes']\n",
    "demo = demo.iloc[:,[1,26]]\n",
    "demo.reset_index()\n",
    "print('Number of articles who include respect for persons: ', len(demo))\n",
    "\n",
    "\n",
    "for index in range(len(demo)):\n",
    "    print(demo.iloc[index, 0])\n",
    "    print(demo.iloc[index, 1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5755e970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles who include benefience:  2\n",
      "255\n",
      "Experimentally, our CA-Net outperforms the 1st place by a noticeable margin on Kaggle DSB2017 dataset.\n",
      "---\n",
      "257\n",
      "From the introduction: \n",
      "For image-based disease benign/malignant diagnosis, it is crucial to learn the disease-related representation for prediction, due to the necessity of trustworthy (to patients), explainable (to clinicians) and good generalization ability in healthcare.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#column 1: id\n",
    "#column 25: respect for persons\n",
    "#column 27: benefience\n",
    "#column 29: justice\n",
    "#column 31: law/public interest\n",
    "\n",
    "demo = df[df.iloc[:,27] == 'Yes']\n",
    "demo = demo.iloc[:,[1,28]]\n",
    "demo.reset_index()\n",
    "print('Number of articles who include beneficence: ', len(demo))\n",
    "\n",
    "\n",
    "for index in range(len(demo)):\n",
    "    print(demo.iloc[index, 0])\n",
    "    print(demo.iloc[index, 1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "044bbedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles who include justice:  0\n"
     ]
    }
   ],
   "source": [
    "#column 1: id\n",
    "#column 25: respect for persons\n",
    "#column 27: benefience\n",
    "#column 29: justice\n",
    "#column 31: law/public interest\n",
    "\n",
    "demo = df[df.iloc[:,29] == 'Yes']\n",
    "demo = demo.iloc[:,[1,30]]\n",
    "demo.reset_index()\n",
    "print('Number of articles who include justice: ', len(demo))\n",
    "\n",
    "\n",
    "for index in range(len(demo)):\n",
    "    print(demo.iloc[index, 0])\n",
    "    print(demo.iloc[index, 1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a2952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles who include law/public interest:  45\n",
      "173\n",
      "Our code is available at https://​github.​com/​berenslab/​genderBagNets.\n",
      "---\n",
      "74\n",
      "Our code is made publicly available here.\n",
      "\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87196-3_​6) contains supplementary material, which is available to authorized users.\n",
      "\n",
      "There are several references throughout to the supplementary material\n",
      "\n",
      "From the conclusion: This demonstrates that our model can learn a meaningful and relevant representation of healthy brains which can be used to discriminate patients in small data-sets. An ablation study showed that our method consistently improves upon SimCLR for three different sets of transformations. We also made a step towards a debiased algorithm by demonstrating that our model is less sensitive to the site effect than other SOTA fully supervised algorithms trained from scratch. We think this is still an important issue leading to strong biases in machine learning algorithms and it currently leads to costly harmonization protocols between hospitals during acquisitions. Finally, as a step towards reproducible research, we made our code public and we will release the BHB dataset to the scientific community soon.\n",
      "---\n",
      "321\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​69) contains supplementary material, which is available to authorized users\n",
      "---\n",
      "316\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​64) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "309\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​57) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "304\n",
      "Code is available: https://​github.​com/​llockhar/​Embryo-Stage-Onset-Detection.\n",
      "---\n",
      "284\n",
      "The code, models and features dataset are available at https://​github.​com/​fepegar/​gestures-miccai-2021.\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​32) contains supplementary material, which is available to authorized users\n",
      "\n",
      "---\n",
      "280\n",
      "Supplementary electronic material\n",
      "---\n",
      "278\n",
      "Supplementary electronic material\n",
      "---\n",
      "258\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​6) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "324\n",
      "A Keras implementation of AMINN is released (https://​github.​com/​martellab-sri/​AMINN).\n",
      "\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​72) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "331\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​79) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "170\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​42) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "106\n",
      "All code is publicly available1.\n",
      "---\n",
      "118\n",
      "(Code is available at https://​github.​com/​LLYXC/​OXnet.).\n",
      "---\n",
      "124\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87196-3_​56) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "322\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​70) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "320\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​68) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "318\n",
      "Our implementation is available at https://​github.​com/​ai-med/​DAFT.\n",
      "\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​66) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "312\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​60) contains supplementary material, which is available to authorized users\n",
      "---\n",
      "311\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​59) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "310\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​58) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "302\n",
      "The source code is available at: https://​github.​com/​SunjHan/​Hybrid-Representation-Learning-Approach-for-Rare-Disease-Classification.\n",
      "---\n",
      "283\n",
      "Code is released at https://​github.​com/​agaldran/​balanced_​mixup\n",
      "---\n",
      "271\n",
      "The code is publicly available on Github\n",
      "---\n",
      "269\n",
      "The table showing the demographics and electronic supplementary material\n",
      "---\n",
      "268\n",
      "Also has supplementary electronic material, but there is a mention of what the supplementary material contains in the text, more info about the datasets, so some of the above questions may have different answers if this was available\n",
      "---\n",
      "266\n",
      "There is a doi in the electronic supplementary material, but still only available behind a paywall and am unsure how much more material is actually provided\n",
      "---\n",
      "262\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​10) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "255\n",
      "To some degree perhaps - though who knows what is actually in this supplementary material?\n",
      "Also no mention is made of the repo/code for reproducibility... \n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​3) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "138\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​10) contains supplementary material, which is available to authorized users.\n",
      "\n",
      "They actually mention (ish) what is is the supplementary material: \n",
      "5 Experiments\n",
      "Models. We compare four models: the single-view model, the late-join model, and the token-based and pixel-based cross-view transformers. All models use the same ResNet-18 architecture [4] for the convolution and pooling blocks up to the global average pooling layer. We use pre-trained weights on ImageNet, as provided by PyTorch. After global average pooling, we concatenate the feature vectors for both iews and use this as input for a single fully connected layer that computes the output. (See the supplementary material for a detailed view.)\n",
      "\n",
      "The transformer performance was not very sensitive to the number of heads or tokens: all settings produced similar results (see the table in the supplementary results)\n",
      "---\n",
      "139\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​11) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "145\n",
      "This one also includes some mention of what is in the supplementary material\n",
      "\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​17) contains supplementary material, which is available to authorized users.\n",
      "\n",
      "Implementation details for the SCM and the classifier, and the best validation log-likelihood for each model are shown in the supplementary material.\n",
      "Detailed demographic information of the datasets is provided in the supplementary material.\n",
      "---\n",
      "159\n",
      "(Code will be made available at https://​github.​com/​liuquande/​FedIRM).\n",
      "---\n",
      "160\n",
      "One of their main contributions is a privacy aspect, keeping used data private, but more on the model level than on the personal level....\n",
      "\n",
      "We also propose the peer anonymization (PA) technique as a core component of $$\\texttt {FedPerl}$$. PA preserves privacy and reduces the communication cost while maintaining the performance without additional complexity. \n",
      "\n",
      "Recently, deep learning-based methods have shown dermatologist-level [5, 24] or superior performance [8, 12, 28] in skin cancer classification. Yet, most of these methods rely on a large curated amount of centralized labeled data, which is usually not available due to privacy issues [16].\n",
      "\n",
      "Our novel peer anonymization (PA) technique, is simple yet effective to anonymize the peer such that it is less prone to model inversion or deanonymization. PA is designed carefully to reduce the communication cost while maintains performance. Nevertheless, a privacy guarantee for aggregated models (not individuals) is an open issue and has not been thoroughly investigated in the community and mathematical analysis is yet to be proven. Generalization to unseen client is yet to be investigated in future work. This includes investigating different approaches to profile the clients in building the community. Further, a dynamic policy of when and which community to approach should be further investigated.\n",
      "\n",
      "also includes github and info about supplementary material\n",
      "(https://​github.​com/​tbdair/​FedPerlV1.​0).\n",
      "\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​32) contains supplementary material, which is available to authorized users.\n",
      "\n",
      "Implementation. We opt for EfficientNet as our architecture. Adam for optimization for 500 rounds. The batch size and participation rate were set to 16 & 0.3, respectively. The learning rate, $$\\beta $$, $$\\gamma $$, T were investigated and found best at 0.00005, 0.5, 0.01, and 2, respectively, whereas $$\\tau $$ found best at 0.6 & 0.9 for the federated and local models respectively. Further details in the suppl. material.\n",
      "---\n",
      "161\n",
      "Topic is a solution to the issue of: the patient privacy and ethical concerns impede constructing centralized datasets with increasing size (which have been shown to do well with computer aided diagnosis)\n",
      "\n",
      "The code and dataset are available at https://​github.​com/​CityU-AIM-Group/​PRR-FL.\n",
      "---\n",
      "164\n",
      "Pseudocode included\n",
      "---\n",
      "180\n",
      "The code is available at https://​github.​com/​catarina-barata/​CBIR_​Explainability_​Skin_​Cancer.\n",
      "\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​52) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "69\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87196-3_​1) contains supplementary material, which is available to authorized users.\n",
      "---\n",
      "72\n",
      "Codes are available in https://​github.​com/​hongweilibran/​imbalanced-SSL.\n",
      "\n",
      "Electronic supplementary material\n",
      "The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87196-3_​4) contains supplementary material, which is available to authorized users.\n",
      "\n",
      "\n",
      "We build a 3D convolutional neural network with two bottleneck blocks as the encoder for all experiments (details in Supplementary).\n",
      "---\n",
      "73\n",
      "Our code is available at https://​github.​com/​easonyang1996/​CS-CO.\n",
      "---\n",
      "78\n",
      "The code and data are available at https://​github.​com/​hrlblab/​SimTriplet.\n",
      "---\n",
      "308\n",
      "Code available at https://​github.​com/​jemtan/​PII.\n",
      "---\n",
      "266\n",
      "Their code is freely available on github, link in abstract\n",
      "---\n",
      "257\n",
      "To provide convenience for latter works, we publish our spitted test set of DDSM [2] in supplementary.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#column 1: id\n",
    "#column 25: respect for persons\n",
    "#column 27: benefience\n",
    "#column 29: justice\n",
    "#column 31: law/public interest\n",
    "\n",
    "demo = df[df.iloc[:,31] == 'Yes']\n",
    "demo = demo.iloc[:,[1,32]]\n",
    "demo.reset_index()\n",
    "print('Number of articles who include law/public interest: ', len(demo))\n",
    "\n",
    "\n",
    "for index in range(len(demo)):\n",
    "    print(demo.iloc[index, 0])\n",
    "    print(demo.iloc[index, 1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129caeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
