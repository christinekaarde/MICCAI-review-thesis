Tidsstempel,What is the article's index?,Which year is the article from?,Is the article accurately labelled as classification?,"If not accurately labelled as classification, what would you label it as?",Please input the quote from which you infer the answer to the previous question (if possible),What is the aim or task of the article? (input quote),How does the article justify this aim or task? (input quote),Which method is used for classification?,Which performance measures are used? ,Does the article use segmentation as preprocessing?,Please input the quote from which you infer the answer to the previous question (if possible),Does the dataset used in the article have a title?,Please input the quote from which you infer the answer to the previous question (if possible),What is the size of the used dataset? (input quote),What type is the dataset?,Please input the quote from which you infer the answer to the previous question (if possible),Is the survey/method of how the dataset was obtained accessible?,Please input the quote from which you infer the answer to the previous question (if possible),Does the article mention the demographics of the patients/images included in the used dataset?,Please input the quote from which you infer the answer to the previous question (if possible),Does the article mention the intent for collecting the dataset? The intended task for the dataset?,Please input the quote from which you infer the answer to the previous question (if possible),Does the article disclose any affiliations?,Please input the quote from which you infer the answer to the previous question (if possible),"Does the article include anything about respect for persons (informed consent, voluntary participation) participating in the dataset? ",Please input the quote from which you infer the answer to the previous question (if possible),"Does the article have any mention of benefience, minimising risk/maximising benefit of work? ",Please input the quote from which you infer the answer to the previous question (if  possible),"Does the article have any mention of justice (equal treatment, fair selection of subjects)?",Please input the quote from which you infer the answer to the previous question (if  possible),"Does the article mention any respect for law/public interest (transparency in methods/results, accountability for actions)?",Please input the quote from which you infer the answer to the previous question (if  possible),Are there any other comments/interesting aspects?
26/10/2022 10.53.32,173,2021,Yes,It was accurately labelled,Interpretable Gender Classification from Retinal Fundus Images Using BagNets,"Deep neural networks (DNNs) are able to predict a person’s gender from retinal fundus images with high accuracy, even though this task is usually considered hardly possible by ophthalmologists. Therefore, it has been an open question which features allow reliable discrimination between male and female fundus images. To study this question, we used a particular DNN architecture called BagNet, which extracts local features from small image patches and then averages the class evidence across all patches. The BagNet performed on par with the more sophisticated Inception-v3 model, showing that the gender information can be read out from local features alone. ","In recent years, deep neural networks (DNNs) have achieved physician-level accuracy in various image-based medical tasks, e.g. in radiology [21], dermatology [10], pathology [15] and ophthalmology [7, 12]. Moreover, in some cases DNNs have been shown to have good performance in tasks that are not straightforward for physicians: for example, they can accurately predict the gender from retinal images [25]. As this task is typically not clinically relevant, ophthalmologists are not explicitly trained for it. ",Neural network,"AUC, Accuracy",No,Nothing is mentioned,Yes,"The UK Biobank [27] offers a large-scale and multi-modal repository of health-related data from the UK. From this, we obtained records of over 84, 000 subjects with 174, 465 fundus images from both eyes and multiple visits per participant. Male and female subjects constituted $$46\%$$ and $$54\%$$ of the data, respectively. As a substantial fraction of the images were not gradable due to image quality issues (artefacts, high contrast, or oversaturation), we used the EyeQual networks [6] to filter out poor images. 47, 939 images ($$47\%$$ male, $$53\%$$ female) passed the quality check by the EyeQual ensemble. We partitioned them into the training, validation and test sets with $$75\%$$, $$10\%$$ and $$15\%$$ of subjects, respectively, making sure that all images from each subject were allocated to the same set.

Additionally, we obtained 29 fundus images from patients (11 male, 18 female, all older than 47 years) at the University Eye Hospital with permission of the Institutional Ethics Board. We used these additional images as an independent test set. For all images, we applied a circular mask to capture the 95% central area and to remove camera artifacts at the borders","The UK Biobank [27] offers a large-scale and multi-modal repository of health-related data from the UK. From this, we obtained records of over 84, 000 subjects with 174, 465 fundus images from both eyes and multiple visits per participant. Male and female subjects constituted $$46\%$$ and $$54\%$$ of the data, respectively. As a substantial fraction of the images were not gradable due to image quality issues (artefacts, high contrast, or oversaturation), we used the EyeQual networks [6] to filter out poor images. 47, 939 images ($$47\%$$ male, $$53\%$$ female) passed the quality check by the EyeQual ensemble. We partitioned them into the training, validation and test sets with $$75\%$$, $$10\%$$ and $$15\%$$ of subjects, respectively, making sure that all images from each subject were allocated to the same set.

Additionally, we obtained 29 fundus images from patients (11 male, 18 female, all older than 47 years) at the University Eye Hospital with permission of the Institutional Ethics Board. We used these additional images as an independent test set.","Private, Public","One private, one public",No,"The UK Biobank [27] offers a large-scale and multi-modal repository of health-related data from the UK. From this, we obtained records of over 84, 000 subjects with 174, 465 fundus images from both eyes and multiple visits per participant. Male and female subjects constituted $$46\%$$ and $$54\%$$ of the data, respectively. As a substantial fraction of the images were not gradable due to image quality issues (artefacts, high contrast, or oversaturation), we used the EyeQual networks [6] to filter out poor images. 47, 939 images ($$47\%$$ male, $$53\%$$ female) passed the quality check by the EyeQual ensemble. We partitioned them into the training, validation and test sets with $$75\%$$, $$10\%$$ and $$15\%$$ of subjects, respectively, making sure that all images from each subject were allocated to the same set.

Additionally, we obtained 29 fundus images from patients (11 male, 18 female, all older than 47 years) at the University Eye Hospital with permission of the Institutional Ethics Board. We used these additional images as an independent test set.",Yes,"The UK Biobank [27] offers a large-scale and multi-modal repository of health-related data from the UK. From this, we obtained records of over 84, 000 subjects with 174, 465 fundus images from both eyes and multiple visits per participant. Male and female subjects constituted $$46\%$$ and $$54\%$$ of the data, respectively. As a substantial fraction of the images were not gradable due to image quality issues (artefacts, high contrast, or oversaturation), we used the EyeQual networks [6] to filter out poor images. 47, 939 images ($$47\%$$ male, $$53\%$$ female) passed the quality check by the EyeQual ensemble. We partitioned them into the training, validation and test sets with $$75\%$$, $$10\%$$ and $$15\%$$ of subjects, respectively, making sure that all images from each subject were allocated to the same set.

Additionally, we obtained 29 fundus images from patients (11 male, 18 female, all older than 47 years) at the University Eye Hospital with permission of the Institutional Ethics Board. We used these additional images as an independent test set.

Gender - but as this is the binary classification they are testing this is necessary!",Yes,"They expanded their dataset with data specifically for this article: 

Additionally, we obtained 29 fundus images from patients (11 male, 18 female, all older than 47 years) at the University Eye Hospital with permission of the Institutional Ethics Board. We used these additional images as an independent test set.",Yes,"Institute for Ophthalmic Research, Tübingen, Germany
(2)
Tübingen AI Center, Tübingen, Germany
(3)
University Eye Clinic, University of Tübingen, 72076 Tübingen, Germany

We thank Wieland Brendel for his support with BagNets. This research was supported by the German Ministry of Science and Education (BMBF, 01GQ1601 and 01IS18039A) and the German Science Foundation (BE5601/4-2 and EXC 2064, project number 390727645). Hanna Faber received research funding from the Junior Clinician Scientist Program of the Faculty of Medicine, Eberhard Karls University of Tübingen, Germany (application number 463–0–0). Additional funding was provided by Novartis AG through a research grant. The funding bodies did not have any influence in the study planning and design. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Indu Ilanchezian.",Yes,"Additionally, we obtained 29 fundus images from patients (11 male, 18 female, all older than 47 years) at the University Eye Hospital with permission of the Institutional Ethics Board. We used these additional images as an independent test set.

So not necessarily consent by patients themselves, at least not mentioned, but the ethics board was asked",No,Nothing is mentioned,No,Nothing is mentioned,Yes,Our code is available at https://​github.​com/​berenslab/​genderBagNets.,
27/10/2022 10.21.42,74,2021,Yes,It was accurately labelled,"Traditional supervised learning with deep neural networks requires a tremendous amount of labelled data to converge to a good solution. For 3D medical images, it is often impractical to build a large homogeneous annotated dataset for a specific pathology. Self-supervised methods offer a new way to learn a representation of the images in an unsupervised manner with a neural network. In particular, contrastive learning has shown great promises by (almost) matching the performance of fully-supervised CNN on vision tasks. Nonetheless, this method does not take advantage of available meta-data, such as participant’s age, viewed as prior knowledge. Here, we propose to leverage continuous proxy metadata, in the contrastive learning framework, by introducing a new loss called y-Aware InfoNCE loss. Specifically, we improve the positive sampling during pre-training by adding more positive examples with similar proxy meta-data with the anchor, assuming they share similar discriminative semantic features. With our method, a 3D CNN model pre-trained on $$10^4$$ multi-site healthy brain MRI scans can extract relevant features for three classification tasks: schizophrenia, bipolar diagnosis and Alzheimer’s detection. When fine-tuned, it also outperforms 3D CNN trained from scratch on these tasks, as well as state-of-the-art self-supervised methods. Our code is made publicly available here.","Traditional supervised learning with deep neural networks requires a tremendous amount of labelled data to converge to a good solution. For 3D medical images, it is often impractical to build a large homogeneous annotated dataset for a specific pathology. Self-supervised methods offer a new way to learn a representation of the images in an unsupervised manner with a neural network. In particular, contrastive learning has shown great promises by (almost) matching the performance of fully-supervised CNN on vision tasks. Nonetheless, this method does not take advantage of available meta-data, such as participant’s age, viewed as prior knowledge. Here, we propose to leverage continuous proxy metadata, in the contrastive learning framework, by introducing a new loss called y-Aware InfoNCE loss. Specifically, we improve the positive sampling during pre-training by adding more positive examples with similar proxy meta-data with the anchor, assuming they share similar discriminative semantic features. With our method, a 3D CNN model pre-trained on $$10^4$$ multi-site healthy brain MRI scans can extract relevant features for three classification tasks: schizophrenia, bipolar diagnosis and Alzheimer’s detection. When fine-tuned, it also outperforms 3D CNN trained from scratch on these tasks, as well as state-of-the-art self-supervised methods. Our code is made publicly available here.","Traditional supervised learning with deep neural networks requires a tremendous amount of labelled data to converge to a good solution. For 3D medical images, it is often impractical to build a large homogeneous annotated dataset for a specific pathology. Self-supervised methods offer a new way to learn a representation of the images in an unsupervised manner with a neural network. In particular, contrastive learning has shown great promises by (almost) matching the performance of fully-supervised CNN on vision tasks. Nonetheless, this method does not take advantage of available meta-data, such as participant’s age, viewed as prior knowledge. Here, we propose to leverage continuous proxy metadata, in the contrastive learning framework, by introducing a new loss called y-Aware InfoNCE loss. Specifically, we improve the positive sampling during pre-training by adding more positive examples with similar proxy meta-data with the anchor, assuming they share similar discriminative semantic features. With our method, a 3D CNN model pre-trained on $$10^4$$ multi-site healthy brain MRI scans can extract relevant features for three classification tasks: schizophrenia, bipolar diagnosis and Alzheimer’s detection. When fine-tuned, it also outperforms 3D CNN trained from scratch on these tasks, as well as state-of-the-art self-supervised methods. Our code is made publicly available here.","Neural network, self supervised learning",AUC,No,Nothing is mentioned,Yes,"5 different sets
Big Healthy Brains (BHB) dataset. We aggregated 13 publicly available datasets1 of 3D T1 MRI scans of healthy controls (HC) acquired on more than 70 different scanners and comprising $$N=10^4$$ samples. We use this dataset only to pre-train our model with the participant’s age as the proxy meta-data. The learnt representation is then tested on the following four data-sets using as final task a binary classification between HC and patients.

SCHIZCONNECT-VIP2. It comprises $$N=605$$ multi-site MRI scans including 275 patients with strict schizophrenia (SCZ) and 330 HC.

BIOBD [15, 23]. This dataset includes $$N=662$$ MRI scans acquired on 8 different sites with 356 HC and 306 patients with bipolar disorder (BD).

BSNIP [25]. It includes $$N=511$$ MRI scans with $$N=200$$ HC, $$N=194$$ SCZ and $$N=117$$ BD. This independent dataset is used only at test time in Fig. 2b).

Alzheimer’s Disease Neuroimaging Initiative (ADNI-GO)3. We use $$N=387$$ co-registered T1-weighted MRI images divided in $$N=199$$ healthy controls and $$N=188$$ Alzheimer’s patients (AD). We only included one scan per patient at the first session (baseline).
All data-sets have been pre-processed in the same way with a non-linear registration to the MNI template and a gray matter extraction step. The final spatial resolution is 1.5 mm isotropic and the images are of size $$121\times 145\times 121$$.","5 different sets
Big Healthy Brains (BHB) dataset. We aggregated 13 publicly available datasets1 of 3D T1 MRI scans of healthy controls (HC) acquired on more than 70 different scanners and comprising $$N=10^4$$ samples. We use this dataset only to pre-train our model with the participant’s age as the proxy meta-data. The learnt representation is then tested on the following four data-sets using as final task a binary classification between HC and patients.

SCHIZCONNECT-VIP2. It comprises $$N=605$$ multi-site MRI scans including 275 patients with strict schizophrenia (SCZ) and 330 HC.

BIOBD [15, 23]. This dataset includes $$N=662$$ MRI scans acquired on 8 different sites with 356 HC and 306 patients with bipolar disorder (BD).

BSNIP [25]. It includes $$N=511$$ MRI scans with $$N=200$$ HC, $$N=194$$ SCZ and $$N=117$$ BD. This independent dataset is used only at test time in Fig. 2b).

Alzheimer’s Disease Neuroimaging Initiative (ADNI-GO)3. We use $$N=387$$ co-registered T1-weighted MRI images divided in $$N=199$$ healthy controls and $$N=188$$ Alzheimer’s patients (AD). We only included one scan per patient at the first session (baseline).
All data-sets have been pre-processed in the same way with a non-linear registration to the MNI template and a gray matter extraction step. The final spatial resolution is 1.5 mm isotropic and the images are of size $$121\times 145\times 121$$.","Private, Public","Mixed, both public and private",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
NeuroSpin, CEA Saclay, Université Paris-Saclay, Gif-sur-Yvette, France
(2)
LTCI, Télécom Paris, IPParis, Paris, France
(3)
Department of Neuropsychology, Johannes-Gutenberg University of Mainz, Mainz, Germany
(4)
Department of Neurosciences, Fondazione IRCCS, University of Milan, Milan, Italy
(5)
Université Grenoble Alpes, Inserm U1216, CHU Grenoble Alpe, Grenoble, France
(6)
Centre for Neuroimaging and Cognitive Genomics (NICOG), Galway, Ireland
(7)
Department of Neuroscience, University of Geneva, Geneva, Switzerland
(8)
Department of Psychiatry, Western Psychiatric Institute, University of Pittsburgh, Pittsburgh, USA
(9)
Department of Psychiatry, UC San Diego, San Diego, CA, USA

This work was granted access to the HPC resources of IDRIS under the allocation 2020-AD011011854 made by GENCI.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Our code is made publicly available here.

Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87196-3_​6) contains supplementary material, which is available to authorized users.

There are several references throughout to the supplementary material

From the conclusion: This demonstrates that our model can learn a meaningful and relevant representation of healthy brains which can be used to discriminate patients in small data-sets. An ablation study showed that our method consistently improves upon SimCLR for three different sets of transformations. We also made a step towards a debiased algorithm by demonstrating that our model is less sensitive to the site effect than other SOTA fully supervised algorithms trained from scratch. We think this is still an important issue leading to strong biases in machine learning algorithms and it currently leads to costly harmonization protocols between hospitals during acquisitions. Finally, as a step towards reproducible research, we made our code public and we will release the BHB dataset to the scientific community soon.",
27/10/2022 12.13.54,122,2021,Yes,It was accurately labelled,"Combining Attention-Based Multiple Instance Learning and Gaussian Processes for CT Hemorrhage Detection

Uses the right perf measures","Intracranial hemorrhage (ICH) is a life-threatening emergency with high rates of mortality and morbidity. Rapid and accurate detection of ICH is crucial for patients to get a timely treatment. In order to achieve the automatic diagnosis of ICH, most deep learning models rely on huge amounts of slice labels for training. Unfortunately, the manual annotation of CT slices by radiologists is time-consuming and costly. To diagnose ICH, in this work, we propose to use an attention-based multiple instance learning (Att-MIL) approach implemented through the combination of an attention-based convolutional neural network (Att-CNN) and a variational Gaussian process for multiple instance learning (VGPMIL). ","Acute intracranial hemorrhage (ICH) has always been a life-threatening event that causes high mortality and morbidity rate [13]. Rapid and early detection of ICH is essential because nearly $$30\%$$ of the life loss happens in the first 24 h [18]. In order to prompt the optimal treatment to patients in short time, computer-aided diagnosis (CAD) is being designed to establish a better triaging protocol.

Recently, deep learning (DL) algorithms have been proposed for the diagnosis of ICH. ",Neural network,"AUC, Accuracy, Precision, Recall, F1 score",No,Nothing is mentioned,Yes,"A collection of 39650 slices of head CT images acquired from 1150 patients published in the 2019 Radiological Society of North America (RSNA) challenge [1] are included in this study. The number of slices ranges from 24 to 57 for each scan (512 $$\times $$ 512 pixels). In order to mimic the way radiologists adjust different window widths (W) and centers (C) to read CT images, we apply three windows for each CT slice in a scan with the Hounsfield Units (HU) to enhance the display of brain, blood, and soft tissue, respectively, using [W:80, C:40], [W:200, C:80] and [W:380, C:40]. The three window slices are concatenated into three channels and normalized to [0,1]. The CT scans are split into 1000 (Scan-P:411, Scan-N:589; Slice-P:4976, Slice-N: 29520) for training and validation and the rest 150 (Scan-P:72, Scan-N:78; Slice-P:806, Slice-N: 4448) for testing. Positive(P) represents the case with ICH and Negative(N) represents the normal case. 

In addition, the models trained on the RSNA dataset are further evaluated on 490 scans (Scan-P:205, Scan-N:285) of an external CQ500 dataset acquired from different institutions in India [6] to test the robustness of the model, each of which has 16 to 128 slices and goes through the same preprocessing steps.","A collection of 39650 slices of head CT images acquired from 1150 patients published in the 2019 Radiological Society of North America (RSNA) challenge [1] are included in this study. The number of slices ranges from 24 to 57 for each scan (512 $$\times $$ 512 pixels). In order to mimic the way radiologists adjust different window widths (W) and centers (C) to read CT images, we apply three windows for each CT slice in a scan with the Hounsfield Units (HU) to enhance the display of brain, blood, and soft tissue, respectively, using [W:80, C:40], [W:200, C:80] and [W:380, C:40]. The three window slices are concatenated into three channels and normalized to [0,1]. The CT scans are split into 1000 (Scan-P:411, Scan-N:589; Slice-P:4976, Slice-N: 29520) for training and validation and the rest 150 (Scan-P:72, Scan-N:78; Slice-P:806, Slice-N: 4448) for testing. Positive(P) represents the case with ICH and Negative(N) represents the normal case. In addition, the models trained on the RSNA dataset are further evaluated on 490 scans (Scan-P:205, Scan-N:285) of an external CQ500 dataset acquired from different institutions in India [6] to test the robustness of the model, each of which has 16 to 128 slices and goes through the same preprocessing steps.","Public, Private",One public one private,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Image and Video Processing Laboratory, Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL 60208, USA
(2)
Depto. Ciencias de la Computacion e I.A., Universidad de Granada, 18071 Granada, Spain

This work has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska Curie grant agreement No 860627 (CLARIFY Project) and also from the Spanish Ministry of Science and Innovation under project PID2019-105142RB-C22.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
20/10/2022 14.31.14,321,2021,Yes,It was accurately labelled,"Radiotherapy plays a vital role in treating patients with esophageal cancer (EC), whereas potential complications such as esophageal fistula (EF) can be devastating and even life-threatening. Therefore, predicting EF risks prior to radiotherapies for EC patients is crucial for their clinical treatment and quality of life. We propose a novel method of combining thoracic Computerized Tomography (CT) scans and clinical tabular data to improve the prediction of EF risks in EC patients. The multimodal network includes encoders to extract salient features from images and clinical data, respectively. In addition, we devise a self-attention module, named VisText, to uncover the complex relationships and correlations among different features. The associated multimodal features are integrated with clinical features by aggregation to further enhance prediction accuracy. Experimental results indicate that our method classifies EF status for EC patients with an accuracy of 0.8366, F1 score of 0.7337, specificity of 0.9312 and AUC of 0.9119, outperforming other methods in comparison.",We propose a novel method of combining thoracic Computerized Tomography (CT) scans and clinical tabular data to improve the prediction of EF risks in EC patients. ,"Radiotherapy plays a vital role in treating patients with esophageal cancer (EC), whereas potential complications such as esophageal fistula (EF) can be devastating and even life-threatening. Therefore, predicting EF risks prior to radiotherapies for EC patients is crucial for their clinical treatment and quality of life.

Esophageal cancer (EC) is the 6th most common cause of cancer-related death. To treat patients with unresectable locally advanced esophageal squamous cell carcinoma (SCC), chemotherapy and/or radiotherapy have demonstrated effectiveness and received considerable attention [1, 2]. Unfortunately, esophageal fistula (EF) is one of the complications resulting from these treatments [3]. Around 4.8–22.1% of the EC patients developed EF due to chemoradiotherapy [5]; this drastically reduces life expectancy to a rate of two to three months.",Neural network,"AUC, Specificity, Accuracy, F1 score",No,Nothing is mentioned,No,"The retrospective study includes data relating to EC patients collected from 2014–2019. Among the total 553 eligible patients, 186 patients who had developed EF were assigned to the case group (positive group); the remaining 367 patients who had not developed EF were assigned to the control group (negative group). Negative cases were then matched with positive cases to the ratio of 2:1 by the diagnosis time of EC, marriage, gender, and race, which has been commonly adopted in existent clinical research. The data collected for this study includes clinical and image data. The development of EF from time of scan ranges in [0, 1401] days. All data, excluding those on treatment, were collected before treatment.

The clinical dataset was collected using a standardized questionnaire, including general, diagnostic, treatment, and hematological data. There are 7 numerical and 27 categorical variables. Categorical variables were further processed to one-hot encoding matrices for each patient (n = 74). ","The retrospective study includes data relating to EC patients collected from 2014–2019. Among the total 553 eligible patients, 186 patients who had developed EF were assigned to the case group (positive group); the remaining 367 patients who had not developed EF were assigned to the control group (negative group). Negative cases were then matched with positive cases to the ratio of 2:1 by the diagnosis time of EC, marriage, gender, and race, which has been commonly adopted in existent clinical research. The data collected for this study includes clinical and image data. The development of EF from time of scan ranges in [0, 1401] days. All data, excluding those on treatment, were collected before treatment.

The clinical dataset was collected using a standardized questionnaire, including general, diagnostic, treatment, and hematological data. There are 7 numerical and 27 categorical variables. Categorical variables were further processed to one-hot encoding matrices for each patient (n = 74). ",Private,"The retrospective study includes data relating to EC patients collected from 2014–2019. Among the total 553 eligible patients, 186 patients who had developed EF were assigned to the case group (positive group); the remaining 367 patients who had not developed EF were assigned to the control group (negative group). Negative cases were then matched with positive cases to the ratio of 2:1 by the diagnosis time of EC, marriage, gender, and race, which has been commonly adopted in existent clinical research. The data collected for this study includes clinical and image data. The development of EF from time of scan ranges in [0, 1401] days. All data, excluding those on treatment, were collected before treatment.

The clinical dataset was collected using a standardized questionnaire, including general, diagnostic, treatment, and hematological data. There are 7 numerical and 27 categorical variables. Categorical variables were further processed to one-hot encoding matrices for each patient (n = 74). ",No,Nothing is mentioned,Yes,"by the diagnosis time of EC, marriage, gender, and race (and more which they do not specify)
The clinical dataset was collected using a standardized questionnaire, including general, diagnostic, treatment, and hematological data. There are 7 numerical and 27 categorical variables. Categorical variables were further processed to one-hot encoding matrices for each patient (n = 74). ",No,Maybe partially for this project?,No,"Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia
(2)
Department of Radiation Oncology, Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences, Jinan, China
(3)
College of Intelligence and Computing, Tianjin University, Tianjin, China
(4)
School of Software Technology, Zhejiang University, Hangzhou, China
(5)
Department of Computer Science and Technology, Heilongjiang University, Harbin, China",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​69) contains supplementary material, which is available to authorized users",
20/10/2022 14.18.16,316,2021,Yes,It was accurately labelled," DOF predicts OS in glioma patients with a median C-index of 0.788 ± 0.067, significantly outperforming (p = 0.023) the best performing unimodal model with a median C-index of 0.718 ± 0.064. ","Here, we predict the overall survival (OS) of glioma patients from diverse multimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to combine information from multiparametric MRI exams, biopsy-based modalities (such as H&E slide images and/or DNA sequencing), and clinical variables into a comprehensive multimodal risk score. ","Clinical decision-making in oncology involves multimodal data such as radiology scans, molecular profiling, histopathology slides, and clinical factors. Despite the importance of these modalities individually, no deep learning framework to date has combined them all to predict patient prognosis.

Glioma is an intuitive candidate for deep learning-based multimodal biomarkers owing to the presence of well-characterized prognostic information across modalities [5], as well as its severity
",Neural network,concordance index,No,Nothing is mentioned,No,176 patients (see patient selection in Fig. S2) with Gd-T1w and T2w-FLAIR scans from the TCGA-GBM [23] and TCGA-LGG [24] studies were obtained from TCIA [25] and annotated by 7 radiologists to delineate the enhancing lesion and edema region.,176 patients (see patient selection in Fig. S2) with Gd-T1w and T2w-FLAIR scans from the TCGA-GBM [23] and TCGA-LGG [24] studies were obtained from TCIA [25] and annotated by 7 radiologists to delineate the enhancing lesion and edema region.,Private,Seemingly not available upon search,No,Nothing is mentioned,No,"14 clinical features were included into an SNN for the prediction of prognosis. The feature set included demographic and treatment details, as well as subjective histological subtype (see Table S3).

But table s3 is not found in the article.... so who knows where that is, or what is included..",Yes,"For this article: (generated from other previous datasets)
176 patients (see patient selection in Fig. S2) with Gd-T1w and T2w-FLAIR scans from the TCGA-GBM [23] and TCGA-LGG [24] studies were obtained from TCIA [25] and annotated by 7 radiologists to delineate the enhancing lesion and edema region.",No,"Tempus Labs, Inc., Chicago, IL, USA",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​64) contains supplementary material, which is available to authorized users.",
20/10/2022 14.00.50,315,2021,Yes,It was accurately labelled,Co-graph Attention Reasoning Based Imaging and Clinical Features Integration for Lymph Node Metastasis Prediction,"Lymph node metastasis (LNM) is the most critical prognosis factor in esophageal squamous cell carcinoma (ESCC). Effective and adaptive integration of preoperative CT images and multi-sourced non-imaging clinical factors is a challenging issue. In this work, we propose a graph-based reasoning model to learn new representations from multi-categorical clinical parameters for LNM prediction","Esophageal cancer (EC) is the seventh most common cancer leading to death in the United States [1]. The overall 5-year survival rate between 2010 and 2016 is 19.9%. [2] Diagnosed at early stage 1, EC patients’ chance of surviving 5 years can be increased to 47.1%. [2] An independent factor in EC prognosis is lymph node metastasis (LNM), which is most common in the significant histological EC subtype, esophageal squamous cell carcinoma (ESCC). [3] Thus, predicting LNM preoperatively is critical in clinical treatment decisions and planning for radiotherapy and surgery.",Graph analysis,"AUC, Specificity, Sensitivity, Precision-Recall Area Under Curve",No,Nothing is mentioned,No,397 ESCC patients from Shandong Cancer Hospital were enrolled in this retrospective study.,397 ESCC patients from Shandong Cancer Hospital were enrolled in this retrospective study.,Private,397 ESCC patients from Shandong Cancer Hospital were enrolled in this retrospective study.,Yes,"397 ESCC patients from Shandong Cancer Hospital were enrolled in this retrospective study. Inclusion criteria were as follows: (a) patients over 18 years old who took Contrast-Enhanced CT within 15 days before surgery between October 2013 and November 2018; (b) patients with pathologically confirmed LN status after surgery. Exclusion criteria included: (a) patients who received prior treatment before surgery; (b) patients who developed adenocarcinoma or other tumours. Each patient has 1 to 6 LNs. LNs of four patients are shown in Fig. 1. In total, there are 924 (798 negative and 126 positive) LNs. All the LNs have pathologically con-firmed status after surgery.
CT images were reconstructed using a matrix of 512 × 512 pixels. The pixel spacing varies between different scans within a range of 0.5859 mm × 0.5859 mm and 0.9766 mm × 0.9766 mm. The slice thickness is 5 mm. The number of slices in different patients’ thorax CT scans is between 58 and 117. Manual delineations of LNs were performed and verified by experienced radiation oncologists on CT.",Yes,"There are 20 non-imaging clinical parameters following into four categories. The first type is general information of age (62.69 ± 7.37) and gender (83.12% Male). The second type includes histories of smoking (60.00% Yes), drinking (57.79% Yes), hypertension (24.6% Yes), diabetes (7.8% Yes), heart disease (5.79% Yes), cerebral infarction (2.26% Yes), and gastritis (17.18% Yes). Third type pathologic features include preoperative tumor markers of CEA (3.06 ± 1.97), NSE (13.94 ± 4.45), CA19–9 (13.27 ± 9.82), CA125 (11.90 ± 5.92), CA72–4 (2.63 ± 4.45), CA15–3 (15.64 ± 8.66), Cyfra21–1 (3.26 ± 2.20), and SCC (1.55 ± 1.03). Last category is hematological parameters of neutrophil count (4.29 ± 2.00), lymphocyte count (1.83 ± 0.67), and platelet count (252.90 ± 67.48).",Yes,For this article,No,")
Department of Computer Science and Information Technology, La Trobe University, Melbourne, Australia
(2)
Department of Computer Science and Technology, Heilongjiang University, Harbin, China
(3)
College of Intelligence and Computing, Tianjin University, Tianjin, China
(4)
Department of Radiation Oncology, Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences, Jinan, China",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
20/10/2022 13.56.15,314,2021,Yes,It was accurately labelled,"Motivated by partial bidirectional generative adversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method that imputes one modality combining the conditional knowledge from another modality. Specifically, C-PBiGAN introduces a conditional latent space in a missing imputation framework that jointly encodes the available multi-modal data, along with a class regularization loss on imputed data to recover discriminative information. To our knowledge, it is the first generative adversarial model that addresses multi-modal missing imputation by modeling the joint distribution of image and non-image data. We validate our model with both the national lung screening trial (NLST) dataset and an external clinical validation cohort. The proposed C-PBiGAN achieves significant improvements in lung cancer risk estimation compared with representative imputation methods (e.g., AUC values increase in both NLST (+2.9%) and in-house dataset (+4.3%) compared with PBiGAN, p < 0.05)."," Motivated by partial bidirectional generative adversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method that imputes one modality combining the conditional knowledge from another modality. ","Data from multi-modality provide complementary information in clinical prediction, but missing data in clinical cohorts limits the number of subjects in multi-modal learning context. Multi-modal missing imputation is challenging with existing methods when 1) the missing data span across heterogeneous modalities (e.g., image vs. non-image); or 2) one modality is largely missing.

To our knowledge, we are the first to impute missing data by modeling joint distribution of image and non-image data with adversarial training; 

Lung cancer has the highest cancer death rate [1] and early diagnosis with low-dose computed tomography (CT) can reduce the risk of dying from lung cancer by 20% [2, 3]. Risk factors (e.g., age and nodule size) are widely used in machine learning and established prediction models [4–7]. With deep learning techniques, CT image features can be automatically extracted at the nodule-level [8], scan-level [9], or patient-level with longitudinal scans [10]. Previous studies demonstrated that CT image features and risk factors provide complementary information, which is combined to improve lung cancer risk estimation [11].",Neural network,AUC,No,Nothing is mentioned,Yes,"Two datasets are studied, 1) the national lung screening trail (NLST) [3] and 2) an in-house screening dataset from Vanderbilt Lung Screening Program (VLSP, https://​www.​vumc.​org/​radiology/​lung).",,Private,"Upon searching both appear to be private: 
Two datasets are studied, 1) the national lung screening trail (NLST) [3] and 2) an in-house screening dataset from Vanderbilt Lung Screening Program (VLSP, https://​www.​vumc.​org/​radiology/​lung).",No,Nothing is mentioned,Yes,"But this makes sense as the task is a multi-modality solution! Needs more information than just a scan... 

In the clinical screening process (Fig. 1), patients’ demographic information (e.g., age and gender) is captured in electronic medical records (EMR). 

We consider two longitudinal CTs (TP0 for previous, TP1 for current) as the complete data for image modality. The non-image modality includes the following 14 risk factors: age, sex, education, body mass index, race, quit smoke time, smoke status, pack-year, chronic obstructive pulmonary disease, personal cancer history, family lung cancer history, nodule size, spiculation, upper lobe of nodule.",No,Nothing is mentioned,Yes,"EECS, Vanderbilt University, Nashville, TN 37235, USA
(2)
Vanderbilt University Medical Center, Nashville, TN 37235, USA

This research was supported by NSF CAREER 1452485, R01 EB017230 and R01 CA253923. This study was supported in part by U01 CA196405 to Massion. This project was supported in part by the National Center for Research Resources, Grant UL1 RR024975-01, and is now at the National Center for Advancing Translational Sciences, Grant 2 UL1 TR000445-06. This study was funded in part by the Martineau Innovation Fund Grant through the Vanderbilt-Ingram Cancer Center Thoracic Working Group and NCI Early Detection Research Network 2U01CA152662 to PPM.
",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
20/10/2022 09.19.50,309,2021,Yes,It was accurately labelled,We show that the proposed framework can learn robust representations from unlabeled data that can be used for downstream tasks such as metagenome sequence classification with limited access to labeled data.,"The emergence of novel pathogens and zoonotic diseases like the SARS-CoV-2 have underlined the need for developing novel diagnosis and intervention pipelines that can learn rapidly from small amounts of labeled data. 
This is particularly a difficult task given that closely related pathogens can share more than $$90\%$$ of their genome structure. In this work, we address these challenges by proposing MG-Net, a self-supervised representation learning framework that leverages multi-modal context using pseudo-imaging data derived from clinical metagenome sequences. We show that the proposed framework can learn robust representations from unlabeled data that can be used for downstream tasks such as metagenome sequence classification with limited access to labeled data. Extensive experiments show that the learned features outperform current baseline metagenome representations, given only 1000 samples per class.","This is particularly a difficult task given that closely related pathogens can share more than $$90\%$$ of their genome structure. In this work, we address these challenges by proposing MG-Net, a self-supervised representation learning framework that leverages multi-modal context using pseudo-imaging data derived from clinical metagenome sequences. We show that the proposed framework can learn robust representations from unlabeled data that can be used for downstream tasks such as metagenome sequence classification with limited access to labeled data. Extensive experiments show that the learned features outperform current baseline metagenome representations, given only 1000 samples per class.

Advances in DNA sequencing technologies [21, 22] have made possible the determination of whole-genome sequences of simple unicellular (e.g., bacteria) and complex multicellular (e.g., human) organisms at a cheaper, faster, and larger scale. ",Neural network,"Precision, Recall",No,Nothing is mentioned,No,"For constructing the dataset for the training and evaluation of automated metagenome-based pathogen detection, we collected metagenome sequences from 13 Bovine Respiratory Disease Complex (BRDC) lung specimens at a local (name redacted to preserve anonymity) diagnostic laboratory using the DNeasy Blood and Tissue Kit (Qiagen, Hilden, Germany). ","For constructing the dataset for the training and evaluation of automated metagenome-based pathogen detection, we collected metagenome sequences from 13 Bovine Respiratory Disease Complex (BRDC) lung specimens at a local (name redacted to preserve anonymity) diagnostic laboratory using the DNeasy Blood and Tissue Kit (Qiagen, Hilden, Germany). 
Clinical metagenome samples from 7 patients were used for training, 1 for validation, while sequences from 5 patients were used for evaluation.",Private,"For constructing the dataset for the training and evaluation of automated metagenome-based pathogen detection, we collected metagenome sequences from 13 Bovine Respiratory Disease Complex (BRDC) lung specimens at a local (name redacted to preserve anonymity) diagnostic laboratory using the DNeasy Blood and Tissue Kit (Qiagen, Hilden, Germany). ",No,Nothing is mentioned,No,Nothing is mentioned,Yes,For this article,Yes,"(1)
Department of Computer Science, Oklahoma State University, Stillwater, OK, USA
(2)
Oklahoma Animal Disease Diagnostic Laboratory, College of Veterinary Medicine, Oklahoma State University, Stillwater, OK, USA

This research was supported in part by the US Department of Agriculture (USDA) grants AP20VSD and B000C011.

We thank Dr. Kitty Cardwell and Dr. Andres Espindola (Institute of Biosecurity and Microbial Forensics, Oklahoma State University) for providing access and assisting with use of the MiFi platform.",Yes,"For constructing the dataset for the training and evaluation of automated metagenome-based pathogen detection, we collected metagenome sequences from 13 Bovine Respiratory Disease Complex (BRDC) lung specimens at a local (name redacted to preserve anonymity) diagnostic laboratory using the DNeasy Blood and Tissue Kit (Qiagen, Hilden, Germany).

Mentioning the redacting of the name for privacy",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​57) contains supplementary material, which is available to authorized users.",
20/10/2022 08.57.28,304,2021,Yes,It was accurately labelled,"In this paper, a classifier is trained to detect embryo development stage with learning strategies added to explicitly address challenges of this task.","In this paper, a classifier is trained to detect embryo development stage with learning strategies added to explicitly address challenges of this task.","In Vitro Fertilization (IVF) treatment is increasingly chosen by couples suffering from infertility as a means to conceive. Time-lapse imaging technology has enabled continuous monitoring of embryos in vitro and time-based development metrics for assessing embryo quality prior to transfer. Timing at which embryos reach certain development stages provides valuable information about their potential to become a positive pregnancy. Automating development stage detection of day 4–5 embryos remains difficult due to small variation between stages. 

With infertility rates rising, more than 100,000 embryo transfers for IVF treatment are performed annually in the US ",Neural network,"Precision, Recall, F1 score",No,Nothing is mentioned,No,"Experiments were performed on a dataset of 117 human embryo time-lapse imaging sequences of days 1–5 in vitro collected at the Pacific Centre for Reproductive Medicine from 2017–2019. Image frames were acquired every 15 min at a single focal plane. Morula and blastocyst stage onset times were annotated by an embryologist. All frames following the morula and blastocyst stage onset were labeled as morula and blastocyst stage for classification, respectively. ","Experiments were performed on a dataset of 117 human embryo time-lapse imaging sequences of days 1–5 in vitro collected at the Pacific Centre for Reproductive Medicine from 2017–2019. Image frames were acquired every 15 min at a single focal plane. Morula and blastocyst stage onset times were annotated by an embryologist. All frames following the morula and blastocyst stage onset were labeled as morula and blastocyst stage for classification, respectively. 
",Private,"Since no publicly available dataset exists for this task, embryo sequences were randomly partitioned into training, validation, and test sets using ratio 70/15/15%, respectively. ",No,Nothing is mentioned,No,"Nothing is mentioned, not really relevant",No,Nothing is mentioned,No,"(1)
Simon Fraser University, Burnaby, Canada
(2)
Pacific Centre for Reproductive Medicine, Burnaby, Canada",No,"Nothing is mentioned, not really relevant - can an embryo give consent? But parents could have, and hopefully did..",No,Nothing is mentioned,No,Nothing is mentioned,Yes,Code is available: https://​github.​com/​llockhar/​Embryo-Stage-Onset-Detection.,
20/10/2022 08.46.59,301,2021,Yes,It was accurately labelled,"Finally, considering the complexity of hematomas distribution in human brain, we build a classification model to automatically identify the situation when hematoma breaks into brain ventricles and formulate a midline correction strategy to locally adjust the midline according to the location and boundary of hematomas.","Our work has four highlights. First, we propose to formulate the brain midline delineation as a 3D hemisphere segmentation task, which recognizes the midline location via enriched anatomical features. Second, we employ a distance-weighted map for midline aware loss. Third, we introduce rectificative learning for the model to handle various head poses. Finally, considering the complexity of hematomas distribution in human brain, we build a classification model to automatically identify the situation when hematoma breaks into brain ventricles and formulate a midline correction strategy to locally adjust the midline according to the location and boundary of hematomas. To our best knowledge, it is the first study focusing on delineating the brain midline on 3D CT images of hematoma patients and handling the situation of ventricle break-in. ","Brain midline delineates the boundary between the two cerebral hemispheres of the human brain, which plays a significant role in guiding intracranial hemorrhage surgery. Large midline shift caused by hematomas remains an inherent challenge for delineation. However, most previous methods only handle normal brains and delineate the brain midline on 2D CT images. In this study, we propose a novel hemisphere-segmented framework (HSF) for generating smooth 3D brain midline especially when large hematoma shifts the midline. 

To our best knowledge, it is the first study focusing on delineating the brain midline on 3D CT images of hematoma patients and handling the situation of ventricle break-in. ",Neural network,Accuracy,Yes,we build a strong segmentation network based on classic 3D-Unet architecture,No," the experiments were implemented on a dataset consisting of 300 CT volumes collected from our collaborating hospitals with approval from the local research ethics committee. The dataset contains 170 non-interaction, 90 non-break-in and 40 break-in examples. All the ground truths were manually annotated by clinicians. Annotation results were double-checked under strict quality control of an experienced expert. The labeled dataset was randomly divided into a training, validation and testing groups at a ratio of 3 : 1 : 1. Volume size in our dataset varies from subject to subject. Specifically, the maximum original size in the dataset is 512 $$\times $$ 512 $$\times $$ 172. Voxel spacing is resampled as 1.0 $$\times $$ 1.0 $$\times $$ 1.0 mm$$^{3}$$. Limited by GPU memory, we randomly cropped the volume size to 192 $$\times $$ 192 $$\times $$ 160. In this study, all the methods were implemented in PyTorch, in which our models ran 150 epochs in a single P40 GPU with 24 GB RAM. Adam with a batch size of 2 and an initial learning rate of 0.0001 was adopted to optimize the models."," the experiments were implemented on a dataset consisting of 300 CT volumes collected from our collaborating hospitals with approval from the local research ethics committee. The dataset contains 170 non-interaction, 90 non-break-in and 40 break-in examples. All the ground truths were manually annotated by clinicians. Annotation results were double-checked under strict quality control of an experienced expert. The labeled dataset was randomly divided into a training, validation and testing groups at a ratio of 3 : 1 : 1. Volume size in our dataset varies from subject to subject. Specifically, the maximum original size in the dataset is 512 $$\times $$ 512 $$\times $$ 172. Voxel spacing is resampled as 1.0 $$\times $$ 1.0 $$\times $$ 1.0 mm$$^{3}$$. Limited by GPU memory, we randomly cropped the volume size to 192 $$\times $$ 192 $$\times $$ 160. In this study, all the methods were implemented in PyTorch, in which our models ran 150 epochs in a single P40 GPU with 24 GB RAM. Adam with a batch size of 2 and an initial learning rate of 0.0001 was adopted to optimize the models.",Private," the experiments were implemented on a dataset consisting of 300 CT volumes collected from our collaborating hospitals with approval from the local research ethics committee. The dataset contains 170 non-interaction, 90 non-break-in and 40 break-in examples. All the ground truths were manually annotated by clinicians. Annotation results were double-checked under strict quality control of an experienced expert. The labeled dataset was randomly divided into a training, validation and testing groups at a ratio of 3 : 1 : 1. Volume size in our dataset varies from subject to subject. Specifically, the maximum original size in the dataset is 512 $$\times $$ 512 $$\times $$ 172. Voxel spacing is resampled as 1.0 $$\times $$ 1.0 $$\times $$ 1.0 mm$$^{3}$$. Limited by GPU memory, we randomly cropped the volume size to 192 $$\times $$ 192 $$\times $$ 160. In this study, all the methods were implemented in PyTorch, in which our models ran 150 epochs in a single P40 GPU with 24 GB RAM. Adam with a batch size of 2 and an initial learning rate of 0.0001 was adopted to optimize the models.",No,Nothing is mentioned,No,Nothing is mentioned,No,"Maybe for this article: 
the experiments were implemented on a dataset consisting of 300 CT volumes collected from our collaborating hospitals with approval from the local research ethics committee. ",No,"(1)
Tencent AI Lab, Shenzhen, China
(2)
Peking Union Medical College Hospital, Beijing, China",Yes,"the experiments were implemented on a dataset consisting of 300 CT volumes collected from our collaborating hospitals with approval from the local research ethics committee. 

Though this does not explicitly mention consent from patients, someone still approved it",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
11/10/2022 09.00.34,294,2021,Yes,It was accurately labelled,Predicting Symptoms from Multiphasic MRI via Multi-instance Attention Learning for Hepatocellular Carcinoma Grading,we propose a two-stage method for automatically predicting HCC grades according to multiphasic magnetic resonance imaging (MRI). ,"Liver cancer is the third leading cause of cancer death in the world, where the hepatocellular carcinoma (HCC) is the most common case in primary liver cancer. In general diagnosis, accurate prediction of HCC grades is of great help to the subsequent treatment to improve the survival rate. Rather than to straightly predict HCC grades from images, it will be more interpretable in clinic to first predict the symptoms and then obtain the HCC grades from the Liver Imaging Reporting and Data System (LI-RADS). Accordingly, we propose a two-stage method for automatically predicting HCC grades according to multiphasic magnetic resonance imaging (MRI). ",Neural network,"Accuracy, Precision, Recall, F1 score",No,Nothing is mentioned,No,"We evaluate our method on a dataset of 439 subjects, including 93, 98, and 248 HCC cases of grading LR-3, LR-4, and LR-5, respectively. All data were obtained using either a 1.5 T (Magnetom Aera, Siemens Healthcare, Erlangen, Germany) or a 3.0 T (uMR 770, United Imaging Healthcare, Shanghai, China) MR scanner with Gd-EOB-DTPA as contrast agent in same hospital and labeled by three radiologists with experience over 5 years. ","We evaluate our method on a dataset of 439 subjects, including 93, 98, and 248 HCC cases of grading LR-3, LR-4, and LR-5, respectively. All data were obtained using either a 1.5 T (Magnetom Aera, Siemens Healthcare, Erlangen, Germany) or a 3.0 T (uMR 770, United Imaging Healthcare, Shanghai, China) MR scanner with Gd-EOB-DTPA as contrast agent in same hospital and labeled by three radiologists with experience over 5 years. ",Private,"We evaluate our method on a dataset of 439 subjects, including 93, 98, and 248 HCC cases of grading LR-3, LR-4, and LR-5, respectively. All data were obtained using either a 1.5 T (Magnetom Aera, Siemens Healthcare, Erlangen, Germany) or a 3.0 T (uMR 770, United Imaging Healthcare, Shanghai, China) MR scanner with Gd-EOB-DTPA as contrast agent in same hospital and labeled by three radiologists with experience over 5 years. ",Yes,"We evaluate our method on a dataset of 439 subjects, including 93, 98, and 248 HCC cases of grading LR-3, LR-4, and LR-5, respectively. All data were obtained using either a 1.5 T (Magnetom Aera, Siemens Healthcare, Erlangen, Germany) or a 3.0 T (uMR 770, United Imaging Healthcare, Shanghai, China) MR scanner with Gd-EOB-DTPA as contrast agent in same hospital and labeled by three radiologists with experience over 5 years. ",No,Nothing is mentioned,Yes,"I think for this article:
We evaluate our method on a dataset of 439 subjects, including 93, 98, and 248 HCC cases of grading LR-3, LR-4, and LR-5, respectively. All data were obtained using either a 1.5 T (Magnetom Aera, Siemens Healthcare, Erlangen, Germany) or a 3.0 T (uMR 770, United Imaging Healthcare, Shanghai, China) MR scanner with Gd-EOB-DTPA as contrast agent in same hospital and labeled by three radiologists with experience over 5 years. ",Yes,"(1)
National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, Shaanxi, China
(2)
Research and Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen, 518057, China
(3)
School of Biomedical Engineering, ShanghaiTech University, Shanghai, China
(4)
Shanghai United Imaging Intelligence Co., Ltd., Shanghai, China

This work was supported in part by the National Natural Science Foundation of China under Grants 61771397, in part by the CAAI-Huawei MindSpore Open Fund under Grants CAAIXSJLJJ-2020-005B, and in part by the China Postdoctoral Science Foundation under Grants BX2021333.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
07/10/2022 12.53.28,284,2021,Yes,It was accurately labelled,We curated a dataset of seizure videos from 68 patients and evaluated GESTURES on its ability to classify seizures ,"Detailed analysis of seizure semiology, the symptoms and signs which occur during a seizure, is critical for management of epilepsy patients. Inter-rater reliability using qualitative visual analysis is often poor for semiological features. Therefore, automatic and quantitative analysis of video-recorded seizures is needed for objective assessment. We present GESTURES, a novel architecture combining convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn deep representations of arbitrarily long videos of epileptic seizures.","Detailed analysis of seizure semiology, the symptoms and signs which occur during a seizure, is critical for management of epilepsy patients. Inter-rater reliability using qualitative visual analysis is often poor for semiological features. Therefore, automatic and quantitative analysis of video-recorded seizures is needed for objective assessment. We present GESTURES, a novel architecture combining convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn deep representations of arbitrarily long videos of epileptic seizures.",Neural network,"AUC, Accuracy, F1 score",No,Don't think they mention it,No,"We curated a dataset comprising 141 FOSs and 77 TCSs videos from 68 epileptic patients undergoing presurgical evaluation at the National Hospital for Neurology and Neurosurgery, London, United Kingdom.

","We curated a dataset comprising 141 FOSs and 77 TCSs videos from 68 epileptic patients undergoing presurgical evaluation at the National Hospital for Neurology and Neurosurgery, London, United Kingdom.

",Private,"We curated a dataset comprising 141 FOSs and 77 TCSs videos from 68 epileptic patients undergoing presurgical evaluation at the National Hospital for Neurology and Neurosurgery, London, United Kingdom.

",Yes,"Patients were recorded using two full high-definition ($$1920 \times 1080$$ pixels, 30 frames per second (FPS)) cameras installed in the EMU as part of standard clinical practice. Infrared is used for acquisition in scenes with low light intensity, such as during nighttime. The acquisition software (Micromed, Treviso, Italy) automatically resizes one of the video streams ($$800 \times 450$$), superimposes it onto the top-left corner of the other stream and stores the montage using MPEG-2. See the supplementary materials for six examples of videos in our dataset.

",No,Nothing is mentioned,Yes,For the article,Yes,"(1)
Department of Medical Physics and Biomedical Engineering, University College London, London, UK
(2)
Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, London, UK
(3)
School of Biomedical Engineering and Imaging Sciences (BMEIS), King’s College London, London, UK
(4)
Department of Clinical and Experimental Epilepsy, UCL Queen Square Institute of Neurology, London, UK
(5)
Department of Clinical Neurophysiology, National Hospital for Neurology and Neurosurgery, London, UK

This work is supported by the Engineering and Physical Sciences Research Council (EPSRC) [EP/R512400/1]. This work is additionally supported by the EPSRC-funded UCL Centre for Doctoral Training in Intelligent, Integrated Imaging in Healthcare (i4health) [EP/S021930/1] and the Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS, UCL) [203145Z/16/Z]. The data acquisition was supported by the National Institute of Neurological Disorders and Stroke [U01-NS090407].

This publication represents, in part, independent research commissioned by the Wellcome Innovator Award [218380/Z/19/Z/]. The views expressed in this publication are those of the authors and not necessarily those of the Wellcome Trust.

The weights for the 2D and 3D models were downloaded from TorchVision and https://​github.​com/​moabitcoin/​ig65m-pytorch, respectively.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"The code, models and features dataset are available at https://​github.​com/​fepegar/​gestures-miccai-2021.
Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​32) contains supplementary material, which is available to authorized users
",
04/10/2022 11.20.08,280,2021,Yes,It was accurately labelled,"This work aims at developing and evaluating a deep learning-based framework, named VinDr-SpineXR, for the classification and localization of abnormalities from spine X-rays","This work aims at developing and evaluating a deep learning-based framework, named VinDr-SpineXR, for the classification and localization of abnormalities from spine X-rays","Radiographs are used as the most important imaging tool for identifying spine anomalies in clinical practice. The evaluation of spinal bone lesions, however, is a challenging task for radiologists",Neural network,"AUC, Specificity, F1 score, Sensitivity",No,Nothing is mentioned,No,Nothing is mentioned," In this work, more than 50,000 raw spine images in DICOM format were retrospectively collected from the Picture Archive and Communication System (PACS) of different primary hospitals between 2010–2020. ",Private," In this work, more than 50,000 raw spine images in DICOM format were retrospectively collected from the Picture Archive and Communication System (PACS) of different primary hospitals between 2010–2020. ",No,The data collection process was conducted under our cooperation with participating hospitals.,Yes," Since this research did not impact clinical care, patient consent was waived. To keep patient’s Protected Health Information (PHI) secure, all patient-identifiable information associated with the images has been removed. Several DICOM attributes that are important for evaluating the spine conditions like patient’s age and sex were retained.",Yes,For this article,No,"(1)
Medical Imaging Center, Vingroup Big Data Institute, Hanoi, Vietnam
(2)
School of Information and Communication Technology, Hanoi University of Science and Technology, Hanoi, Vietnam
(3)
College of Engineering and Computer Science, VinUniversity, Hanoi, Vietnam
(4)
Department of Mathematics, Yale University, New Heaven, USA  Nothing is mentioned",Yes,"Since this research did not impact clinical care, patient consent was waived. To keep patient’s Protected Health Information (PHI) secure, all patient-identifiable information associated with the images has been removed. Several DICOM attributes that are important for evaluating the spine conditions like patient’s age and sex were retained.",No,Nothing is mentioned,No,Nothing is mentioned,Yes,Supplementary electronic material,Interesting addition about the patients rights!
04/10/2022 11.13.09,278,2021,Yes,It was accurately labelled,"When evaluated on a separate biopsy-proven dataset, ADDLE outperforms standard classifiers as well as leading annotator noise competitors [4, 9].","we introduce auto-decoded deep latent embeddings (ADDLE), which explicitly models the tendencies of each rater using an auto-decoder framework. ","Depending on the application, radiological diagnoses can be associated with high inter- and intra-rater variabilities. Most computer-aided diagnosis (CAD) solutions treat such data as incontrovertible, exposing learning algorithms to considerable and possibly contradictory label noise and biases. Thus, managing subjectivity in labels is a fundamental problem in medical imaging analysis.",Neural network,AUC,No,Nothing is mentioned,No,"To this end, we collected a big-data (BD) cohort consisting of $$3\,790$$ patients, $$312\,848$$ US images, and $$36\,602$$ US studies from the PACS of Anonymized. Through the course of clinical care, each study was given a four-class ordinal assessment of either healthy, mild, moderate, or severe steatosis from one of 65 clinicians. We used $$3\,405$$ patients for training and left the rest as a stopping criteria validation set. ","To this end, we collected a big-data (BD) cohort consisting of $$3\,790$$ patients, $$312\,848$$ US images, and $$36\,602$$ US studies from the PACS of Anonymized. Through the course of clinical care, each study was given a four-class ordinal assessment of either healthy, mild, moderate, or severe steatosis from one of 65 clinicians. We used $$3\,405$$ patients for training and left the rest as a stopping criteria validation set. ",Private,"To this end, we collected a big-data (BD) cohort consisting of $$3\,790$$ patients, $$312\,848$$ US images, and $$36\,602$$ US studies from the PACS of Anonymized. Through the course of clinical care, each study was given a four-class ordinal assessment of either healthy, mild, moderate, or severe steatosis from one of 65 clinicians. We used $$3\,405$$ patients for training and left the rest as a stopping criteria validation set. ",No,"To this end, we collected a big-data (BD) cohort consisting of $$3\,790$$ patients, $$312\,848$$ US images, and $$36\,602$$ US studies from the PACS of Anonymized. Through the course of clinical care, each study was given a four-class ordinal assessment of either healthy, mild, moderate, or severe steatosis from one of 65 clinicians. We used $$3\,405$$ patients for training and left the rest as a stopping criteria validation set. ",No,Nothing is mentioned,Yes,"For this article: To this end, we collected a big-data (BD) cohort consisting of $$3\,790$$ patients, $$312\,848$$ US images, and $$36\,602$$ US studies from the PACS of Anonymized",No,"(1)
PAII Inc., Bethesda, MD 20817, USA
(2)
Ruijin Hospital, Shanghai, China
(3)
Chang Gung Memorial Hospital, Linkou, Taoyuan, Taiwan ROC
(4)
PingAn Technology, Shenzhen, China  Nothing is mentioned",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,Supplementary electronic material,
04/10/2022 11.08.07,277,2021,Yes,It was accurately labelled,"In this work, we investigate the feasibility of using a single-phase non-contrast CT scan, a cheaper, simpler, and safer substituent, to detect resectable pancreatic mass and classify the detection as pancreatic ductal adenocarcinoma (PDAC) or other abnormalities (nonPDAC) or normal pancreas.","In this work, we investigate the feasibility of using a single-phase non-contrast CT scan, a cheaper, simpler, and safer substituent, to detect resectable pancreatic mass and classify the detection as pancreatic ductal adenocarcinoma (PDAC) or other abnormalities (nonPDAC) or normal pancreas.","Pancreatic cancer is a relatively uncommon but most deadly cancer. Screening the general asymptomatic population is not recommended due to the risk that a significant number of false positive individuals may undergo unnecessary imaging tests (e.g., multi-phase contrast-enhanced CT scans) and follow-ups, adding health care costs greatly and no clear patient benefits",Neural network,"AUC, Specificity, Sensitivity",Yes,Segmentation for classification is the most straightforward and adopted representation of the task of pancreatic tumor detection.,No,"Our dataset of CT scans of 1627 patients, is consecutively collected in the years of 2016–2018 from a high-volume pancreatic cancer institution. ","Our dataset of CT scans of 1627 patients, is consecutively collected in the years of 2016–2018 from a high-volume pancreatic cancer institution. ",Private,"Our dataset of CT scans of 1627 patients, is consecutively collected in the years of 2016–2018 from a high-volume pancreatic cancer institution. ",No,"Our dataset of CT scans of 1627 patients, is consecutively collected in the years of 2016–2018 from a high-volume pancreatic cancer institution. ",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Johns Hopkins University, Baltimore, USA
(2)
PAII Inc., Bethesda, USA
(3)
PingAn Technology, Shenzhen, China
(4)
Changhai Hospital, Shanghai, China   Y. Xia—Work done during an internship at PAII Inc.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
27/09/2022 11.27.05,258,2021,Yes,It was accurately labelled,"Ultrasound (US) imaging is a fundamental modality for detecting and diagnosing breast lesions, while shear-wave elastography (SWE) serves as a crucial complementary counterpart. Although an automated breast lesion classification system is desired, training of such a system is constrained by data scarcity and modality imbalance problems due to the lack of SWE devices in rural hospitals. ","To enhance the diagnosis with only US available, in this work, we propose a knowledge-guided data augmentation framework, which consists of a modal translater and a semantic inverter, achieving cross-modal and semantic data augmentation simultaneously. ","Breast cancer, the most commonly diagnosed cancer, is the fifth leading cause of cancer death all over the world [24].",Neural network,"AUC, Accuracy, Precision, Recall, F1 score",No,Nothing is mentioned,No,"From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. ","From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. ",Private,"From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. ",Yes,"The Super Linear SL-15-4 probe of ultrafast ultrasound device Aixplorer (Super Sonic Imagine, Aix-en-Provence, France) was used for imaging data collection. The maximum stiffness scale of SWE images was selected as 180 Kilopascal (kPa). ",No,Nothing is mentioned,Yes,"From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. ",Yes,"This work was supported in part by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), 111 project (BP0719010), Shanghai Science and Technology Committee (18DZ2270700) and Shanghai Jiao Tong University Science and Technology Innovation Special Fund (ZH2018ZDA17).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​6) contains supplementary material, which is available to authorized users.",
21/10/2022 08.53.54,324,2021,Yes,It was accurately labelled,"In this paper, we present an end-to-end autoencoder-based multiple instance neural network (AMINN) for the prediction of survival outcomes in multifocal CRLM patients using radiomic features extracted from contrast-enhanced MRIs.","In this paper, we present an end-to-end autoencoder-based multiple instance neural network (AMINN) for the prediction of survival outcomes in multifocal CRLM patients using radiomic features extracted from contrast-enhanced MRIs.","Colorectal cancer is one of the most common and lethal cancers and colorectal cancer liver metastases (CRLM) is the major cause of death in patients with colorectal cancer. Multifocality occurs frequently in CRLM, but is relatively unexplored in CRLM outcome prediction. Most existing clinical and imaging biomarkers do not take the imaging features of all multifocal lesions into account. ",Neural network,"AUC, Accuracy",No,Nothing is mentioned,No,We used a retrospective cohort of 108 colorectal cancer liver metastases (CRLM) patients that were eligible for hepatic resection and were treated at our institution.,We used a retrospective cohort of 108 colorectal cancer liver metastases (CRLM) patients that were eligible for hepatic resection and were treated at our institution.,Private,We used a retrospective cohort of 108 colorectal cancer liver metastases (CRLM) patients that were eligible for hepatic resection and were treated at our institution.,No,Nothing is mentioned,No,"The cohort has been described in detail in previous work [3]. Demographics of the multifocal cohort is available in Table S2.

Another one where the table is not in the article... and the description is in another article - I guess for space reasons..",No,Nothing is mentioned,Yes,"Department of Medical Biophysics, University of Toronto, Toronto, ON, Canada
(2)
Department of Medical Imaging, University of Toronto, Toronto, ON, Canada
(3)
Sunnybrook Research Institute, Toronto, ON, Canada
(4)
Centre Hospitalier Universitaire de Lyon, Lyon, France


The authors would like to thank The Natural Sciences and Engineering Research Council of Canada (NSERC) for funding, and acknowledge the contribution of Drs. Karanicolas, Law and Coburn in helping to create the patient cohort for this study.",Yes,"We used a retrospective cohort of 108 colorectal cancer liver metastases (CRLM) patients that were eligible for hepatic resection and were treated at our institution. Informed consent was waived by the institutional review board.

So not necessarily a good thing..",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"A Keras implementation of AMINN is released (https://​github.​com/​martellab-sri/​AMINN).

Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​72) contains supplementary material, which is available to authorized users.",
21/10/2022 09.15.55,331,2021,Yes,It was accurately labelled,"Finally, we ensemble the patch classification scores to calculate patient-wise grades.",COVID-19 image analysis has mostly focused on diagnostic tasks using single timepoint scans acquired upon disease presentation or admission. We present a deep learning-based approach to predict lung infiltrate progression from serial chest radiographs (CXRs) of COVID-19 patients.,"Coronavirus disease 2019 (COVID-19) remains at the forefront of threats to public health. As a result, there continues to be a critical need to further understand the progression of the disease process.
Despite the many studies analyzing the use of CXRs in COVID-19, machine learning applications have been limited to diagnostic tasks including differentiating COVID-19 from viral pneumonia and predicting clinical outcomes such as mortality and mechanical ventilation requirement ",Neural network,"Accuracy, Precision, Recall",No,Nothing is mentioned,Yes,"Our multi-institutional dataset, COVIDProg [5], contains 621 antero-posterior CXR scans from 93 COVID-19 patients, collected from multiple days. 23 cases were obtained from Newark Beth Israel Medical Center. The remaining 70 cases curated from Stony Brook University Hospital. All the CXRs were of dimension $$3470\times 4234$$. Additional details can be found in Supplementary Sect. 3.","Our multi-institutional dataset, COVIDProg [5], contains 621 antero-posterior CXR scans from 93 COVID-19 patients, collected from multiple days. 23 cases were obtained from Newark Beth Israel Medical Center. The remaining 70 cases curated from Stony Brook University Hospital. All the CXRs were of dimension $$3470\times 4234$$. Additional details can be found in Supplementary Sect. 3.",Private,Nothing else is mentioned,No,Nothing is mentioned,No,"Nothing is mentioned, apart from: Additional details can be found in Supplementary Sect. 3.",No,Nothing is mentioned,Yes,"
Department of Computer Science, Stony Brook University, Stony Brook, NY, USA
(2)
Department of Biomedical Informatics, Stony Brook University, Stony Brook, NY, USA
(3)
Department of Radiology, Newark Beth Israel Medical Center, Newark, NJ, USA


Reported research was supported by the OVPR and IEDM seed grants, 2020 at Stony Brook University, NIGMS T32GM008444, and NIH 75N92020D00021 (subcontract). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​79) contains supplementary material, which is available to authorized users.",
26/10/2022 09.49.07,170,2021,Yes,It was accurately labelled,"Abstract
In order to ensure that a radiomics-based machine learning model will robustly generalize to new, unseen data (which may harbor significant variations compared to the discovery cohort), radiomic features are often screened for stability via test/retest or cross-site evaluation. However, as stability screening is often conducted independent of the feature selection process, the resulting feature set may not be simultaneously optimized for discriminability, stability, as well as sparsity. In this work, we present a novel radiomic feature selection approach termed SPARse sTable lAsso (SPARTA), uniquely developed to identify a highly discriminative and sparse set of features which are also stable to acquisition or institution variations. The primary contribution of this work is the integration of feature stability as a generalizable regularization term into a least absolute shrinkage and selection operator (LASSO)-based optimization function. Secondly, we utilize a unique non-convex sparse relaxation approach inspired by proximal algorithms to provide a computationally efficient convergence guarantee for our novel algorithm. SPARTA was evaluated on three different multi-institutional imaging cohorts to identify the most relevant radiomic features for distinguishing: (a) healthy from diseased lesions in 147 prostate cancer patients via T2-weighted MRI, (b) healthy subjects from Crohn’s disease patients via 170 CT enterography scans, and (c) responders and non-responders to chemoradiation in 82 rectal cancer patients via T2w MRI. When compared to 3 state-of-the-art feature selection schemes, features selected via SPARTA yielded significantly higher classifier performance on unseen data in multi-institutional validation (hold-out AUCs of 0.91, 0.91, and 0.93 in the 3 cohorts).","In order to ensure that a radiomics-based machine learning model will robustly generalize to new, unseen data (which may harbor significant variations compared to the discovery cohort), radiomic features are often screened for stability via test/retest or cross-site evaluation. However, as stability screening is often conducted independent of the feature selection process, the resulting feature set may not be simultaneously optimized for discriminability, stability, as well as sparsity. In this work, we present a novel radiomic feature selection approach termed SPARse sTable lAsso (SPARTA), uniquely developed to identify a highly discriminative and sparse set of features which are also stable to acquisition or institution variations","In radiomics [1] applications, feature selection strategies such as maximum relevance minimum redundancy (mRMR) [2], least absolute shrinkage and selection operator (LASSO) [3], or Wilcoxon rank-sum testing (WLCX) [4], are widely used to find a relatively parsimonious (i.e. sparse) subset of features, which in combination are able to accurately distinguish between any classes of interest (i.e. are discriminatory). Critically, the ultimate validation of these radiomics models is in a multi-institutional setting where unseen datasets may suffer from significant variations compared to the original discovery cohort. Towards this, it has become critical to determine whether selected radiomic features are stable or reproducible within an institution (intra-site), between institutions (inter-site), as well as in repeated test/re-test evaluation [5]. The underlying hypothesis here is that stable radiomic features which are also discriminable will result in better model generalizability on new, unseen data.",Random Forest,AUC,No,Nothing is mentioned,No,"All experiments were conducting using three different, multi-institutional, retrospectively accrued cohorts that had been segregated into independent discovery and validation sets (see Table 1).

1.	
C1 (prostate cancer) comprised 147 diagnostic T2-weighted (T2w) prostate MRIs from 4 institutions, with the goal of distinguishing benign from malignant lesions in the peripheral zone (discovery: 3 sites, validation: 1 site).

 
2.	
C2 (Crohn’s disease) comprised 170 CT enterography scans from patients being screened for Crohn’s disease with endoscopic confirmation of disease presence. The goal was to distinguish between healthy and diseased terminal ileum regions within this single institutional cohort harboring significant batch effects [25] due to changes in acquisition parameters.

 
3.	
C3 (rectal cancer) comprised 82 post-treatment T2w rectal MRIs from two institutions, where the goal is to distinguish pathologic complete response (pCR) or non-response to chemoradiation based on rectal wall regions (discovery: 1 site, validation: 1 site).

 
Implementation: As summarized in Fig. 1, total of 405 3D radiomic features were extracted on a voxel-wise basis from all imaging scans, after which the mean value of each feature was computed within expert annotated ROIs","All experiments were conducting using three different, multi-institutional, retrospectively accrued cohorts that had been segregated into independent discovery and validation sets (see Table 1).

1.	
C1 (prostate cancer) comprised 147 diagnostic T2-weighted (T2w) prostate MRIs from 4 institutions, with the goal of distinguishing benign from malignant lesions in the peripheral zone (discovery: 3 sites, validation: 1 site).

 
2.	
C2 (Crohn’s disease) comprised 170 CT enterography scans from patients being screened for Crohn’s disease with endoscopic confirmation of disease presence. The goal was to distinguish between healthy and diseased terminal ileum regions within this single institutional cohort harboring significant batch effects [25] due to changes in acquisition parameters.

 
3.	
C3 (rectal cancer) comprised 82 post-treatment T2w rectal MRIs from two institutions, where the goal is to distinguish pathologic complete response (pCR) or non-response to chemoradiation based on rectal wall regions (discovery: 1 site, validation: 1 site).

 
Implementation: As summarized in Fig. 1, total of 405 3D radiomic features were extracted on a voxel-wise basis from all imaging scans, after which the mean value of each feature was computed within expert annotated ROIs",Private,"Nothing is mentioned, making me think private",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Case Western Reserve University, Cleveland, OH 44106, USA
(2)
Louis Stokes Cleveland VA Medical Center, Cleveland, OH 44106, USA

Research supported by NCI (1U24CA199374-01, 1R01CA249992-01A1, 1R01CA202752-01A1, 1R01CA208236-01A1, 1R01CA216579-01A1, 1R01CA220581-01A1, 1R01CA257612-01A1, 1U01CA239055-01, 1U01CA248226-01, 1U54CA254566-01, 1F31CA216935-01A1), NHLBI (R01HL15127701A1), NIBIB (1R43EB028736-01), NCRR (1C06RR12463-01), DOD/CDMRP (W81XWH-19-1-0668, W81XWH-15-1-0558, W81XWH-20-1-0851, W81XWH-18-1-0440, W81XWH-20-1-0595, W81XWH-18-1-0404, CA200789), VA (IBX004121A Merit Review Award), the KPMP Glue Grant, the Ohio Third Frontier Technology Validation Fund, the CTSC of Cleveland (UL1TR0002548), the Wallace H. Coulter Foundation Program in the Department of Biomedical Engineering at Case Western Reserve University, as well as sponsored research agreements from Bristol Myers-Squibb, Boehringer-Ingelheim, and Astrazeneca. Content solely responsibility of the authors and does not necessarily represent the official views of the NIH, USDVA, DOD, or the United States Government.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​42) contains supplementary material, which is available to authorized users.",
27/10/2022 10.34.21,82,2021,Yes,It was accurately labelled,"Most deep-learning based magnetic resonance image (MRI) analysis methods require numerous amounts of labelling work manually done by specialists, which is laborious and time-consuming. In this paper, we aim to develop a hybrid-supervised model generation strategy, called SpineGEM, which can economically generate a high-performing deep learning model for the classification of multiple pathologies of lumbar degeneration disease (LDD).","Most deep-learning based magnetic resonance image (MRI) analysis methods require numerous amounts of labelling work manually done by specialists, which is laborious and time-consuming. In this paper, we aim to develop a hybrid-supervised model generation strategy, called SpineGEM, which can economically generate a high-performing deep learning model for the classification of multiple pathologies of lumbar degeneration disease (LDD).","Most deep-learning based magnetic resonance image (MRI) analysis methods require numerous amounts of labelling work manually done by specialists, which is laborious and time-consuming. In this paper, we aim to develop a hybrid-supervised model generation strategy, called SpineGEM, which can economically generate a high-performing deep learning model for the classification of multiple pathologies of lumbar degeneration disease (LDD).


Magnetic resonance imaging (MRI) is widely used in Orthopaedics for the assessment of spine pathologies [1, 2]. Automated MRI analyses can have great clinical significance, by improving the efficiency and consistency of diagnosis [3]. For pathology classifications, the accuracy generated by a deep learning model can be comparable with human specialists [4, 5]. However, numerous amounts of manual labels are required for the training process of the model, which is expensive, laborious, and time-consuming.","Neural network, self supervised learning","Precision, Recall",Yes," In preprocessing, an unsupervised pipeline MRI-SegFlow was adopted to generate the semantic segmentation of IVD and VB, and the regions of IVDs were identified based on the segmentation results. ",No,A dataset sourced from the Hong Kong Disc Degeneration Cohort (HKDDC) was adopted for the validation of our method. It consisted of 1600 sagittal lumbar T2-weighted MRI cases and each of them contained five IVDs from L1 to S1 (totally 8000 IVDs in the dataset).,A dataset sourced from the Hong Kong Disc Degeneration Cohort (HKDDC) was adopted for the validation of our method. It consisted of 1600 sagittal lumbar T2-weighted MRI cases and each of them contained five IVDs from L1 to S1 (totally 8000 IVDs in the dataset).,Private,A dataset sourced from the Hong Kong Disc Degeneration Cohort (HKDDC) was adopted for the validation of our method. It consisted of 1600 sagittal lumbar T2-weighted MRI cases and each of them contained five IVDs from L1 to S1 (totally 8000 IVDs in the dataset).,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"For this article, from existing dataset",No,"The University of Hong Kong, Pok Fu Lam, Hong Kong
(2)
VoxelCloud Inc., Gayley Avenue 1085, Los Angeles, CA 90024, USA",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
27/10/2022 11.58.27,106,2021,Yes,It was accurately labelled,"Abstract
The task of 3D shape classification is closely related to finding a good representation of the shapes. In this study, we focus on surface representations of complex anatomies and on how such representations can be utilized for super- and unsupervised classification. We present a novel Implicit Neural Distance Representation based on unsigned distance fields (UDFs). The UDFs can be embedded into a low-dimensional latent space, which is optimized using only the shape itself. We demonstrate that this self-optimized latent space holds important global shape information useful for reconstructing the anatomies, but also that unsupervised clustering of the latent vectors successfully separates different anatomies (left atrium, left/right ear-canals and human faces). Finally, we show how the representation can be used to do gender classification of human face geometries, which is a notoriously hard problem.","Abstract
The task of 3D shape classification is closely related to finding a good representation of the shapes. In this study, we focus on surface representations of complex anatomies and on how such representations can be utilized for super- and unsupervised classification. We present a novel Implicit Neural Distance Representation based on unsigned distance fields (UDFs). The UDFs can be embedded into a low-dimensional latent space, which is optimized using only the shape itself. We demonstrate that this self-optimized latent space holds important global shape information useful for reconstructing the anatomies, but also that unsupervised clustering of the latent vectors successfully separates different anatomies (left atrium, left/right ear-canals and human faces). Finally, we show how the representation can be used to do gender classification of human face geometries, which is a notoriously hard problem.","Being able to describe and classify complex anatomical shapes are tasks inevitably linked to finding a good representation of the shape. In the human body, a variety of complex anatomies exists and often such anatomies are represented as a 3D mesh surface either as an isosurface in an image volume or directly from a 3D surface scanner. Representing such 3D meshes in a geometric deep learning framework that can extract global shape characteristics and use them for clustering or classification is not straightforward. ",Neural network,Accuracy,No,Nothing is mentioned,Yes,"ESOF contains 3D face-scans of 394 humans with natural expressions acquired using a Canfield Vectra M3 scanner. Only the 3D geometry was used. For each scan the self-reported gender and age was noted (192/202 male/female, age 0–84 years). The raw scans were manually cropped to include the face-only region and pre-aligned using automatically annotated facial landmarks [17].
EARS contains 3D scans of 571 ear canals from humans (259/312 right/left ear) acquired with a 3Shape S200 scanner. The ear canals were roughly aligned from the scanner and accurate pre-alignment was achieved using iterative closest point (ICP) [21]. The ear canals were aligned depending on side (left/right).
LA contains 106 3D mesh models of the Left Atrium (LA) and the Left Atrial Appendage (LAA). The mesh models were created using manual segmentation of cardiac computed tomography angiographies (CCTA) supplied by the Department of Radiology, Copenhagen Central Hospital, Denmark. The segmentation was carried out in 3D Slicer2 The CCTA-images were recorded for research purposes and fully anonymized. The LA surfaces were pre-aligned using ICP with centroid matching.","ESOF contains 3D face-scans of 394 humans with natural expressions acquired using a Canfield Vectra M3 scanner. Only the 3D geometry was used. For each scan the self-reported gender and age was noted (192/202 male/female, age 0–84 years). The raw scans were manually cropped to include the face-only region and pre-aligned using automatically annotated facial landmarks [17].
EARS contains 3D scans of 571 ear canals from humans (259/312 right/left ear) acquired with a 3Shape S200 scanner. The ear canals were roughly aligned from the scanner and accurate pre-alignment was achieved using iterative closest point (ICP) [21]. The ear canals were aligned depending on side (left/right).
LA contains 106 3D mesh models of the Left Atrium (LA) and the Left Atrial Appendage (LAA). The mesh models were created using manual segmentation of cardiac computed tomography angiographies (CCTA) supplied by the Department of Radiology, Copenhagen Central Hospital, Denmark. The segmentation was carried out in 3D Slicer2 The CCTA-images were recorded for research purposes and fully anonymized. The LA surfaces were pre-aligned using ICP with centroid matching.",Private,Nothing else is mentioned,No,Nothing is mentioned,Yes," For one dataset yes: For each scan the self-reported gender and age was noted (192/202 male/female, age 0–84 years) (but also the one where they want to find out gender, so necessary)",No,Nothing is mentioned,Yes,"
Section for Image Computing, Technical University of Denmark, Kgs. Lyngby, Denmark
(2)
Physense, Department of Information and Communication Technologies, Universitat Pompeu Fabra, Barcelona, Spain
(3)
Department of Cardiology, Rigshospitalet, University of Copenhagen, Copenhagen, Denmark

This work was supported by a PhD grant from the Technical University of Denmark - Department of Applied Mathematics and Computer Science (DTU Compute) and the Spanish Ministry of Science, Innovation and Universities under the Retos I+D Programme (RTI2018-101193-B-I00).",Yes,Participants in all three datasets gave informed consent for use of their data.,No,Nothing is mentioned,No,Nothing is mentioned,Yes,All code is publicly available1.,
27/10/2022 12.02.10,112,2021,Yes,It was accurately labelled,"Abstract
Ulcerative colitis (UC) classification, which is an important task for endoscopic diagnosis, involves two main difficulties. First, endoscopic images with the annotation about UC (positive or negative) are usually limited. Second, they show a large variability in their appearance due to the location in the colon. Especially, the second difficulty prevents us from using existing semi-supervised learning techniques, which are the common remedy for the first difficulty. In this paper, we propose a practical semi-supervised learning method for UC classification by newly exploiting two additional features, the location in a colon (e.g., left colon) and image capturing order, both of which are often attached to individual images in endoscopic image sequences. The proposed method can extract the essential information of UC classification efficiently by a disentanglement process with those features. Experimental results demonstrate that the proposed method outperforms several existing semi-supervised learning methods in the classification task, even with a small number of annotated images.","Abstract
Ulcerative colitis (UC) classification, which is an important task for endoscopic diagnosis, involves two main difficulties. First, endoscopic images with the annotation about UC (positive or negative) are usually limited. Second, they show a large variability in their appearance due to the location in the colon. Especially, the second difficulty prevents us from using existing semi-supervised learning techniques, which are the common remedy for the first difficulty. In this paper, we propose a practical semi-supervised learning method for UC classification by newly exploiting two additional features, the location in a colon (e.g., left colon) and image capturing order, both of which are often attached to individual images in endoscopic image sequences. The proposed method can extract the essential information of UC classification efficiently by a disentanglement process with those features. Experimental results demonstrate that the proposed method outperforms several existing semi-supervised learning methods in the classification task, even with a small number of annotated images.","Abstract
Ulcerative colitis (UC) classification, which is an important task for endoscopic diagnosis, involves two main difficulties. First, endoscopic images with the annotation about UC (positive or negative) are usually limited. Second, they show a large variability in their appearance due to the location in the colon. Especially, the second difficulty prevents us from using existing semi-supervised learning techniques, which are the common remedy for the first difficulty. In this paper, we propose a practical semi-supervised learning method for UC classification by newly exploiting two additional features, the location in a colon (e.g., left colon) and image capturing order, both of which are often attached to individual images in endoscopic image sequences. The proposed method can extract the essential information of UC classification efficiently by a disentanglement process with those features. Experimental results demonstrate that the proposed method outperforms several existing semi-supervised learning methods in the classification task, even with a small number of annotated images.","Neural network,  semi supervised learning","Specificity, Accuracy, Precision, Recall, F1 score",No,Nothing is mentioned,No,"The dataset consists of 388 endoscopic image sequences, each of which contains a different number of images, comprising 10, 262 images in total. UC and location labels were attached to each image based on annotations by medical experts. Out of 10, 262 images, 6, 678 were labeled as UC (positive) and the remaining 3, 584 were normal (negative). There were three classes for the location label: right colon, left colon, and rectum. In the experiments, the dataset was randomly split into image sequence units, and 7, 183, 2, 052, and 1, 027 images were used as training, validation, and test set, respectively. To simulate the limitation of the UC-labeled images, the labeled image ratio R for the training set used by the semi-supervised learning methods was set to 0.1.","The dataset consists of 388 endoscopic image sequences, each of which contains a different number of images, comprising 10, 262 images in total. UC and location labels were attached to each image based on annotations by medical experts. Out of 10, 262 images, 6, 678 were labeled as UC (positive) and the remaining 3, 584 were normal (negative). There were three classes for the location label: right colon, left colon, and rectum. In the experiments, the dataset was randomly split into image sequence units, and 7, 183, 2, 052, and 1, 027 images were used as training, validation, and test set, respectively. To simulate the limitation of the UC-labeled images, the labeled image ratio R for the training set used by the semi-supervised learning methods was set to 0.1.",Private,"The dataset consists of 388 endoscopic image sequences, each of which contains a different number of images, comprising 10, 262 images in total. UC and location labels were attached to each image based on annotations by medical experts. Out of 10, 262 images, 6, 678 were labeled as UC (positive) and the remaining 3, 584 were normal (negative). There were three classes for the location label: right colon, left colon, and rectum. In the experiments, the dataset was randomly split into image sequence units, and 7, 183, 2, 052, and 1, 027 images were used as training, validation, and test set, respectively. To simulate the limitation of the UC-labeled images, the labeled image ratio R for the training set used by the semi-supervised learning methods was set to 0.1.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"
Kyushu University, Fukuoka City, Japan
(2)
National Institute of Informatics, Tokyo, Japan
(3)
Kyoto Second Red Cross Hospital, Kyoto, Japan

This work was supported by JSPS KAKENHI Grant Number JP20H04211 and AMED Grant Number JP20lk1010036h0002.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
27/10/2022 12.08.09,118,2021,Yes,It was accurately labelled,"Abstract
Chest X-ray (CXR) is the most typical diagnostic X-ray examination for screening various thoracic diseases. Automatically localizing lesions from CXR is promising for alleviating radiologists’ reading burden. However, CXR datasets are often with massive image-level annotations and scarce lesion-level annotations, and more often, without annotations. Thus far, unifying different supervision granularities to develop thoracic disease detection algorithms has not been comprehensively addressed. In this paper, we present OXnet, the first deep omni-supervised thoracic disease detection network to our best knowledge that uses as much available supervision as possible for CXR diagnosis. We first introduce supervised learning via a one-stage detection model. Then, we inject a global classification head to the detection model and propose dual attention alignment to guide the global gradient to the local detection branch, which enables learning lesion detection from image-level annotations. We also impose intra-class compactness and inter-class separability with global prototype alignment to further enhance the global information learning. Moreover, we leverage a soft focal loss to distill the soft pseudo-labels of unlabeled data generated by a teacher model. Extensive experiments on a large-scale chest X-ray dataset show the proposed OXnet outperforms competitive methods with significant margins. Further, we investigate omni-supervision under various annotation granularities and corroborate OXnet is a promising choice to mitigate the plight of annotation shortage for medical image diagnosis (Code is available at https://​github.​com/​LLYXC/​OXnet.).","Abstract
Chest X-ray (CXR) is the most typical diagnostic X-ray examination for screening various thoracic diseases. Automatically localizing lesions from CXR is promising for alleviating radiologists’ reading burden. However, CXR datasets are often with massive image-level annotations and scarce lesion-level annotations, and more often, without annotations. Thus far, unifying different supervision granularities to develop thoracic disease detection algorithms has not been comprehensively addressed. In this paper, we present OXnet, the first deep omni-supervised thoracic disease detection network to our best knowledge that uses as much available supervision as possible for CXR diagnosis. We first introduce supervised learning via a one-stage detection model. Then, we inject a global classification head to the detection model and propose dual attention alignment to guide the global gradient to the local detection branch, which enables learning lesion detection from image-level annotations. We also impose intra-class compactness and inter-class separability with global prototype alignment to further enhance the global information learning. Moreover, we leverage a soft focal loss to distill the soft pseudo-labels of unlabeled data generated by a teacher model. Extensive experiments on a large-scale chest X-ray dataset show the proposed OXnet outperforms competitive methods with significant margins. Further, we investigate omni-supervision under various annotation granularities and corroborate OXnet is a promising choice to mitigate the plight of annotation shortage for medical image diagnosis (Code is available at https://​github.​com/​LLYXC/​OXnet.).","Modern object detection algorithms often require a large amount of supervision signals. However, annotating abundant medical images for disease detection is infeasible due to the high dependence of expert knowledge and expense of human labor. Consequently, many medical datasets are weakly labeled or, more frequently, unlabeled [22]. This situation especially exists for chest X-rays (CXR), which is the most commonly performed diagnostic X-ray examination. Apart from massive unlabeled data, CXR datasets often have image-level annotations that can be easily obtained by text mining from numerous radiological reports [9, 27], while lesion-level annotations (e.g., bounding boxes) scarcely exist [7, 28]. Therefore, efficiently leveraging available annotations to develop thoracic disease detection algorithms has significant practical value.","Neural network, Weakly-Supervised Learning",Precision,No,Nothing is mentioned,No,"In total, 32,261 frontal CXR images taken from 27,253 patients were used. ","In total, 32,261 frontal CXR images taken from 27,253 patients were used. ",Private,"In total, 32,261 frontal CXR images taken from 27,253 patients were used. ",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China
(2)
Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China
(3)
Imsight AI Research Lab, Shenzhen, China
(4)
Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Beijing, China

This work was supported by Key-Area Research and Development Program of Guangdong Province, China (2020B010165004), Hong Kong Innovation and Technology Fund (Project No. ITS/311/18FP and Project No. ITS/426/17FP.), and National Natural Science Foundation of China with Project No. U1813204.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,(Code is available at https://​github.​com/​LLYXC/​OXnet.).,
27/10/2022 12.19.09,124,2021,Yes,It was accurately labelled,"Abstract
Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician’s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines.","Abstract
Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician’s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines.","A growing challenge in medical imaging is the need for more qualified experts to read an increasing volume of medical images, which has led to interpretation delays and reduced quality of healthcare [25]. Deep learning models in radiology [5], dermatology [6], and other areas [7] can increase physician throughput to alleviate this challenge. However, a major bottleneck to developing such models is the need for large labeled datasets [7].",Neural network,"AUC, Precision, F1 score",No,Nothing is mentioned,No,"Since medical imaging datasets with gaze data are not readily available, we collected gaze data by collaborating with radiologists. We consider three datasets: classifying chest X-rays for pneumothorax (CXR-P), classifying chest X-rays for a general abnormality (CXR-A), and classifying brain MRI slices for metastasis (METS) (all binary image classification). Positive and negative samples from each task can be found in Figure S.2.",No mention,Private,They collect it them selves,Yes,"There is a description of how they did it:

To collect gaze data, we built custom software to interface with a screen-based Tobii Pro Nano eye tracker. This state-of-the-art eye tracker is robust to head movements, corrective lenses, and lighting conditions. At the start of each session, each radiologist went through a 9-point calibration process. While in use, the program displays a single image to the user and collects gaze coordinates. Once the user has analyzed the image, they press a key to indicate the label given to the image. The program then saves the set of gaze coordinates that overlapped with the image and displays the next image.",No,Nothing is mentioned,Yes,For the purpose of this article,No,"
Department of Electrical Engineering, Stanford University, Stanford, USA
(2)
Institute for Computational and Mathematical Engineering, Stanford University, Stanford, USA
(3)
Department of Computer Science, Stanford University, Stanford, USA
(4)
Department of Radiology, Stanford University, Stanford, USA
(5)
Khoury College of Computer Sciences, Northeastern University, Boston, USA
(6)
Department of Biomedical Data Science, Stanford University, Stanford, USA",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87196-3_​56) contains supplementary material, which is available to authorized users.",
11/10/2022 08.54.16,291,2021,Yes,It was accurately labelled,Extensive evaluations on three different medical image classification tasks and three classifier backbones support that our method consistently improves the performance of the classifier which even has been trained by any re-balancing strategy. ,"Intelligent diagnosis is often biased toward common diseases due to data imbalance between common and rare diseases. Such bias may still exist even after applying re-balancing strategies during model training. To further alleviate the bias, we propose a novel method which works not in the training but in the inference phase.","Intelligent diagnosis is often biased toward common diseases due to data imbalance between common and rare diseases. Such bias may still exist even after applying re-balancing strategies during model training. To further alleviate the bias, we propose a novel method which works not in the training but in the inference phase. For any test input data, based on the difference between the temperature-tuned classifier output and a target probability distribution derived from the inverse frequency of different diseases, the input data can be slightly perturbed in a way similar to adversarial learning.",Neural network,Recall,No,Nothing is mentioned,Yes,"The proposed method was extensively evaluated on three imbalanced medical image datasets, Skin7 [5], OCTMNIST [27], and X-ray6 (Table 1). Specially, X-ray6 contains six diseases of X-ray images (Atelectasis, Cardiomegaly, Emphysema, Hernia, Mass, Effusion), where the six classes were selected from the original 14-class dataset ChestX-ray14 [24] by removing those classes of images which may contain multiple or ambiguous diseases in single images. ","3 different, different sizes","Private, Public","The proposed method was extensively evaluated on three imbalanced medical image datasets, Skin7 [5], OCTMNIST [27], and X-ray6 (Table 1). Specially, X-ray6 contains six diseases of X-ray images (Atelectasis, Cardiomegaly, Emphysema, Hernia, Mass, Effusion), where the six classes were selected from the original 14-class dataset ChestX-ray14 [24] by removing those classes of images which may contain multiple or ambiguous diseases in single images. ",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"For one of the three it was created for this article specifically: 
The proposed method was extensively evaluated on three imbalanced medical image datasets, Skin7 [5], OCTMNIST [27], and X-ray6 (Table 1). Specially, X-ray6 contains six diseases of X-ray images (Atelectasis, Cardiomegaly, Emphysema, Hernia, Mass, Effusion), where the six classes were selected from the original 14-class dataset ChestX-ray14 [24] by removing those classes of images which may contain multiple or ambiguous diseases in single images. ",Yes,"(1)
School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China
(2)
Key Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, China

This work is supported by the National Natural Science Foundation of China (No. 62071502, U1811461), the Guangdong Key Research and Development Program (No. 2020B1111190001, 2019B020228001), and the Meizhou Science and Technology Program (No. 2019A0102005).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
29/09/2022 09.53.59,267,2021,Yes,It was accurately labelled,"In this paper, we propose a selective attention regularization module (SAttenReg) to mimic the diagnosis process of pathologists. Specifically, to explicitly encourage the model to focus on clinically interpretable features (e.g., nuclei and fat droplets), SAttenReg learns the attention map with the regularization of clinically interpretable features. Furthermore, with the different contributions of histological features, the model can selectively focus on different histological features based on the distribution of nuclei in each instance. Experiments conducted on the in-house Liver-NAS and public Biopsy4Grading biopsy image datasets show that our method achieves superior classification performance with promising localization results.",we propose a selective attention regularization module (SAttenReg) to mimic the diagnosis process of pathologists.,"Nonalcoholic fatty liver disease (NAFLD) is the most common cause of liver disease worldwide [21, 22]. It is estimated that the prevalence of NAFLD is between 25% and 45%, which has become an important public health concern [4, 20]. ",Neural network,"Specificity, F1 score, Sensitivity",No,Nothing is mentioned,Yes,Biopsy4grading and Liver-NAS,"Biopsy4Grading [7] is a public liver section dataset collected from animals studies. Each liver tiles (299 $$\times $$ 299 pixels) were assigned to discrete pathologist-like sub-scores for quantifying NAS-related components of ballooning degeneration (0–2), lobular inflammation (0–3), steatosis (0–3) and fibrosis (0–4), corresponding to the Kleiner score system [11]. Liver-NAS is a private dataset of liver biopsy images collecting from 9 patients. Image tiles were generated from whole slide images ($$\sim 106259\times 306939$$ pixels) with an area of $$224\times 224$$ pixels, which can guarantee the pathologist to sufficiently identify the relevant histological features within the tile.","Private, Public","Both, one is public, one is private",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Department of Computer Science, Hong Kong Baptist University, kowloon, Hong Kong  This work was supported by the Health and Medical Research Fund Project under Grant 07180216. We acknowledge insightful discussion with Anthony W.H. CHAN. We also thank Vincent WS WONG, Grace LH WONG, and Howard H.W. LEUNG from the Chinese University of Hong Kong for help with data preparation.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
20/10/2022 14.38.10,322,2021,Yes,It was accurately labelled,"The WSI-aggregation module fuses all the region representations from different WSIs of one patient to predict patient-level survival risk. We conduct experiments on two WSI datasets that have accompanying survival data, i.e., NLST and TCGA-LUSC. The proposed method achieves state-of-the-art performances with concordance indices of 0.734 for NLST and 0.668 for TCGA-LUSC, outperforming existing approaches","In this paper, we propose a Hybrid Aggregation Network (HANet) to adaptively aggregate information from multiple WSIs of one patient for survival analysis.","Understanding of prognosis and mortality is crucial for evaluating the treatment plans for patients. Recent developments of digital pathology and deep learning bring the possibility of predicting survival time using histopathology whole slide images (WSIs). However, most prevalent methods usually rely on a small set of patches sampled from a WSI and are unable to directly learn from an entire WSI. We argue that a small patch set cannot fully represent patients’ survival risks due to the heterogeneity of tumors; moreover, multiple WSIs from one patient need to be evaluated together.

Survival analysis is becoming a popular field in healthcare research. The purpose of survival analysis is to examine how specified factors (e.g. smoking, age, treatment, etc.) affect the occurrence probability of a particular event (e.g. death, recurrence of the disease, etc.) at a certain time point. Clinicians can exploit survival analysis to evaluate the significance of prognostic variables and subsequently make an early decision among treatment options.

We present a novel Hybrid Aggregation Network (HANet)",Neural network,concordance index,No,Nothing is mentioned,Yes,"Two public survival analysis datasets which provide WSIs, National Lung Screening Trial - lung squamous cell carcinoma (NLST-LUSC) [16] and The Cancer Genome Atlas (TCGA) [11], are used in this study.","Two public survival analysis datasets which provide WSIs, National Lung Screening Trial - lung squamous cell carcinoma (NLST-LUSC) [16] and The Cancer Genome Atlas (TCGA) [11], are used in this study.

The NLST randomized 53,454 adults of age 55 to 74 with at least 30-year smoking history as high risk group for lung cancer screening",Public,"Two public survival analysis datasets which provide WSIs, National Lung Screening Trial - lung squamous cell carcinoma (NLST-LUSC) [16] and The Cancer Genome Atlas (TCGA) [11], are used in this study.
",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"aetherAI, Taipei, Taiwan
(2)
AstraZeneca, London, UK

We are grateful to Taiwan’s National Center for High-performance Computing for providing computing resources. We also thank Szu-Hua Chen, M.D. (aetherAI) for valuable advice.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​70) contains supplementary material, which is available to authorized users.",
20/10/2022 14.25.25,320,2021,Yes,It was accurately labelled,"we propose a novel semi-supervised approach named GKD based on the knowledge distillation. We train a teacher component that employs the label-propagation algorithm besides a deep neural network to benefit from the graph and non-graph modalities only in the training phase. The teacher component embeds all the available information into the soft pseudo-labels. The soft pseudo-labels are then used to train a deep student network for disease prediction of unseen test data for which the graph modality is unavailable. We perform our experiments on two public datasets for diagnosing Autism spectrum disorder, and Alzheimer’s disease, along with a thorough analysis on synthetic multi-modal datasets. According to these experiments, GKD outperforms the previous graph-based deep learning methods in terms of accuracy, AUC, and Macro F1","we propose a novel semi-supervised approach named GKD based on the knowledge distillation. We train a teacher component that employs the label-propagation algorithm besides a deep neural network to benefit from the graph and non-graph modalities only in the training phase. The teacher component embeds all the available information into the soft pseudo-labels. The soft pseudo-labels are then used to train a deep student network for disease prediction of unseen test data for which the graph modality is unavailable. We perform our experiments on two public datasets for diagnosing Autism spectrum disorder, and Alzheimer’s disease, along with a thorough analysis on synthetic multi-modal datasets. According to these experiments, GKD outperforms the previous graph-based deep learning methods in terms of accuracy, AUC, and Macro F1","The increased amount of multi-modal medical data has opened the opportunities to simultaneously process various modalities such as imaging and non-imaging data to gain a comprehensive insight into the disease prediction domain. Recent studies using Graph Convolutional Networks (GCNs) provide novel semi-supervised approaches for integrating heterogeneous modalities while investigating the patients’ associations for disease prediction. However, when the meta-data used for graph construction is not available at inference time (e.g., coming from a distinct population), the conventional methods exhibit poor performance.",Neural network,"AUC, Accuracy, F1 score",No,Nothing is mentioned,Yes,"The Autism Brain Imaging Data Exchange (ABIDE) [6, 8] database provides the neuroimaging (functional MRI) and phenotypic data of 1112 patients with binary labels indicating the presence of diagnosed Autism Spectrum Disorder (ASD).

The Alzheimer’s Disease Prediction Of Longitudinal Evolution (TADPOLE) [22] is a subset of Alzheimer’s Disease Neuroimaging Initiative (ADNI) data which is introduced in the TADPOLE challenge. The dataset has binary labels indicating if the patient’s Alzheimer’s status will be progressed in the next six months. The dataset contains 151 instances with diagnosis or progression in Alzheimer’s disease and 1878 who keep their status.","The Autism Brain Imaging Data Exchange (ABIDE) [6, 8] database provides the neuroimaging (functional MRI) and phenotypic data of 1112 patients with binary labels indicating the presence of diagnosed Autism Spectrum Disorder (ASD).

The Alzheimer’s Disease Prediction Of Longitudinal Evolution (TADPOLE) [22] is a subset of Alzheimer’s Disease Neuroimaging Initiative (ADNI) data which is introduced in the TADPOLE challenge. The dataset has binary labels indicating if the patient’s Alzheimer’s status will be progressed in the next six months. The dataset contains 151 instances with diagnosis or progression in Alzheimer’s disease and 1878 who keep their status.",Public,"The Autism Brain Imaging Data Exchange (ABIDE) [6, 8] database provides the neuroimaging (functional MRI) and phenotypic data of 1112 patients with binary labels indicating the presence of diagnosed Autism Spectrum Disorder (ASD).

The Alzheimer’s Disease Prediction Of Longitudinal Evolution (TADPOLE) [22] is a subset of Alzheimer’s Disease Neuroimaging Initiative (ADNI) data which is introduced in the TADPOLE challenge. The dataset has binary labels indicating if the patient’s Alzheimer’s status will be progressed in the next six months. The dataset contains 151 instances with diagnosis or progression in Alzheimer’s disease and 1878 who keep their status.",No,Nothing is mentioned,No,"None of them directly mention demographics as such...
For graph construction, first, we discard the phenotypic features that are only available for ASD patients including Autism Diagnostic Interview-Revised scores (ADIs), Autism Diagnostic Observation Schedule (ADOS) scores, Vineland Adaptive Behavior scores (VINELAND), Wechsler Intelligence Scale of Children (WISC), to prevent label leakage, since they are mostly available for ASD patients. The rest of the phenotypic features of patients are used for training a simple auto-encoder to both reconstruct the input via the decoder and classify the ASD state via a classifier using the latent low-dimensional representation of data (encoder’s output). (so for first dataset they do not expand on what they actually include)

These biomarkers contains cognitive tests, MRI measures, PET measures, Cerebrospinal fluid (CSF) measures, APOE and age risk factors. (For ADNI)",No,Nothing is mentioned,No,"Computer Aided Medical Procedures, Technical University of Munich, Munich, Germany
(2)
Sharif University of Technology, Tehran, Iran
(3)
Whiting School of Engineering, Johns Hopkins University, Baltimore, USA",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​68) contains supplementary material, which is available to authorized users.",
20/10/2022 14.10.12,318,2021,Yes,It was accurately labelled,"Prior work on diagnosing Alzheimer’s disease from magnetic resonance images of the brain established that convolutional neural networks (CNNs) can leverage the high-dimensional image information for classifying patients. However, little research focused on how these models can utilize the usually low-dimensional tabular information, such as patient demographics or laboratory measurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a general-purpose module for CNNs that dynamically rescales and shifts the feature maps of a convolutional layer, conditional on a patient’s tabular clinical information. We show that DAFT is highly effective in combining 3D image and tabular information for diagnosis and time-to-dementia prediction, where it outperforms competing CNNs with a mean balanced accuracy of 0.622 and mean c-index of 0.748, respectively. ","Prior work on diagnosing Alzheimer’s disease from magnetic resonance images of the brain established that convolutional neural networks (CNNs) can leverage the high-dimensional image information for classifying patients. However, little research focused on how these models can utilize the usually low-dimensional tabular information, such as patient demographics or laboratory measurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a general-purpose module for CNNs that dynamically rescales and shifts the feature maps of a convolutional layer, conditional on a patient’s tabular clinical information. We show that DAFT is highly effective in combining 3D image and tabular information for diagnosis and time-to-dementia prediction, where it outperforms competing CNNs with a mean balanced accuracy of 0.622 and mean c-index of 0.748, respectively. ","In recent years, deep convolutional neural networks (CNNs) have become the standard for classification of Alzheimer’s disease (AD) from magnetic resonance images (MRI) of the brain (see e.g. [4, 29] for an overview). CNNs excel at extracting high-level information about the neuroanatomy from MRI. However, brain MRI only offers a partial view on the underlying changes causing cognitive decline. Therefore, clinicians and researchers often rely on tabular data such as patient demographics, family history, or laboratory measurements from cerebrospinal fluid for diagnosis. In contrast to image information, tabular data is typically low-dimensional and individual variables capture rich clinical knowledge.",Neural network,"Accuracy, concordance index",Yes,We first segment scans with FreeSurfer,Yes,T1 brain MRI from the Alzheimer’s Disease Neuroimaging Initiative (ADNI),,Public,Table 1 summarizes the datasets.,No,Nothing is mentioned,Yes,"Tabular data comprises 9 variables: age, gender, education, ApoE4, cerebrospinal fluid biomarkers A$$\beta _{42}$$, P-tau181 and T-tau, and two summary measures derived from 18F-fluorodeoxyglucose and florbetapir PET scans. ",No,Nothing is mentioned,Yes,"Artificial Intelligence in Medical Imaging (AI-Med), Department of Child and Adolescent Psychiatry, Ludwig-Maximilians-Universität, Munich, Germany

This research was supported by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation, and the Federal Ministry of Education and Research in the call for Computational Life Sciences (DeepMentia, 031L0200A).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Our implementation is available at https://​github.​com/​ai-med/​DAFT.

Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​66) contains supplementary material, which is available to authorized users.",
20/10/2022 13.48.08,313,2021,Yes,It was accurately labelled,"Specifically, we define a new measure, termed radiomics score, to capture the difficulty of classifying a set of samples.",". In this work, we propose a novel deep curriculum learning method that utilizes radiomics information as a source of additional knowledge to guide training using customized curriculums. Specifically, we define a new measure, termed radiomics score, to capture the difficulty of classifying a set of samples. We use the radiomics score to enable a newly designed curriculum-based training scheme. In this scheme, the loss function component is weighted and initialized by the corresponding radiomics score of each sample, and furthermore, the weights are continuously updated in the course of training based on our customized curriculums to enable curriculum learning. We implement and evaluate our methods on a typical computer-aided diagnosis of breast cancer.","The traditional way of training Convolutional Neural Networks (CNNs) makes use of images as the sole source of data. In medical imaging applications, additional information or clinical knowledge are often available along with the data, such as pre-assessment made by clinicians, auxiliary tasks in relation to a target task, qualitative clinical experience, etc. These sources of information can be useful for a target task but are mostly ignored in the current practice of data-driven deep learning modeling.",Neural network,AUC,Yes,"The target task is to perform a computer-aided diagnosis of malignant cases from benign cases. The segmentations acquired by radiologists are available for all lesions, and we use these segmentations to extract radiomics features. ",Yes,"We use CBIS-DDSM [15], a publicly available dataset of digitized film mammogram images for experiments. This dataset contains 1,462 images of mass lesions that are either benign (N = 722) or malignant (N = 740).","We use CBIS-DDSM [15], a publicly available dataset of digitized film mammogram images for experiments. This dataset contains 1,462 images of mass lesions that are either benign (N = 722) or malignant (N = 740).",Public,"We use CBIS-DDSM [15], a publicly available dataset of digitized film mammogram images for experiments. This dataset contains 1,462 images of mass lesions that are either benign (N = 722) or malignant (N = 740).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Intelligent Systems Program, School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA
(2)
Department of Radiology, University of Pittsburgh School of Medicine, Pittsburgh, PA, USA
(3)
Magee-Womens Hospital, University of Pittsburgh Medical Center, Pittsburgh, PA, USA
(4)
Department of Electrical and Computer Engineering, Swanson School of Engineering, University of Pittsburgh, Pittsburgh, PA, USA
(5)
Department of Biomedical Informatics and Department of Bioengineering, University of Pittsburgh, Pittsburgh, PA, USA

This work was supported by National Institutes of Health grants (1R01CA193603, 3R01CA193603-03S1, and 1R01CA218405), the UPMC Hillman Cancer Center Developmental Pilot Program, and an Amazon Machine Learning Research Award. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation (NSF) grant number ACI-1548562. Specifically, it used the Bridges-2 system, which is supported by NSF award number ACI-1928147, at the Pittsburgh Supercomputing Center (PSC).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
20/10/2022 09.34.34,312,2021,Yes,It was accurately labelled,"The scarcity of labeled data is a major bottleneck for developing accurate and robust deep learning-based models for histopathology applications. The problem is notably prominent for the task of metastasis detection in lymph nodes, due to the tissue’s low tumor-to-non-tumor ratio, resulting in labor- and time-intensive annotation processes for the pathologists. This work explores alternatives on how to augment the training data for colon carcinoma metastasis detection when there is limited or no representation of the target domain. Through an exhaustive study of cross-validated experiments with limited training data availability, we evaluate both an inter-organ approach utilizing already available data for other tissues, and an intra-organ approach, utilizing the primary tumor. Both these approaches result in little to no extra annotation effort. Our results show that these data augmentation strategies can be an efficient way of increasing accuracy on metastasis detection, but fore-most increase robustness.","This work explores alternatives on how to augment the training data for colon carcinoma metastasis detection when there is limited or no representation of the target domain. Through an exhaustive study of cross-validated experiments with limited training data availability, we evaluate both an inter-organ approach utilizing already available data for other tissues, and an intra-organ approach, utilizing the primary tumor. Both these approaches result in little to no extra annotation effort. Our results show that these data augmentation strategies can be an efficient way of increasing accuracy on metastasis detection, but fore-most increase robustness.","The scarcity of labeled data is a major bottleneck for developing accurate and robust deep learning-based models for histopathology applications.

Colon cancer is the third most common cancer type in the world, where the majority of the cases are classified as adenocarcinoma",Neural network,AUC,No,Nothing is mentioned,No,Nothing is mentioned,"we propose to leverage the readiness of the primary colon cancer tumor, as well as already existing carcinoma datasets for different organs tissue (breast and skin)

In the conducted experiments, the target colon adenocarcinoma dataset consists of data from 37 anonymized patients, where data from 5 patients were used as the test set, and the rest used for training and validation
",Public,All three datasets are publicly available for use in legal and ethical medical diagnostics research.,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Department of Science and Technology, Linkoping University, Linköping, Sweden
(2)
Center for Medical Image Science and Visualization, Linkoping University, Linköping, Sweden
(3)
Sectra AB, Linköping, Sweden

We would like to thank Martin Lindvall for the interesting discussions and insights into the use of cancer type-specific primary tumor data for lymph node metastasis detection, and Panagiotis Tsirikoglou for the suggestions in results analysis. This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, the strategic research environment ELLIIT, and the VINNOVA grant 2017-02447 for the Analytic Imaging Diagnostics Arena (AIDA).",Yes,"All three datasets are publicly available for use in legal and ethical medical diagnostics research.

 In the conducted experiments, the target colon adenocarcinoma dataset consists of data from 37 anonymized patients, ",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​60) contains supplementary material, which is available to authorized users",
20/10/2022 09.28.11,311,2021,Yes,It was accurately labelled,Linear Prediction Residual for Efficient Diagnosis of Parkinson’s Disease from Gait,Linear Prediction Residual for Efficient Diagnosis of Parkinson’s Disease from Gait,"Parkinson’s Disease (PD) is a chronic and progressive neurological disorder that results in rigidity, tremors and postural instability. There is no definite medical test to diagnose PD and diagnosis is mostly a clinical exercise. Although guidelines exist, about 10–30% of the patients are wrongly diagnosed with PD. Hence, there is a need for an accurate, unbiased and fast method for diagnosis. In this study, we propose LPGNet, a fast and accurate method to diagnose PD from gait. LPGNet uses Linear Prediction Residuals (LPR) to extract discriminating patterns from gait recordings and then uses a 1D convolution neural network with depth-wise separable convolutions to perform diagnosis. ",Neural network,"AUC, Accuracy, F1 score",No,Nothing is mentioned,No,"We use a publicly available dataset [8] that is a collection of data collected from three different studies [6, 9, 22]. The dataset consists of 306 gait recordings from 93 patients with PD and 73 healthy control subjects.","We use a publicly available dataset [8] that is a collection of data collected from three different studies [6, 9, 22]. The dataset consists of 306 gait recordings from 93 patients with PD and 73 healthy control subjects.",Public,"We use a publicly available dataset [8] that is a collection of data collected from three different studies [6, 9, 22]. The dataset consists of 306 gait recordings from 93 patients with PD and 73 healthy control subjects.",No,Nothing is mentioned,No,Nothing is mentioned,Yes,Nothing is mentioned,Yes,"Center for Computational Natural Sciences and Bioinformatics, IIIT Hyderabad, Hyderabad, India

This study was supported by funding from IHub-Data and IIIT Hyderabad. We would also like to thank Dr. K Sudarsana Reddy for the discussions we had regarding the theoretical correctness of the method presented.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​59) contains supplementary material, which is available to authorized users.",
20/10/2022 09.24.09,310,2021,Yes,It was accurately labelled,"Content-based image retrieval (CBIR) is of increasing interest for clinical applications spanning differential diagnosis, prognostication, and indexing of electronic radiology databases. However, meaningful CBIR for radiology applications requires capabilities to address the semantic gap and assess similarity based on fine-grained image features. We observe that images in radiology databases are often accompanied by free-text radiologist reports containing rich semantic information. Therefore, we propose a Multimodal Multitask Deep Learning (MMDL) approach for CBIR on radiology images. Our proposed approach employs multimodal database inputs for training, learns semantic feature representations for each modality, and maps these representations into a common subspace. During testing, we use representations from the common subspace to rank similarities between the query and database. To enhance our framework for fine-grained image retrieval, we provide extensions employing deep descriptors and ranking loss optimization. We performed extensive evaluations on the MIMIC Chest X-ray (MIMIC-CXR) dataset with images and reports from 227,835 studies. Our results demonstrate performance gains over a typical unimodal CBIR strategy. Further, we show that the performance gains of our approach are robust even in scenarios where only a subset of database images are paired with free-text radiologist reports. Our work has implications for next-generation medical image indexing and retrieval systems.",we propose a Multimodal Multitask Deep Learning (MMDL) approach for CBIR on radiology images.,"Content-based image retrieval (CBIR) is of increasing interest for clinical applications spanning differential diagnosis, prognostication, and indexing of electronic radiology databases. However, meaningful CBIR for radiology applications requires capabilities to address the semantic gap and assess similarity based on fine-grained image features. We observe that images in radiology databases are often accompanied by free-text radiologist reports containing rich semantic information. ",Neural network,"Accuracy, Precision",No,Nothing is mentioned,Yes,"We evaluated our multimodal retrieval framework on the MIMIC Chest X-ray (MIMIC-CXR) Database v2.0.0. This dataset contains 377,110 images and paired reports corresponding to 64,579 patients with their annotations categorized into 14 classes","We evaluated our multimodal retrieval framework on the MIMIC Chest X-ray (MIMIC-CXR) Database v2.0.0. This dataset contains 377,110 images and paired reports corresponding to 64,579 patients with their annotations categorized into 14 classes",Public,Can be found upon searching for it,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Institute for Infocomm Research, A*STAR, Singapore, Singapore
(2)
College of Computer Science, Sichuan University, Chengdu, Sichuan, China

Research efforts were supported by funding and infrastructure for deep learning and medical imaging research from the Institute for Infocomm Research, Science and Engineering Research Council, A*STAR, Singapore. We thank Victor Getty, Vijay Chandrasekhar and Ivan Ho Mien from the Institute for Infocomm Research, A*STAR for their valuable inputs. We also acknowledge insightful discussions with Jayashree Kalpathy-Cramer at the Massachusetts General Hospital, Boston, USA",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​58) contains supplementary material, which is available to authorized users.",
20/10/2022 09.03.26,306,2021,Yes,It was accurately labelled,We define our task as the image classification.,"The performance of anatomy site recognition is critical for computer-aided diagnosis systems such as the quality evaluation of endoscopic examinations and the automatic generating of electronic medical records. To achieve an accurate recognition model, it requires extensive training samples and precise annotations from human experts, especially for deep learning based methods. However, due to the similar appearance of gastrointestinal (GI) anatomy sites, it is hard to annotate accurately and is expensive to acquire such high quality dataset at a large scale. Therefore, to balance the cost-performance trade-offs, in this work we propose an effective annotation refinement approach which leverages a small amount of trust data that is accurately labelled by experts to further improve the training performance on a large amount of noisy label data. ","The performance of anatomy site recognition is critical for computer-aided diagnosis systems such as the quality evaluation of endoscopic examinations and the automatic generating of electronic medical records. To achieve an accurate recognition model, it requires extensive training samples and precise annotations from human experts, especially for deep learning based methods. However, due to the similar appearance of gastrointestinal (GI) anatomy sites, it is hard to annotate accurately and is expensive to acquire such high quality dataset at a large scale. Therefore, to balance the cost-performance trade-offs, in this work we propose an effective annotation refinement approach which leverages a small amount of trust data that is accurately labelled by experts to further improve the training performance on a large amount of noisy label data. ",Neural network,Accuracy,No,Nothing is mentioned,Yes,"The experiments are conducted using CIFAR-10 with synthetic noise labels. The original dataset consists of 50,000 training samples and 10,000 testing samples with clean labels of 10 classes.","The experiments are conducted using CIFAR-10 with synthetic noise labels. The original dataset consists of 50,000 training samples and 10,000 testing samples with clean labels of 10 classes.",Public,"Googling the name, can find it",No,Nothing is mentioned,No,"Nothing is mentioned, not relevant, synthetic data",No,Nothing is mentioned,No,"(1)
NEC Laboratories, Beijing, China",No,"Nothing is mentioned, not relevant, synthetic data",No,Nothing is mentioned,No,"Nothing is mentioned, not relevant, synthetic data",No,Nothing is mentioned,
20/10/2022 08.52.58,302,2021,Yes,It was accurately labelled,"To this end, we propose in this work a novel hybrid approach to rare disease classification, featuring two key novelties targeted at the above drawbacks.","To this end, we propose in this work a novel hybrid approach to rare disease classification, featuring two key novelties targeted at the above drawbacks.","Rare diseases are characterized by low prevalence and are often chronically debilitating or life-threatening. Imaging-based classification of rare diseases is challenging due to the severe shortage in training examples. Few-shot learning (FSL) methods tackle this challenge by extracting generalizable prior knowledge from a large base dataset of common diseases and normal controls, and transferring the knowledge to rare diseases. Yet, most existing methods require the base dataset to be labeled and do not make full use of the precious examples of the rare diseases.

Rare diseases are a significant public health issue and a challenge to healthcare. On aggregate, the number of people suffering from rare diseases worldwide is estimated over 400 million, and there are about 5000–7000 rare diseases—with 250 new ones appearing each year [27]. Patients with rare diseases face delayed diagnosis: 10% of patients spent 5–30 years to reach a final diagnosis. Besides, many rare diseases can be misdiagnosed. ","Unsupervised learning, Neural network","Accuracy, F1 score",No,Nothing is mentioned,Yes,"The ISIC 2018 skin lesion classification dataset [4, 30]1 includes 10,015 dermoscopic images from seven disease categories: melanocytic nevus (6,705), benign keratosis (1,099), melanoma (1,113), basal cell carcinoma (514), actinic keratosis (327), dermatofibroma (115), and vascular lesion (142)"," The ISIC 2018 skin lesion classification dataset [4, 30]1 includes 10,015 dermoscopic images from seven disease categories: melanocytic nevus (6,705), benign keratosis (1,099), melanoma (1,113), basal cell carcinoma (514), actinic keratosis (327), dermatofibroma (115), and vascular lesion (142). From that, we simulate the task of rare disease classification as below. Following Li et al. [18], we use the four classes with the most cases as the CDNC dataset $$D_\mathrm {base}$$, and the other three as the rare disease dataset $$D_\mathrm {rare}$$. K images are sampled for each class in $$D_\mathrm {rare}$$ to compose the support set S for a 3-way K-shot task.",Public,"There is a link to a challenge, that is now finished I believe, but presumably the data is still available to researchers?",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Xiamen University, Xiamen, China
(2)
Tencent Jarvis Lab, Shenzhen, China

This work was supported by the Fundamental Research Funds for the Central Universities (Grant No. 20720190012), Key-Area Research and Development Program of Guangdong Province, China (No. 2018B010111001), and Scientific  and Technical Innovation 2030 - “New Generation Artificial Intelligence” Project (No. 2020AAA0104100).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,The source code is available at: https://​github.​com/​SunjHan/​Hybrid-Representation-Learning-Approach-for-Rare-Disease-Classification.,
11/10/2022 09.15.48,300,2021,Yes,It was accurately labelled,Meta-modulation Network for Domain Generalization in Multi-site fMRI Classification,we propose a novel framework that adaptively calibrates the site-specific features into site-invariant features via a novel modulation mechanism. ,"In general, it is expected that large amounts of functional magnetic resonance imaging (fMRI) would be helpful to deduce statistically meaningful biomarkers or to build generalized predictive models for brain disease diagnosis. However, the site-variation inherent in rs-fMRI hampers the researchers to use the entire samples collected from multiple sites because it involves the unfavorable heterogeneity in data distribution, thus negatively impact on identifying biomarkers and making a diagnostic decision. To alleviate this challenging multi-site problem, we propose a novel framework that adaptively calibrates the site-specific features into site-invariant features via a novel modulation mechanism.",Neural network,"AUC, Specificity, Accuracy, Sensitivity",No,Nothing is mentioned,Yes,We validated our proposed network over the public Autism Brain Imaging Data Exchange (ABIDE) dataset comprised of data from multiple sites.,"We used 1, 032 samples from a total of 16 sites3, in which 496 subjects are with autism spectrum disorders (ASD) and 536 subjects are with typical developments (TD).",Public,We validated our proposed network over the public Autism Brain Imaging Data Exchange (ABIDE) dataset comprised of data from multiple sites.,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Department of Brain and Cognitive Engineering, Korea University, Seoul, Republic of Korea
(2)
Department of Artificial Intelligence, Korea University, Seoul, Republic of Korea

This work was supported by National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2019R1A2C1006543) and partially by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00079, Artificial Intelligence Graduate School Program (Korea University)).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
11/10/2022 09.12.06,298,2021,Yes,It was accurately labelled,"In this paper, we propose the collaborative diagnosis-synthesis framework (CDSF) for joint missing neuroimage imputation and multi-modal diagnosis of neurodegenerative disorders. ","In this paper, we propose the collaborative diagnosis-synthesis framework (CDSF) for joint missing neuroimage imputation and multi-modal diagnosis of neurodegenerative disorders. ","The missing data issue is a common problem in multi-modal neuroimage (e.g., MRI and PET) based diagnosis of neurodegenerative disorders.",Neural network,"AUC, Specificity, Accuracy, F1 score, Sensitivity, Matthews correlation coefficient",No,Nothing is mentioned,Yes,"Two subsets of the Alzheimer’s Disease Neuroimaging Initiative (ADNI) studies [13], including ADNI-1 phase and ADNI-2 phase, were used for this study. All collected subjects in ADNI-1/-2 have baseline MRI data, while only part of them have PET images. We follow the same steps in [2, 10] to process the collected data. The subjects in ADNI-1/2 were divided into three categories: AD, cognitively normal (CN), and MCI. MCI could be further divided into progressive MCI (pMCI) and static MCI (sMCI) that would or would not progress to AD within 36 months after the baseline. Totally, we also have 205 AD, 231 CN, 165 pMCI, and 147 sMCI subjects in ADNI-1, and 162 AD, 209 CN, 89 pMCI, and 256 sMCI subjects in ADNI-2.","Two subsets of the Alzheimer’s Disease Neuroimaging Initiative (ADNI) studies [13], including ADNI-1 phase and ADNI-2 phase, were used for this study. All collected subjects in ADNI-1/-2 have baseline MRI data, while only part of them have PET images. We follow the same steps in [2, 10] to process the collected data. The subjects in ADNI-1/2 were divided into three categories: AD, cognitively normal (CN), and MCI. MCI could be further divided into progressive MCI (pMCI) and static MCI (sMCI) that would or would not progress to AD within 36 months after the baseline. Totally, we also have 205 AD, 231 CN, 165 pMCI, and 147 sMCI subjects in ADNI-1, and 162 AD, 209 CN, 89 pMCI, and 256 sMCI subjects in ADNI-2.",Public,ADNI,,nothing is mentioned,No,nothing is mentioned,No,nothing is mentioned,Yes,"(1)
National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, Shaanxi, China
(2)
Research and Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen, 518057, China
(3)
School of Biomedical and Engineering, ShanghaiTech University, Shanghai, 201210, China

This work was supported in part by the National Natural Science Foundation of China under Grants 61771397, in part by the CAAI-Huawei MindSpore Open Fund under Grants CAAIXSJLJJ-2020-005B, and in part by the China Postdoctoral Science Foundation under Grants BX2021333.",No,nothing is mentioned,No,nothing is mentioned,No,nothing is mentioned,No,nothing is mentioned,
07/10/2022 13.16.05,290,2021,Yes,It was accurately labelled,"Current deep learning models are characterised by catastrophic forgetting of old knowledge when learning new classes. This poses a challenge in intelligent diagnosis systems where initially only training data of a limited number of diseases are available. In this case, updating the intelligent system with data of new diseases would inevitably downgrade its performance on previously learned diseases. Inspired by the process of learning new knowledge in human brains, we propose a Bayesian generative model for continual learning built on a fixed pre-trained feature extractor. In this model, knowledge of each old class can be compactly represented by a collection of statistical distributions, e.g. with Gaussian mixture models, and naturally kept from forgetting in continual learning. Experiments on two skin image sets showed that the proposed approach outperforms state-of-the-art approaches which even keep some images of old classes during continual learning of new classes.","we propose a Bayesian generative model for continual learning built on a fixed pre-trained feature extractor. In this model, knowledge of each old class can be compactly represented by a collection of statistical distributions, e.g. with Gaussian mixture models, and naturally kept from forgetting in continual learning.","Current deep learning models are characterised by catastrophic forgetting of old knowledge when learning new classes. This poses a challenge in intelligent diagnosis systems where initially only training data of a limited number of diseases are available.  In this case, updating the intelligent system with data of new diseases would inevitably downgrade its performance on previously learned diseases. Inspired by the process of learning new knowledge in human brains,",Neural network,Recall,No,Nothing is mentioned,Yes,"The proposed approach was extensively evaluated on two medical skin image datasets. Skin7 [5] is a skin lesion dataset from the challenge of dermoscopic image classification held by the International Skin Imaging Collaboration (ISIC) in 2018. It consists of 7 disease categories, and each image is of size $$600\times 450$$ pixels. This dataset presents severe class imbalance, with the largest class 60 times larger than the smallest one. Skin40 is a subset of 193 classes of skin disease images collected from the internet [18]. Skin40 contains two types of images, dermoscopic images which have relatively consistent imaging conditions (e.g., similar illumination) and therefore low levels of imaging noise, and clinical images captured mostly with digital cameras or mobile phones. ","The proposed approach was extensively evaluated on two medical skin image datasets. Skin7 [5] is a skin lesion dataset from the challenge of dermoscopic image classification held by the International Skin Imaging Collaboration (ISIC) in 2018. It consists of 7 disease categories, and each image is of size $$600\times 450$$ pixels. This dataset presents severe class imbalance, with the largest class 60 times larger than the smallest one. Skin40 is a subset of 193 classes of skin disease images collected from the internet [18]. Skin40 contains two types of images, dermoscopic images which have relatively consistent imaging conditions (e.g., similar illumination) and therefore low levels of imaging noise, and clinical images captured mostly with digital cameras or mobile phones. ",Public,"The proposed approach was extensively evaluated on two medical skin image datasets. Skin7 [5] is a skin lesion dataset from the challenge of dermoscopic image classification held by the International Skin Imaging Collaboration (ISIC) in 2018. It consists of 7 disease categories, and each image is of size $$600\times 450$$ pixels. This dataset presents severe class imbalance, with the largest class 60 times larger than the smallest one. Skin40 is a subset of 193 classes of skin disease images collected from the internet [18]. Skin40 contains two types of images, dermoscopic images which have relatively consistent imaging conditions (e.g., similar illumination) and therefore low levels of imaging noise, and clinical images captured mostly with digital cameras or mobile phones. ",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China
(2)
Key Laboratory of Machine Intelligence and Advanced Computing, MOE, China
(3)
Pazhou Lab, Guangzhou, China

This work is supported in part by the National Natural Science Foundation of China (grant No. 62071502, U1811461), the Guangdong Key Research and Development Program (grant No. 2020B1111190001, 2019B020228001), and the Meizhou Science and Technology Program (grant No. 2019A0102005).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
07/10/2022 13.09.24,288,2021,Yes,Other medical imaging task,The spine centerline extraction is formulated as a row-wise classification task,we propose a deep learning-based approach to simultaneously estimate spine centerline and spinal curvature with shared convolutional backbone,Spinal curvature estimation plays an important role in adolescent idiopathic scoliosis (AIS) evaluation and treatment. ,Graph analysis,symmetric mean absolute percentage ,No,If I understand correctly it doesn't use segmentation...,Yes,"The public dataset of 609 spinal anterior-posterior X-ray images, which is used as the training set in the Accurate Automated Spinal Curvature Estimation (AASCE) challenge 2019, is used for the evaluation of our method.","The public dataset of 609 spinal anterior-posterior X-ray images, which is used as the training set in the Accurate Automated Spinal Curvature Estimation (AASCE) challenge 2019, is used for the evaluation of our method.",Public,"The public dataset of 609 spinal anterior-posterior X-ray images, which is used as the training set in the Accurate Automated Spinal Curvature Estimation (AASCE) challenge 2019, is used for the evaluation of our method.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Institute of Intelligent Machines, HFIPS, Chinese Academy of Sciences, Hefei, China
(2)
University of Science and Technology of China, Hefei, China
(3)
School of Information Engineering, Zhengzhou University, Zhengzhou, China
(4)
Nullmax, Shanghai, China
(5)
The First Affiliated Hospital of USTC, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China

This work is supported in part by the grant of NSFC (61804100, 61973294, 61806181), KRDP of Anhui Province (201904a05020086) and CAS (GJTD-2018-15).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
07/10/2022 13.04.36,287,2021,Yes,It was accurately labelled,"Clinical finding summaries from an orthopantomogram, or a dental panoramic radiograph, have significant potential to improve patient communication and speed up clinical judgments. While orthopantomogram is a first-line tool for dental examinations, no existing work has explored the summarization of findings from it. A finding summary has to find teeth in the imaging study and label the teeth with several types of past treatments. To tackle the problem, we develop DeepOPG that breaks the summarization process into functional segmentation and tooth localization, the latter of which is further refined by a novel dental coherence module. We also leverage weak supervision labels to improve detection results in a reinforcement learning scenario.","we develop DeepOPG that breaks the summarization process into functional segmentation and tooth localization, the latter of which is further refined by a novel dental coherence module. We also leverage weak supervision labels to improve detection results in a reinforcement learning scenario.","Clinical finding summaries from an orthopantomogram, or a dental panoramic radiograph, have significant potential to improve patient communication and speed up clinical judgments. While orthopantomogram is a first-line tool for dental examinations, no existing work has explored the summarization of findings from it. ",Neural network,AUC,Yes,we develop DeepOPG that breaks the summarization process into functional segmentation,Yes,"we use the UFBA-UESC Dental Images Deep dataset [9] where there are 1,500 OPG images in total","we use the UFBA-UESC Dental Images Deep dataset [9] where there are 1,500 OPG images in total",Public,Available upon request,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,"(1)
MIT CSAIL, Cambridge, USA
(2)
Chung Shan Medical University, Taichung, Taiwan",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,"The dataset and code are made available online.

Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​35) contains supplementary material, which is available to authorized users.",
07/10/2022 13.00.00,286,2021,Yes,It was accurately labelled,Based on the performance measures this lesion matching is a form of classification,"A holistic understanding of dual-view transformation (DVT) is an enabling technique for computer-aided diagnosis (CAD) of breast lesion in mammogram, e.g., micro-calcification ($$\mu $$C) or mass matching, dual-view feature extraction etc. Learning a complete DVT usually relies on a dense supervision which indicates a corresponding tissue in one view for each tissue in another. Since such dense supervision is infeasible to obtain in practical, a sparse supervision of some traceable lesion tissues across two views is thus an alternative but will lead to a defective DVT, limiting the performance of existing CAD systems dramatically. To address this problem, our solution is simple but very effective, i.e., densifying the existing sparse supervision by synthesizing lesions across two views","Since such dense supervision is infeasible to obtain in practical, a sparse supervision of some traceable lesion tissues across two views is thus an alternative but will lead to a defective DVT, limiting the performance of existing CAD systems dramatically. ",Neural network,"AUC, Balanced accuracy",No,Nothing is mentioned,Yes,"we conduct several experiments of cross-view $$\mu $$C matching and evaluate on two public mammography dataset, i.e., the INbreast (115 patients and 410 images in total) and the CBIS-DDSM (3103 images in total). ","we conduct several experiments of cross-view $$\mu $$C matching and evaluate on two public mammography dataset, i.e., the INbreast (115 patients and 410 images in total) and the CBIS-DDSM (3103 images in total). ",Public,"we conduct several experiments of cross-view $$\mu $$C matching and evaluate on two public mammography dataset, i.e., the INbreast (115 patients and 410 images in total) and the CBIS-DDSM (3103 images in total). ",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
School of Electronic Information and Communication, Huazhong University of Science and Technology, Wuhan, China
(2)
Britton Chance Center for Biomedical Photonics, Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China
(3)
MoE Key Laboratory for Biomedical Photonics, Collaborative Innovation Center for Biomedical Engineering, School of Engineering Sciences, Huazhong University of Science and Technology, Wuhan, China
(4)
Hong Kong University of Science and Technology, Hong Kong, China
(5)
Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China

This work was supported by the National Natural Science Foundation of China (6187241762061160490), the project of Wuhan Science and Technology Bureau (2020010601012167), the Open Project of Wuhan National Laboratory for Optoelectronics (2018WNLOKF025), the Fundamental Research Funds for the Central Universities (2021XXJS033).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
04/10/2022 11.27.12,283,2021,Yes,It was accurately labelled,Highly imbalanced datasets are ubiquitous in medical image classification problems.,"we propose a novel mechanism for sampling training data based on the popular MixUp regularization technique, which we refer to as Balanced-MixUp. ","Highly imbalanced datasets are ubiquitous in medical image classification problems. In such problems, it is often the case that rare classes associated to less prevalent diseases are severely under-represented in labeled databases, typically resulting in poor performance of machine learning algorithms due to overfitting in the learning process.",Neural network,"Matthews Correlation Coefficient,  quadratic-weighted kappa score",No,Nothing is mentioned,Yes,Multiple datasets are used,Different sizes,Public,"Eyepacs database1 the largest publicly available dataset, we use the Hyper-Kvasir dataset4, recently made available [2]",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Bournemouth University, Poole, UK
(2)
University of Adelaide, Adelaide, Australia
(3)
BCN Medtech, Department of Information and Communication Technologies, Universitat Pompeu Fabra, Barcelona, Spain
(4)
Catalan Institution for Research and Advanced Studies (ICREA), Barcelona, Spain    This work was partially supported by a Marie Skłodowska-Curie Global Fellowship (No. 892297) and by Australian Research Council grants (DP180103232 and FT190100525).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,Code is released at https://​github.​com/​agaldran/​balanced_​mixup,
04/10/2022 11.03.57,276,2021,Yes,It was accurately labelled,this paper tackles the problem of learning a model from the source data for which can directly generalize to an unseen target domain for SCD prediction.,this paper tackles the problem of learning a model from the source data for which can directly generalize to an unseen target domain for SCD prediction.,"It is highly desired to predict the progress of SCD for possible intervention of AD-related cognitive decline. Many neuroimaging-based methods have been developed for AD diagnosis, but there are few studies devoted to automated progress prediction of SCD due to the limited number of SCD subjects. ",Neural network,"AUC, Specificity, Accuracy, Sensitivity, balanced accuracy",No,"Following [17], all brain MRIs go through a standard pre-processing pipeline, including (i) skull stripping, (ii) intensity correction, (iii) re-sampling to the same resolution of $$1\times 1\times 1$$ mm$$^{3}$$ and (iv) spatial normalization to the Automated Anatomical Labeling (AAL) template. We employ the SPM software package2 as the main tool to facilitate the MR image pre-processing.",Yes," A total of 1, 393 T1-weighted structural MRIs from the publicly available Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset1 are used in this work to train a prediction model."," A total of 1, 393 T1-weighted structural MRIs from the publicly available Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset1 are used in this work to train a prediction model.",Public," A total of 1, 393 T1-weighted structural MRIs from the publicly available Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset1 are used in this work to train a prediction model.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, USA
(2)
School of Biomedical Engineering, Southern Medical University, Guangzhou, 510515, China
(3)
Department of Geriatric Psychiatry, Shanghai Mental Health Center, Shanghai Jiao Tong University School of Medicine, Shanghai, 200030, China    H. Guan and M. Liu were partly supported by NIH grant (No. AG041721).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
03/10/2022 09.13.12,271,2021,Yes,It was accurately labelled," Since normal human airways share an anatomical structure, we design a graph prototype whose structure follows the normal airway anatomy. Then, we learn the prototype and a graph neural network from a weakly-supervised airway dataset, i.e., only the holistic label is available, indicating if the airway has anomaly or not, but which bronchus node has the anomaly is unknown. During inference, the graph neural network predicts the anomaly score at both the holistic level and node-level of an airway. "," Since normal human airways share an anatomical structure, we design a graph prototype whose structure follows the normal airway anatomy. Then, we learn the prototype and a graph neural network from a weakly-supervised airway dataset, i.e., only the holistic label is available, indicating if the airway has anomaly or not, but which bronchus node has the anomaly is unknown. During inference, the graph neural network predicts the anomaly score at both the holistic level and node-level of an airway. ",Detecting the airway anomaly can be an essential part to aid the lung disease diagnosis. ,"Graph analysis, Neural network","Specificity, Sensitivity",No,"But the data has to be segmented before using their model: Given the segmented and classified airway results shared by [7] (Fig. 2(b–c)), we encode the anatomical structure of the airways, as well as their image properties and graph properties into feature vectors. ",Yes,"We collected datasets from 3 resources: The Lung Image Database Consortium (LIDC) [8], The Lung Tissue Research Consortium (LTRC) [9], and The National Lung Screening Trial (NLST)","We collected datasets from 3 resources: The Lung Image Database Consortium (LIDC) [8], The Lung Tissue Research Consortium (LTRC) [9], and The National Lung Screening Trial (NLST) [10], leading to 62 normal samples and 23 anomaly samples in total. ",Public,"Looking at the references and googling, all 3 seem to be public",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,"(1)
Stony Brook University, Stony Brook, NY, USA  Nothing is mentioned",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,The code is publicly available on Github,
03/10/2022 09.06.44,270,2021,Yes,It was accurately labelled,computer-aided mild cognitive impairment (MCI) conversion prediction,we propose a region ensemble model using a divide and conquer strategy to capture the disease’s finer representation,"Despite many recent advances, computer-aided mild cognitive impairment (MCI) conversion prediction is still a very challenging task due to: 1) the abnormal areas are subtle compared to the size of the whole brain, 2) the features’ dimension is much larger than the number of samples. ",Neural network,"AUC, Specificity, Accuracy, Sensitivity",Yes,"Since our method needs voxel-level annotation to extract brain regions, we use the dataset in [13] and the method in [19] to train a segmentation model which segments the whole brain into 134 regions.",Yes,We perform experiments on the public Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset [18],"Following [9–12], in all experiments, we treat ADNI-1 as the training set and leave ADNI-2 for testing to make an easier comparison. The training and testing set contains 226 sMCI vs. 167 pMCI and 239 sMCI vs. 38 pMCI, respectively. We also collect 199 AD and 229 NC samples in ADNI-1 as the additional samples to optimize the proposed relation regularized loss.",Public,ADNI is available ,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 100149, China
(2)
NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
(3)
Brainnetome Center, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
(4)
CAS Center for Excellence of Brain Science and Intelligence Technology, Beijing, 100190, China   This work has been supported by the National Key Research and Development Program Grant 2018AAA0100400, the National Natural Science Foundation of China (NSFC) grants 61773376, 61836014, 61721004 and 31870984.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
29/09/2022 10.07.02,269,2021,Yes,It was accurately labelled,we propose a tensor-based multi-index representation learning (TMRL) framework for fMRI-based MDD detection,", we propose a tensor-based multi-index representation learning (TMRL) framework for fMRI-based MDD detection","Major depression disorder (MDD) is one of the most prevalent disabling disorder, characterized by depressed mood, loss of interest or pleasure in nearly all activities. This mental illness has a high mortality rate due to the suicidal behavior of MDD patients, while the high cost of treatment troubles patients, their family members, and society [1, 2]. Even though many efforts have been made in clinical neuroscience and psychiatric research, the unknown etiology and pathological mechanism still prevent us from fully understanding the disease.",representation learning,"Specificity, Accuracy, F1 score, Sensitivity",No,"Each fMRI scan was basically pre-processed by using the Data Processing Assistant for Resting-State fMRI (DPARSF). In this pipeline, we first discard the first 10 time points, followed by slice timing correction, head motion correction, regression of nuisance co-variants of head motion parameters, white matter, and cerebrospinal fluid. Images are then normalized with an EPI template in the MNI space, resampling to $$3 \times 3\times 3\,\text {mm}^{3}$$ resolution, and spatial smoothing using a $$6\,\text {mm}$$ full-width at half-maximum (FWHM) Gaussian kernel. ",Yes,The public rs-fMRI dataset,The public rs-fMRI dataset consists of 533 subjects1,Public,The public rs-fMRI dataset consists of 533 subjects1,No,Nothing is mentioned,Yes,"Article contains a table covering the gender, age, education year, first period, on medication and illness time of subjects included!",No,Nothing is mentioned,Yes,"(1)
Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, USA
(2)
Brainnetome Center & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
(3)
State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing, 100678, China
(4)
School of Computer Science and Technology, East China Normal University, Shanghai, 200241, China   This work was finished when D. Yao was visiting the University of North Carolina at Chapel Hill. D. Yao and M. Liu was partly supported by NIH grant (No. AG041721). Z. Zhang was partly supported by the National Key Research and Development Program of China (No. 2016YFD0700100).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,The table showing the demographics and electronic supplementary material,
29/09/2022 10.01.00,268,2021,Yes,It was accurately labelled,Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification,"we propose a novel Categorical Relation-preserving Contrastive Knowledge Distillation (CRCKD) algorithm, which takes the commonly used mean-teacher model as the supervisor. ","The amount of medical images for training deep classification models is typically very scarce, making these deep models prone to overfit the training data. Studies showed that knowledge distillation (KD), especially the mean-teacher framework which is more robust to perturbations, can help mitigate the over-fitting effect. ","neural network, knowledge distillation","Accuracy, F1 score,  average precision, balanced accuracy,",No,Nothing is mentioned,Yes,"HAM10000 [23, 24] and APTOS datasets [25].","The HAM10000 consists of 10015 dermoscopy images labeled by 7 types of skin lesions. In APTOS, there are 3662 fundus images for grading diabetic retinopathy into five categories. ",Public,"Dataset: We evaluated our proposed CRCKD framework on the HAM10000 [23, 24] and APTOS datasets [25]. The HAM10000 consists of 10015 dermoscopy images labeled by 7 types of skin lesions. In APTOS, there are 3662 fundus images for grading diabetic retinopathy into five categories. These two datasets both suffer from severe class imbalance. A detailed description of these two datasets is provided in the supplementary material.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong, China
(2)
Department of Information Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong, China
(3)
School of Informatics, Xiamen University, Xiamen, China
(4)
Department of Electrical Engineering, City University of Hong Kong, Kowloon, Hong Kong, China
(5)
Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China  The work described in this paper was supported by National Key R&D program of China with Grant No. 2019YFB1312400, Hong Kong RGC CRF grant C4063-18G, and Hong Kong RGC GRF grant #14211420.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Also has supplementary electronic material, but there is a mention of what the supplementary material contains in the text, more info about the datasets, so some of the above questions may have different answers if this was available",
29/09/2022 08.59.47,266,2021,Yes,It was accurately labelled,"Universal Lesion Detection (ULD) in computed tomography (CT) [1–14], which aims to localize different types of lesions instead of identifying lesion types [15–26], plays an essential role in computer-aided diagnosis (CAD","In this paper we propose a BM-based conditional training for two-stage ULD, which can (i) reduce positive vs. negative anchor imbalance via a BM-based conditioning (BMC) mechanism for anchor sampling instead of traditional IoU-based rule; and (ii) adaptively compute size-adaptive BM (ABM) M. de Bruijne et al. (eds.)Medical Image Computing and Computer Assisted Intervention – MICCAI 2021Image Processing, Computer Vision, Pattern Recognition, and Graphics12905
https://doi.org/10.1007/978-3-030-87240-3_14
Conditional Training with Bounding Map for Universal Lesion Detection
Han Li1, 2  , Long Chen2, 3  , Hu Han2  , Ying Chi4   and S. Kevin Zhou1, 2  
(1)
Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), School of Biomedical Engineering & Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China
(2)
Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China
(3)
School of Electronic, Electrical and Communication Engineering, University of the Chinese Academy of Science, Beijing, China
(4)
Healthcare Intelligence, AIC, DAMO Academy, Alibaba Group, Hangzhou, China
 
 
Han Li
Email: han.li@miracle.ict.ac.cn
 
Long Chen
Email: long.chen@miracle.ict.ac.cn
 
Hu Han (Corresponding author)
Email: hanhu@ict.ac.cn
 
Ying Chi
Email: xinyi.cy@alibaba-inc.com
Abstract
Universal Lesion Detection (ULD) in computed tomography plays an essential role in computer-aided diagnosis. Promising ULD results have been reported by coarse-to-fine two-stage detection approaches, but such two-stage ULD methods still suffer from issues like imbalance of positive v.s. negative anchors during object proposal and insufficient supervision problem during localization regression and classification of the region of interest (RoI) proposals. While leveraging pseudo segmentation masks such as bounding map (BM) can reduce the above issues to some degree, it is still an open problem to effectively handle the diverse lesion shapes and sizes in ULD. In this paper we propose a BM-based conditional training for two-stage ULD, which can (i) reduce positive vs. negative anchor imbalance via a BM-based conditioning (BMC) mechanism for anchor sampling instead of traditional IoU-based rule; and (ii) adaptively compute size-adaptive BM (ABM) from lesion bounding-box, which is used for improving lesion localization accuracy via ABM-supervised segmentation.",we propose a novel training mechanism for ULD to effectively reduce positive vs. negative anchor imbalance via a BM-based conditioning (BMC) mechanism in stage-1 and improve lesion localization accuracy by leveraging a size-adaptive BM (ABM) for supervising the segmentation branch in stage-2. ,map regression,Sensitivity,Yes,"Sect. 2.2 introduces the BMC mechanism, and Sect. 2.3 explains the newly introduced ABM-supervised segmentation branch.",Yes,DeepLesion dataset,"The dataset contains 32,735 lesions on 32,120 axial slices from 10,594 CT studies of 4,427 unique patients. Most existing datasets typically focus on one type of lesion, while DeepLesion contains a variety of lesions with large diameters ranges (from 0.21 to 342.5 mm). The 12-bit intensity CT is rescaled to [0,255] with different window ranges settings used in different frameworks. Also, every CT slice is resized and interpolated according to the detection frameworks’ setting. We follow the official split, i.e., $$70\%$$ for training, $$15\%$$ for validation and $$15\%$$ for testing.",Public,Searching for it shows it can be found by anyone,Don't know,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), School of Biomedical Engineering & Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China
(2)
Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China
(3)
School of Electronic, Electrical and Communication Engineering, University of the Chinese Academy of Science, Beijing, China
(4)
Healthcare Intelligence, AIC, DAMO Academy, Alibaba Group, Hangzhou, China   This research was supported in part by the Natural Science Foundation of China (grant 61732004), Youth Innovation Promotion Association CAS (grant 2018135) and Alibaba Group through Alibaba Innovative Research Program.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"There is a doi in the electronic supplementary material, but still only available behind a paywall and am unsure how much more material is actually provided","I would say this is perhaps more classification adjacent, ULD seems to be about localizing different types of lesions, and is used for computer-aided diagnosis, but not as a stand alone part.. "
27/09/2022 11.37.14,262,2021,Yes,It was accurately labelled,"In this paper, we propose a coherent cooperative learning framework based on transfer learning for unsupervised cross-domain classification.","In this paper, we propose a coherent cooperative learning framework based on transfer learning for unsupervised cross-domain classification.","In the practical application of medical image analysis, due to the different data distributions of source domain and target domain and the lack of the labels of target domain, domain adaptation for unsupervised cross-domain classification attracts widespread attention. However, current methods take knowledge transfer model and classification model as two separate training stages, which inadequately considers and utilizes the intrinsic information interaction between module","Unsupervised learning, Transfer learning, Neural network","Accuracy, Precision, Recall, F1 score",No,Nothing is mentioned,Yes,"We use three databases in the experiments, and their information is shown in Table 1. The Chest X-Ray1 is divided into training dataset and testing dataset [12]. Single lesion2 and Multiple lesions3 are the training datasets of two open lesion recognition competitions.","No quote, but there is Table 1 which includes the number of images of each used dataset",Public,"The Chest X-Ray1 is divided into training dataset and testing dataset [12]. Single lesion2 and Multiple lesions3 are the training datasets of two open lesion recognition competitions. (and I think the first one is also accessible, there is a link in the footnote)",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Shanghai Key Laboratory of Multidimensional Information Processing, School of Communication and Electronic Engineering, East China Normal University, Shanghai, China  This work was supported in part by 2030 National Key Research and Development Program of China (2018AAA0100500), the National Nature Science Foundation of China (no. 61773166), Projects of International Cooperation of Shanghai Municipal Science and Technology Committee (14DZ2260800), the Fundamental Research Funds for the Central Universities, and the ECNU Academic Innovation Promotion Program for Excellent Doctoral Students (YBNLTS2021-040).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​10) contains supplementary material, which is available to authorized users.",
27/09/2022 11.14.02,255,2021,Yes,It was accurately labelled,"In the early diagnosis of lung cancer, an important step is classifying malignancy/benignity for each lung nodule.","Recently, the contextual features attract increasing attention, due to the complementary information they provide. Clinically, such contextual features refer to the features of nodule’s surrounding structures, such that (together with nodule’s features) they can expose discriminate patterns for the malignant/benign, such as vascular convergence and fissural attachment. To leverage such contextual features, we propose a Context Attention Network (CA-Net) which extracts both nodule’s and contextual features and then effectively fuses them during malignancy/benignity classification. To accurately identify the contextual features that contain structures distorted/attached by the nodule, we take the nodule’s features as a reference via an attention mechanism. Further, we propose a feature fusion module that can adaptively adjust the weights of nodule’s and contextual features across nodules.","Recently, the contextual features attract increasing attention, due to the complementary information they provide. Clinically, such contextual features refer to the features of nodule’s surrounding structures, such that (together with nodule’s features) they can expose discriminate patterns for the malignant/benign, such as vascular convergence and fissural attachment. To leverage such contextual features, we propose a Context Attention Network (CA-Net) which extracts both nodule’s and contextual features and then effectively fuses them during malignancy/benignity classification",Neural network,"AUC, Accuracy, Log Loss",Yes,"The whole pipeline of our method, namely Context Attention Network (CA-Net), is illustrated in Fig. 2. As shown, it is the sequential of three stages: (i) Nodule Detection that detects all nodules from the CT image;",Yes,DSB2017 dataset,"There are 1397, 198, and 506 patients in the training, validation, and test set, respectively.",Public,"Experimentally, our CA-Net outperforms the 1st place by a noticeable margin on Kaggle DSB2017 dataset.",No,Nothing is mentioned in the article,No,This is the only description in the article: This dataset provides pathologically proven lung cancer label for each patient.,No,Nothing is mentioned in the article,Yes,"(1)
Department of Computer Science and Technology, Peking University, Beijing, China
(2)
Peking University, Beijing, China
(3)
Deepwise AI Lab, Beijing, China
(4)
University of Hong Kong, Pokfulam, Hong Kong   This work was supported by MOST-2018AAA0102004, NSFC-61625201, and the Beijing Municipal Science and Technology Planning Project (Grant No. Z201100005620008).",No,Nothing is mentioned in the article,Yes,"Experimentally, our CA-Net outperforms the 1st place by a noticeable margin on Kaggle DSB2017 dataset.",No,Nothing is mentioned in the article,Yes,"To some degree perhaps - though who knows what is actually in this supplementary material?
Also no mention is made of the repo/code for reproducibility... 
Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​3) contains supplementary material, which is available to authorized users.",
21/10/2022 08.59.27,325,2021,Yes,It was accurately labelled,"Current methods for whole slide image (WSI) histopathology subregion classification and survival prediction rely on phenotype clustering from randomly sampled image tiles or on analyzing key tiles selected by experts from the much larger in size WSIs. These approaches do not capture the whole tissue region present in a histopathology image, also missing the spatial distribution of features that could be critical for good survival predictors. We propose a novel method that extracts a whole slide feature map (WSFM) in the first step and then uses it to train the survival prediction model.","Current methods for whole slide image (WSI) histopathology subregion classification and survival prediction rely on phenotype clustering from randomly sampled image tiles or on analyzing key tiles selected by experts from the much larger in size WSIs. These approaches do not capture the whole tissue region present in a histopathology image, also missing the spatial distribution of features that could be critical for good survival predictors. We propose a novel method that extracts a whole slide feature map (WSFM) in the first step and then uses it to train the survival prediction model.","With large histopathology images being captured at an increasing rate and the development of deep learning models for analyzing such images, image-based computer-aided diagnosis based on segmentation and classification has rapidly expanded. Histopathology images capture tumor growth and morphology with great details, making them highly suitable for patient risk assessment. However, there is little work on histopathology image-based survival analysis because of the challenges involved. First, in contrast to the natural images where a label is learned for a relatively smaller image (100s or 1000s of pixels) input, in histopathology studies labels are learned from extremely large inputs (WSIs might be $$10^{10}$$ pixels). Thus, patient samples with corresponding labels are usually limited, while a massive number of parameters need to be optimized when using WSIs in deep learning-based approaches. Second, histopathology images have extremely heterogeneous structures and textures, and annotating specific regions to facilitate risk prediction is laborious and often unfeasible


(For disease task: We focus on the Glioblastoma multiforme (GBM) brain cancer in our study.)",Neural network,concordance index,No,Nothing is mentioned,Yes,We focus on the Glioblastoma multiforme (GBM) brain cancer in our study. We downloaded the diagnostic WSIs and clinical data for GBM from The Cancer Genome Atlas (TCGA) [13] cohort. This dataset contains 860 h&e stained diagnostic histopathology slides and clinical information from 389 patients. ,We focus on the Glioblastoma multiforme (GBM) brain cancer in our study. We downloaded the diagnostic WSIs and clinical data for GBM from The Cancer Genome Atlas (TCGA) [13] cohort. This dataset contains 860 h&e stained diagnostic histopathology slides and clinical information from 389 patients. ,Public,We evaluate the model performance using the Concordance index (C-index) [2] value and compare it with other state-of-the-art methods on a publicly available dataset.,No,Nothing is mentioned,Yes,"This dataset contains 860 h&e stained diagnostic histopathology slides and clinical information from 389 patients. The clinical data include patient age, gender, ethnicity, race, vital status, days to last follow up and days to death information. 

The clinical features used in the building of SSCNN are numerical age, and categorical gender, ethnicity and race. ",No,Nothing is mentioned,No,"Department of Computer Science, University of Texas at Dallas, Richardson, TX 75080, USA",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
21/10/2022 09.09.51,329,2021,Yes,It was accurately labelled,"In this paper, we propose a novel multi-label chest X-ray classification model that accurately classifies the image finding and also localizes the findings to their correct anatomical regions. ","In this paper, we propose a novel multi-label chest X-ray classification model that accurately classifies the image finding and also localizes the findings to their correct anatomical regions. ","Radiologists usually observe anatomical regions of chest X-ray images as well as the overall image before making a decision. However, most existing deep learning models only look at the entire X-ray image for classification, failing to utilize important anatomical information.","Graph analysis, Neural network",AUC,No,Nothing specific is mentioned,Yes,The Chest ImaGenome dataset ,"There are a total of 217,417 images in the dataset, of which 153,333 have at least one of the L1-L9 labels globally. Of these images, 3,877,010 bounding boxes were extracted automatically and 720,098 of them have at least one or more of the 9 labels.",Public,Can be found upon searching,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
Rensselaer Polytechnic Institute, Troy, NY 12180, USA
(2)
IBM Research, Almaden Research Center, San Jose, CA 95120, USA
(3)
Virginia Tech, Blacksburg, VA 24061, USA

This work was supported by the Rensselaer-IBM AI ResearchCollaboration, part of the IBM AI Horizons Network.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
26/10/2022 08.36.56,138,2021,Yes,It was accurately labelled,"We discuss two example tasks: mammography classification with craniocaudal (CC) and mediolateral oblique (MLO) views, and chest X-ray classification","Multi-view medical image analysis often depends on the combination of information from multiple views. However, differences in perspective or other forms of misalignment can make it difficult to combine views effectively, as registration is not always possible. Without registration, views can only be combined at a global feature level, by joining feature vectors after global pooling. We present a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps. We demonstrate this method on multi-view mammography and chest X-ray datasets. On both datasets, we find that a cross-view transformer that links spatial feature maps can outperform a baseline model that joins feature vectors after global pooling.","Multi-view medical image analysis often depends on the combination of information from multiple views. However, differences in perspective or other forms of misalignment can make it difficult to combine views effectively, as registration is not always possible. Without registration, views can only be combined at a global feature level, by joining feature vectors after global pooling. We present a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps. We demonstrate this method on multi-view mammography and chest X-ray datasets. On both datasets, we find that a cross-view transformer that links spatial feature maps can outperform a baseline model that joins feature vectors after global pooling.",Neural network,AUC,No,Nothing is mentioned,Yes,"CBIS-DDSM. The CBIS-DDSM [5, 7] is a public mammography dataset with craniocaudal (CC) and mediolateral-oblique (MLO) views with manual annotations. We solve a binary classification problem on the scans with mass abnormalities, predicting benign vs. malignant for each CC/MLO pair. We created five subsets for cross-validation, using stratified sampling while ensuring that all scans for a patient remain in the same subset. In total, we used image pairs of 708 breasts (636 unique patients), with approximately 46% labelled malignant.

During preprocessing, we cropped the scans using the method described by Wu et al. [16], using thresholding to position a fixed-size cropping window that includes the breast but excludes most of the empty background. We downsampled the cropped images to 1/16th of the original resolution to obtain images of $$305 \times 188$$ pixels. We normalized the intensities to $$\mu =0$$ and $$\sigma =1$$, measured on the nonzero foreground pixels of each scan.

CheXpert. The CheXpert dataset [6] is a large public dataset of frontal and lateral chest X-ray scans, annotated for 13 different observations with labels negative, positive, uncertain, or unknown (see supplement). We used the downsampled version of the dataset as provided on the website. We selected the visits with complete frontal and lateral views and divided the patients in random subsets for training (23628 samples for 16810 unique patients), validation (3915 s, 2802p) and testing 3870 s, 2802p). We normalized the images to $$\mu =0$$ and $$\sigma =1$$ and used zero-padding to obtain a constant size of $$390 \times 390$$ pixels for each view.","CBIS-DDSM. The CBIS-DDSM [5, 7] is a public mammography dataset with craniocaudal (CC) and mediolateral-oblique (MLO) views with manual annotations. We solve a binary classification problem on the scans with mass abnormalities, predicting benign vs. malignant for each CC/MLO pair. We created five subsets for cross-validation, using stratified sampling while ensuring that all scans for a patient remain in the same subset. In total, we used image pairs of 708 breasts (636 unique patients), with approximately 46% labelled malignant.

During preprocessing, we cropped the scans using the method described by Wu et al. [16], using thresholding to position a fixed-size cropping window that includes the breast but excludes most of the empty background. We downsampled the cropped images to 1/16th of the original resolution to obtain images of $$305 \times 188$$ pixels. We normalized the intensities to $$\mu =0$$ and $$\sigma =1$$, measured on the nonzero foreground pixels of each scan.

CheXpert. The CheXpert dataset [6] is a large public dataset of frontal and lateral chest X-ray scans, annotated for 13 different observations with labels negative, positive, uncertain, or unknown (see supplement). We used the downsampled version of the dataset as provided on the website. We selected the visits with complete frontal and lateral views and divided the patients in random subsets for training (23628 samples for 16810 unique patients), validation (3915 s, 2802p) and testing 3870 s, 2802p). We normalized the images to $$\mu =0$$ and $$\sigma =1$$ and used zero-padding to obtain a constant size of $$390 \times 390$$ pixels for each view.",Public,"CBIS-DDSM. The CBIS-DDSM [5, 7] is a public mammography dataset with craniocaudal (CC) and mediolateral-oblique (MLO) views with manual annotations. We solve a binary classification problem on the scans with mass abnormalities, predicting benign vs. malignant for each CC/MLO pair. We created five subsets for cross-validation, using stratified sampling while ensuring that all scans for a patient remain in the same subset. In total, we used image pairs of 708 breasts (636 unique patients), with approximately 46% labelled malignant.

During preprocessing, we cropped the scans using the method described by Wu et al. [16], using thresholding to position a fixed-size cropping window that includes the breast but excludes most of the empty background. We downsampled the cropped images to 1/16th of the original resolution to obtain images of $$305 \times 188$$ pixels. We normalized the intensities to $$\mu =0$$ and $$\sigma =1$$, measured on the nonzero foreground pixels of each scan.

CheXpert. The CheXpert dataset [6] is a large public dataset of frontal and lateral chest X-ray scans, annotated for 13 different observations with labels negative, positive, uncertain, or unknown (see supplement). We used the downsampled version of the dataset as provided on the website. We selected the visits with complete frontal and lateral views and divided the patients in random subsets for training (23628 samples for 16810 unique patients), validation (3915 s, 2802p) and testing 3870 s, 2802p). We normalized the images to $$\mu =0$$ and $$\sigma =1$$ and used zero-padding to obtain a constant size of $$390 \times 390$$ pixels for each view.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"
Data Science Group, Faculty of Science, Radboud University, Nijmegen, The Netherlands

The research leading to these results is part of the project “MARBLE”, funded from the EFRO/OP-Oost under grant number PROJ-00887. Some of the experiments were carried out on the Dutch national e-infrastructure with the support of SURF Cooperative.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​10) contains supplementary material, which is available to authorized users.

They actually mention (ish) what is is the supplementary material: 
5 Experiments
Models. We compare four models: the single-view model, the late-join model, and the token-based and pixel-based cross-view transformers. All models use the same ResNet-18 architecture [4] for the convolution and pooling blocks up to the global average pooling layer. We use pre-trained weights on ImageNet, as provided by PyTorch. After global average pooling, we concatenate the feature vectors for both iews and use this as input for a single fully connected layer that computes the output. (See the supplementary material for a detailed view.)

The transformer performance was not very sensitive to the number of heads or tokens: all settings produced similar results (see the table in the supplementary results)",
26/10/2022 08.43.55,139,2021,Yes,It was accurately labelled,"But note this is a general method that works (and is tested on classification). They also test on a segmentation task in the article
 
Extensive experiments are conducted and demonstrate that our method is general enough to different tasks and stain methods, including H&E stains for tumor classification and hematological stains for bone marrow cell instance segmentation. The results validate that the proposed stain mix-up can significantly improves the performance on the unseen domains.","
Abstract
Computational histopathology studies have shown that stain color variations considerably hamper the performance. Stain color variations indicate the slides exhibit greatly different color appearance due to the diversity of chemical stains, staining procedures, and slide scanners. Previous approaches tend to improve model robustness via data augmentation or stain color normalization. However, they still suffer from generalization to new domains with unseen stain colors. In this study, we address the issue of unseen color domain generalization in histopathology images by encouraging the model to adapt varied stain colors. To this end, we propose a novel data augmentation method, stain mix-up, which incorporates the stain colors of unseen domains into training data. Unlike previous mix-up methods employed in computer vision, the proposed method constructs the combination of stain colors without using any label information, hence enabling unsupervised domain generalization. Extensive experiments are conducted and demonstrate that our method is general enough to different tasks and stain methods, including H&E stains for tumor classification and hematological stains for bone marrow cell instance segmentation. The results validate that the proposed stain mix-up can significantly improves the performance on the unseen domains.","Computer-aided diagnosis based on histopathology images, such as whole slide images (WSIs) and field of views (FoVs) of tissue sections, gains significant progress owing to the great success of machine learning algorithms in digital pathology. Tissue sections are typically stained with various stains to make tissues visible under the microscope. However, tissue manipulation, staining, and even scanning often result in substantial color appearance variations in histopathology images, and degrade machine learning algorithms due to the domain gap of colors. Thus, it is crucial to take color appearance variations into account when developing machine learning algorithms for histopathology image analysis. Specifically, two strategies are widely used, including 1) augmenting color patterns of training data to enhance model robustness; and 2) normalizing all histopathology images to a single color pattern so that the unfavorable impact of color variations in the subsequent process can be alleviated.",Neural network,AUC,No,Nothing is mentioned,Yes,"CAMELYON17. We use the CAMELYON17 [1] dataset to evaluate the performance of the proposed method on tumor/normal classification. In this dataset, a total of 500 H&E stained WSIs are collected from five medical centers (denoted by $$C_1$$, $$C_2$$, ... $$C_5$$ respectively), 50 of which include lesion-level annotations. All positive and negative WSIs are randomly split into training/validation/test sets with the following distributions: $$C_1: 37/22/15$$, $$C_2: 34/20/14$$, $$C_3: 43/24/18$$, $$C_4: 35/20/15$$, $$C_5: 36/20/15$$. We extract image tiles in a size of $$256 \times 256$$ pixels from the annotated tumors for positive patches and from tissue regions of WSIs without tumors for negative patches.","CAMELYON17. We use the CAMELYON17 [1] dataset to evaluate the performance of the proposed method on tumor/normal classification. In this dataset, a total of 500 H&E stained WSIs are collected from five medical centers (denoted by $$C_1$$, $$C_2$$, ... $$C_5$$ respectively), 50 of which include lesion-level annotations. All positive and negative WSIs are randomly split into training/validation/test sets with the following distributions: $$C_1: 37/22/15$$, $$C_2: 34/20/14$$, $$C_3: 43/24/18$$, $$C_4: 35/20/15$$, $$C_5: 36/20/15$$. We extract image tiles in a size of $$256 \times 256$$ pixels from the annotated tumors for positive patches and from tissue regions of WSIs without tumors for negative patches.",Public,"Available in a challenge, can be found searching",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"aetherAI, Taipei, Taiwan
(2)
National Yang Ming Chiao Tung University, Hsinchu, Taiwan

We thank Wen-Chien Chou M.D.(National Taiwan University Hospital), Ta-Chuan Yu M.D.(National Taiwan University Hospital Yunlin Branch) and Poshing Lee M.D.(Department of Hematopathology, BioReference) for Hema dataset construction. This paper was supported in part by the Ministry of Science and Technology, Taiwan, under Grants MOST 110-2634-F-007-015 and MOST 109-2221-E-009-113-MY3.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​11) contains supplementary material, which is available to authorized users.",
26/10/2022 08.54.34,145,2021,Yes,It was accurately labelled,"Abstract
Heterogeneity in medical data, e.g., from data collected at different sites and with different protocols in a clinical study, is a fundamental hurdle for accurate prediction using machine learning models, as such models often fail to generalize well. This paper leverages a recently proposed normalizing-flow-based method to perform counterfactual inference upon a structural causal model (SCM), in order to achieve harmonization of such data. A causal model is used to model observed effects (brain magnetic resonance imaging data) that result from known confounders (site, gender and age) and exogenous noise variables. Our formulation exploits the bijection induced by flow for the purpose of harmonization. We infer the posterior of exogenous variables, intervene on observations, and draw samples from the resultant SCM to obtain counterfactuals. This approach is evaluated extensively on multiple, large, real-world medical datasets and displayed better cross-domain generalization compared to state-of-the-art algorithms. Further experiments that evaluate the quality of confounder-independent data generated by our model using regression and classification tasks are provided.","Also a more general method, that is shown to work for classification and regression in this article 
This paper leverages a recently proposed normalizing-flow-based method to perform counterfactual inference upon a structural causal model (SCM), in order to achieve harmonization of such data. ","Deep learning models have shown great promise in medical imaging diagnostics [11] and predictive modeling with applications ranging from segmentation tasks [19] to more complex decision-support functions for phenotyping brain diseases and personalized prognosis. However deep learning models tend to have poor reproducibility across hospitals, scanners, and patient cohorts; these high-dimensional models tend to overfit to specific datasets and generalize poorly across training data [6]. One potential solution to the above problem is to train on very large and diverse databases but this can be prohibitive, because data may change frequently (e.g., new imaging devices are introduced) and gathering training labels for medical images is expensive. More importantly, even if it were possible to train a model on data that covers all possible variations across images, such a model would almost certainly sacrifice accuracy in favor of generalization—it would rely on coarse imaging features that are stable across, say imaging devices and patient populations, and might fail to capture more subtle and informative detail. Methods that can tackle heterogeneity in medical data without sacrificing predictive accuracy are needed, including methods for “data harmonization”, which would allow training a classifier on, say data from one site, and obtaining similar predictive accuracy on data from another site.",Graph analysis,Accuracy,Yes,"We first perform a sequence of preprocessing steps on these images, including bias-filed correction [35], brain tissue extraction via skull-stripping [8], and multi-atlas segmentation [9]. Each scan is then segmented into 145 anatomical regions of interests (ROIs) spanning the entire brain, and finally volumes of the ROIs are taken as the features",Yes,ADNI," We use 6,921 3D T1-weighted brain magnetic resonance imaging (MRI) scans acquired from multiple scanners or sites in Alzheimer’s Disease Neuroimaging Initiative (ADNI) [15] and the iSTAGING consortium [13] which consists of Baltimore Longitudinal Study of Aging (BLSA) [2, 31], Study of Health in Pomerania (SHIP) [14] and the UK Biobank (UKBB) [34]. 

Only ADNI is relevant for this work, and they don't state how much they use from ADNI specifically",Public,ADNI,No,Nothing is mentioned,Yes,"This is all they write: 
Detailed demographic information of the datasets is provided in the supplementary material.",No,Nothing is mentioned,Yes,"Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, USA
(2)
Center for Biomedical Image Computing and Analytics (CBICA), Philadelphia, USA
(3)
General Robotics, Automation, Sensing and Perception Laboratory (GRASP), Philadelphia, USA
(4)
Department of Radiology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, USA

We thank Ben Glocker, Nick Pawlowski and Daniel C. Castro for suggestions. This work was supported by the National Institute on Aging (grant numbers RF1AG054409 and U01AG068057) and the National Institute of Mental Health (grant number R01MH112070). Pratik Chaudhari would like to acknowledge the support of the Amazon Web Services Machine Learning Research Award.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"This one also includes some mention of what is in the supplementary material

Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​17) contains supplementary material, which is available to authorized users.

Implementation details for the SCM and the classifier, and the best validation log-likelihood for each model are shown in the supplementary material.
Detailed demographic information of the datasets is provided in the supplementary material.",
26/10/2022 09.07.22,159,2021,Yes,It was accurately labelled,"Federated learning (FL) has emerged with increasing popularity to collaborate distributed medical institutions for training deep networks. However, despite existing FL algorithms only allow the supervised training setting, most hospitals in realistic usually cannot afford the intricate data labeling due to absence of budget or expertise. This paper studies a practical yet challenging FL problem, named Federated Semi-supervised Learning (FSSL), which aims to learn a federated model by jointly utilizing the data from both labeled and unlabeled clients (i.e., hospitals). We present a novel approach for this problem, which improves over traditional consistency regularization mechanism with a new inter-client relation matching scheme. The proposed learning scheme explicitly connects the learning across labeled and unlabeled clients by aligning their extracted disease relationships, thereby mitigating the deficiency of task knowledge at unlabeled clients and promoting discriminative information from unlabeled samples. We validate our method on two large-scale medical image classification datasets.","This paper studies a practical yet challenging FL problem, named Federated Semi-supervised Learning (FSSL), which aims to learn a federated model by jointly utilizing the data from both labeled and unlabeled clients (i.e., hospitals). We present a novel approach for this problem, which improves over traditional consistency regularization mechanism with a new inter-client relation matching scheme. ","Federated learning (FL) has emerged with increasing popularity to collaborate distributed medical institutions for training deep networks. However, despite existing FL algorithms only allow the supervised training setting, most hospitals in realistic usually cannot afford the intricate data labeling due to absence of budget or expertise. This paper studies a practical yet challenging FL problem, named Federated Semi-supervised Learning (FSSL), which aims to learn a federated model by jointly utilizing the data from both labeled and unlabeled clients (i.e., hospitals). ",Neural network,"AUC, Specificity, Accuracy, F1 score, Sensitivity",No,Nothing is mentioned,Yes,"Task 1 - Intracranial Hemorrhage Diagnosis. We perform ICH diagnosis with the RSNA ICH Detection dataset[26], which aims to classify CT slices into 5 subtypes of ICH disease. Since most images in this dataset are healthy without any of the subtypes, we randomly sample 25000 slices from the dataset which contain one of the 5 subtypes of ICH for evaluation. These samples are then randomly divided into 70%, 10% and 20% for training, validation and testing. Since multiple slices may come from the same patient in this dataset, we have ensured no overlapped patients exist across the three split for a valid evaluation.

Task 2 - Skin Lesion Diagnosis. We employ ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection[22] dataset for skin lesion diagnosis, which contains 10015 dermoscopy images in the official training set labeled by 7 types of skin lesions. As the ground truth of official validation and testing set was not released, we randomly divide the entire training set to 70% for training, 10% for validation and 20% for testing. We perform the same data pre-processing for the two tasks. Specifically, we first resized the original images from 512 $$\times $$ 512 to 224 $$\times $$ 224. ","Task 1 - Intracranial Hemorrhage Diagnosis. We perform ICH diagnosis with the RSNA ICH Detection dataset[26], which aims to classify CT slices into 5 subtypes of ICH disease. Since most images in this dataset are healthy without any of the subtypes, we randomly sample 25000 slices from the dataset which contain one of the 5 subtypes of ICH for evaluation. These samples are then randomly divided into 70%, 10% and 20% for training, validation and testing. Since multiple slices may come from the same patient in this dataset, we have ensured no overlapped patients exist across the three split for a valid evaluation.

Task 2 - Skin Lesion Diagnosis. We employ ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection[22] dataset for skin lesion diagnosis, which contains 10015 dermoscopy images in the official training set labeled by 7 types of skin lesions. As the ground truth of official validation and testing set was not released, we randomly divide the entire training set to 70% for training, 10% for validation and 20% for testing. We perform the same data pre-processing for the two tasks. Specifically, we first resized the original images from 512 $$\times $$ 512 to 224 $$\times $$ 224. ",Public,"intracranial hemorrhage data is a challenge dataset and skin lesion is ham10000,  so both public",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Kowloon, Hong Kong SAR, China
(2)
Department of Computer Science and Engineering, Beihang University, Beijing, China
(3)
Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China

The work described in this paper was supported in parts by the following grants: Key-Area Research and Development Program of Guangdong Province, China (2020B010165004), Hong Kong Innovation and Technology Fund (Project No. GHP/110/19SZ), Foundation of China with Project No. U1813204 and Shenzhen-HK Collaborative Development Zone.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,(Code will be made available at https://​github.​com/​liuquande/​FedIRM).,
26/10/2022 09.19.18,160,2021,Yes,It was accurately labelled,FedPerl: Semi-supervised Peer Learning for Skin Lesion Classification,"Skin cancer is one of the most deadly cancers worldwide. Yet, it can be reduced by early detection. Recent deep-learning methods have shown a dermatologist-level performance in skin cancer classification. Yet, this success demands a large amount of centralized data, which is oftentimes not available. Federated learning has been recently introduced to train machine learning models in a privacy-preserved distributed fashion demanding annotated data at the clients, which is usually expensive and not available, especially in the medical field. To this end, we propose $$\texttt {FedPerl}$$, a semi-supervised federated learning method that utilizes peer learning from social sciences and ensemble averaging from committee machines to build communities and encourage its members to learn from each other such that they produce more accurate pseudo labels. We also propose the peer anonymization (PA) technique as a core component of $$\texttt {FedPerl}$$. PA preserves privacy and reduces the communication cost while maintaining the performance without additional complexity. ","Skin cancer is one of the most deadly cancers worldwide. Yet, it can be reduced by early detection. Recent deep-learning methods have shown a dermatologist-level performance in skin cancer classification. Yet, this success demands a large amount of centralized data, which is oftentimes not available. Federated learning has been recently introduced to train machine learning models in a privacy-preserved distributed fashion demanding annotated data at the clients, which is usually expensive and not available, especially in the medical field. To this end, we propose $$\texttt {FedPerl}$$, a semi-supervised federated learning method that utilizes peer learning from social sciences and ensemble averaging from committee machines to build communities and encourage its members to learn from each other such that they produce more accurate pseudo labels. We also propose the peer anonymization (PA) technique as a core component of $$\texttt {FedPerl}$$. PA preserves privacy and reduces the communication cost while maintaining the performance without additional complexity. ","Neural network, semi supervised learning","Precision, Recall, F1 score, relative improvement, additional cost",No,Nothing is mentioned,Yes,"We validated $$\texttt {FedPerl}$$ on 38,000 skin lesion images from four publicly available datasets, namely ISIC19 [4] which consists of 25K images with 8 classes of melanoma (MEL), melanocytic nevus (NV), basal cell carcinoma (BCC), actinic keratosis (AK), benign keratosis (BKL), dermatofibroma (DF), the vascular lesion (VASC), and squamous cell carcinoma (SCC); HAM [25] with 10K images (7 classes); Derm7pt [10] with 1K images (6 classes), and PAD-UFES [15] with 2K images (6 classes).","We validated $$\texttt {FedPerl}$$ on 38,000 skin lesion images from four publicly available datasets, namely ISIC19 [4] which consists of 25K images with 8 classes of melanoma (MEL), melanocytic nevus (NV), basal cell carcinoma (BCC), actinic keratosis (AK), benign keratosis (BKL), dermatofibroma (DF), the vascular lesion (VASC), and squamous cell carcinoma (SCC); HAM [25] with 10K images (7 classes); Derm7pt [10] with 1K images (6 classes), and PAD-UFES [15] with 2K images (6 classes).",Public,"We validated $$\texttt {FedPerl}$$ on 38,000 skin lesion images from four publicly available datasets",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Computer Aided Medical Procedures, Technical University of Munich, Munich, Germany
(2)
Helmholtz AI, Helmholtz Zentrum München, Neuherberg, Germany
(3)
The Whiting School of Engineering, Johns Hopkins University, Baltimore, USA

T.B. is financially supported by the German Academic Exchange Service (DAAD).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"One of their main contributions is a privacy aspect, keeping used data private, but more on the model level than on the personal level....

We also propose the peer anonymization (PA) technique as a core component of $$\texttt {FedPerl}$$. PA preserves privacy and reduces the communication cost while maintaining the performance without additional complexity. 

Recently, deep learning-based methods have shown dermatologist-level [5, 24] or superior performance [8, 12, 28] in skin cancer classification. Yet, most of these methods rely on a large curated amount of centralized labeled data, which is usually not available due to privacy issues [16].

Our novel peer anonymization (PA) technique, is simple yet effective to anonymize the peer such that it is less prone to model inversion or deanonymization. PA is designed carefully to reduce the communication cost while maintains performance. Nevertheless, a privacy guarantee for aggregated models (not individuals) is an open issue and has not been thoroughly investigated in the community and mathematical analysis is yet to be proven. Generalization to unseen client is yet to be investigated in future work. This includes investigating different approaches to profile the clients in building the community. Further, a dynamic policy of when and which community to approach should be further investigated.

also includes github and info about supplementary material
(https://​github.​com/​tbdair/​FedPerlV1.​0).

Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​32) contains supplementary material, which is available to authorized users.

Implementation. We opt for EfficientNet as our architecture. Adam for optimization for 500 rounds. The batch size and participation rate were set to 16 & 0.3, respectively. The learning rate, $$\beta $$, $$\gamma $$, T were investigated and found best at 0.00005, 0.5, 0.01, and 2, respectively, whereas $$\tau $$ found best at 0.6 & 0.9 for the federated and local models respectively. Further details in the suppl. material.",
26/10/2022 09.26.52,161,2021,Yes,It was accurately labelled,Perf measures are F1 and AUC,"Nowadays, deep learning methods with large-scale datasets can produce clinically useful models for computer-aided diagnosis. However, the privacy and ethical concerns are increasingly critical, which make it difficult to collect large quantities of data from multiple institutions. Federated Learning (FL) provides a promising decentralized solution to train model collaboratively by exchanging client models instead of private data. However, the server aggregation of existing FL methods is observed to degrade the model performance in real-world medical FL setting, which is termed as retrogress. To address this problem, we propose a personalized retrogress-resilient framework to produce a superior personalized model for each client. Specifically, we devise a Progressive Fourier Aggregation (PFA) at the server to achieve more stable and effective global knowledge gathering by integrating client models from low-frequency to high-frequency gradually. ","Recent years have witnessed the superior performance of deep learning techniques in the field of computer-aided diagnosis [2, 6, 27]. By collecting large quantities of data from multiple institutions, tailored deep learning methods achieved great success and have been applied in the clinical practice to alleviate the workload of physicians [5, 12, 13]. However, this is not a sustainable way to develop future intelligent healthcare systems, where the patient privacy and ethical concerns impede constructing centralized datasets with increasing size",Neural network,"AUC, F1 score",No,nothing is mentioned,Yes,"we collect 8, 940 and 2, 000 images that are diagnosed as nevus, benign keratosis or melanoma from HAM10K [21] and MSK [3] dataset, ","we collect 8, 940 and 2, 000 images that are diagnosed as nevus, benign keratosis or melanoma from HAM10K [21] and MSK [3] dataset, ",Public,both are public upon searching,No,nothing is mentioned,No,nothing is mentioned,No,nothing is mentioned,Yes,"Department of Electrical Engineering, City University of Hong Kong, Kowloon, Hong Kong, China

This work is supported by Shenzhen-Hong Kong Innovation Circle Category D Project SGDX2019081623300177 (CityU 9240008) and CityU SRG 7005229.",No,nothing is mentioned,No,nothing is mentioned,No,nothing is mentioned,Yes,"Topic is a solution to the issue of: the patient privacy and ethical concerns impede constructing centralized datasets with increasing size (which have been shown to do well with computer aided diagnosis)

The code and dataset are available at https://​github.​com/​CityU-AIM-Group/​PRR-FL.",
26/10/2022 09.34.20,164,2021,Yes,It was accurately labelled,"A label-efficient paradigm in computer vision is based on self-supervised contrastive pre-training on unlabeled data followed by fine-tuning with a small number of labels. Making practical use of a federated computing environment in the clinical domain and learning on medical images poses specific challenges. In this work, we propose FedMoCo, a robust federated contrastive learning (FCL) framework, which makes efficient use of decentralized unlabeled medical data. FedMoCo has two novel modules: metadata transfer, an inter-node statistical data augmentation module, and self-adaptive aggregation, an aggregation module based on representational similarity analysis. To the best of our knowledge, this is the first FCL work on medical images. Our experiments show that FedMoCo can consistently outperform FedAvg, a seminal federated learning framework, in extracting meaningful representations for downstream tasks. We further show that FedMoCo can substantially reduce the amount of labeled data required in a downstream task, such as COVID-19 detection, to achieve a reasonable performance.","A label-efficient paradigm in computer vision is based on self-supervised contrastive pre-training on unlabeled data followed by fine-tuning with a small number of labels. Making practical use of a federated computing environment in the clinical domain and learning on medical images poses specific challenges. In this work, we propose FedMoCo, a robust federated contrastive learning (FCL) framework, which makes efficient use of decentralized unlabeled medical data. ","Recent studies in self-supervised learning (SSL) [21] have led to a renaissance of research on contrastive learning (CL) [1]. Self-supervised or unsupervised CL aims to learn transferable representations from unlabeled data. In a CL framework, a model is first pre-trained on unlabeled data in a self-supervised fashion via a contrastive loss, and then fine-tuned on labeled data. Utilizing the state-of-the-art (SOTA) CL frameworks [3, 8, 16, 24], a model trained with only unlabeled data plus a small amount of labeled data can achieve comparable performance with the same model trained with a large amount of labeled data on various downstream tasks.

As a data-driven approach, deep learning has fueled many breakthroughs in medical image analysis (MIA).",Neural network,Accuracy,No,Nothing is mentioned,Yes,"We use three public large-scale CXR datasets as the unlabeled pre-training data to simulate the federated environment, namely CheXpert [11], ChestX-ray8 [26], and VinDr-CXR","We use three public large-scale CXR datasets as the unlabeled pre-training data to simulate the federated environment, namely CheXpert [11], ChestX-ray8 [26], and VinDr-CXR",Public,"We use three public large-scale CXR datasets as the unlabeled pre-training data to simulate the federated environment, namely CheXpert [11], ChestX-ray8 [26], and VinDr-CXR",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Department of Computer Science, University of Oxford, Oxford, UK

We would like to thank Huawei Technologies Co., Ltd. for providing GPU computing service for this study.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,Pseudocode included,
26/10/2022 10.47.21,172,2021,Yes,It was accurately labelled,"Medical image datasets are hard to collect, expensive to label, and often highly imbalanced. The last issue is underestimated, as typical average metrics hardly reveal that the often very important minority classes have a very low accuracy. In this paper, we address this problem by a feature embedding that balances the classes using contrastive learning as an alternative to the common cross-entropy loss. The approach is largely orthogonal to existing sampling methods and can be easily combined with those. We show on the challenging ISIC2018 and APTOS2019 datasets that the approach improves especially the accuracy of minority classes without negatively affecting the majority ones.

Keywords Imbalance classification Medical imaging Contrastive learning","Medical image datasets are hard to collect, expensive to label, and often highly imbalanced. The last issue is underestimated, as typical average metrics hardly reveal that the often very important minority classes have a very low accuracy. In this paper, we address this problem by a feature embedding that balances the classes using contrastive learning as an alternative to the common cross-entropy loss.","Medical image datasets are hard to collect, expensive to label, and often highly imbalanced. The last issue is underestimated, as typical average metrics hardly reveal that the often very important minority classes have a very low accuracy. In this paper, we address this problem by a feature embedding that balances the classes using contrastive learning as an alternative to the common cross-entropy loss.",Neural network,"Accuracy, F1 score",No,Nothing is mentioned,Yes,We evaluate the proposed method on the ISIC2018 lesion diagnosis dataset [7] which consists of 10015 skin lesion images and 7 predefined categories and APTOS2019 [1] for diabetic retinopathy which has 5 classes and 3662 images.,We evaluate the proposed method on the ISIC2018 lesion diagnosis dataset [7] which consists of 10015 skin lesion images and 7 predefined categories and APTOS2019 [1] for diabetic retinopathy which has 5 classes and 3662 images.,Public,Kaggle dataset and challenge dataset,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Department of Computer Science, University of Freiburg, Freiburg im Breisgau, Germany
(2)
CIBSS – Centre for Integrative Biological Signalling Studies, University of Freiburg, Freiburg im Breisgau, Germany

This study was supported by the Excellence Strategy of the German Federal and State Governments, (CIBSS - EXC 2189).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
26/10/2022 10.57.48,174,2021,Yes,It was accurately labelled,Explainable Classification of Weakly Annotated Wireless Capsule Endoscopy Images Based on a Fuzzy Bag-of-Colour Features Model and Brain Storm Optimization,"In this paper, we propose a novel Explainable Fuzzy Bag-of-Words (XFBoW) feature extraction model, for the classification of weakly annotated WCE images. A comparative advantage of the proposed model over state-of-the-art feature extractors is that it can provide an explainable classification outcome, even with conventional classification schemes, such as Support Vector Machines. The explanations that can be derived are based on the similarity of the image content with the content of the training images, used for the construction of the model. The feature extraction process relies on data clustering and fuzzy sets. Clustering is used to encode the image content into visual words. These words are subsequently used for the formation of fuzzy sets to enable a linguistic characterization of similarities with the training images. A state-of-the-art Brain Storm Optimization algorithm is used as an optimizer to define the most appropriate number of visual words and fuzzy sets and also the fittest parameters of the classifier, in order to optimally classify the WCE images. The training of XFBoW is performed using only image-level, semantic labels instead of detailed, pixel-level annotations. The proposed method is investigated on real datasets that include a variety of GI abnormalities. The results show that XFBoW outperforms several state-of-the-art methods, while providing the advantage of explainability.","In this paper, we propose a novel Explainable Fuzzy Bag-of-Words (XFBoW) feature extraction model, for the classification of weakly annotated WCE images. A comparative advantage of the proposed model over state-of-the-art feature extractors is that it can provide an explainable classification outcome, even with conventional classification schemes, such as Support Vector Machines. The explanations that can be derived are based on the similarity of the image content with the content of the training images, used for the construction of the model. The feature extraction process relies on data clustering and fuzzy sets. Clustering is used to encode the image content into visual words. These words are subsequently used for the formation of fuzzy sets to enable a linguistic characterization of similarities with the training images. A state-of-the-art Brain Storm Optimization algorithm is used as an optimizer to define the most appropriate number of visual words and fuzzy sets and also the fittest parameters of the classifier, in order to optimally classify the WCE images. The training of XFBoW is performed using only image-level, semantic labels instead of detailed, pixel-level annotations. The proposed method is investigated on real datasets that include a variety of GI abnormalities. The results show that XFBoW outperforms several state-of-the-art methods, while providing the advantage of explainability.",SVM,"AUC, Specificity, Accuracy, Sensitivity",No,Nothing is mentioned,Yes,"In this study, a subset of “Dataset 2” from the publicly available KID database is used [16]. This dataset is composed of WCE video frames obtained from the whole GI tract using a MiroCam capsule endoscope with a resolution of 360 × 360 pixels. The dataset includes 303 images of vascular, 44 images of polypoid abnormalities, 227 images of most common inflammatory lesions and 1778 normal images obtained from the esophagus, the stomach, the small bowel and the colon.","In this study, a subset of “Dataset 2” from the publicly available KID database is used [16]. This dataset is composed of WCE video frames obtained from the whole GI tract using a MiroCam capsule endoscope with a resolution of 360 × 360 pixels. The dataset includes 303 images of vascular, 44 images of polypoid abnormalities, 227 images of most common inflammatory lesions and 1778 normal images obtained from the esophagus, the stomach, the small bowel and the colon.",Public,"In this study, a subset of “Dataset 2” from the publicly available KID database is used [16]. This dataset is composed of WCE video frames obtained from the whole GI tract using a MiroCam capsule endoscope with a resolution of 360 × 360 pixels. The dataset includes 303 images of vascular, 44 images of polypoid abnormalities, 227 images of most common inflammatory lesions and 1778 normal images obtained from the esophagus, the stomach, the small bowel and the colon.",,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"University of Thessaly, Papasiopoulou str. 2-4, 35131 Lamia, Greece

This work was supported in part by the grant No. 5024 of the Special Account of Research Grants of the University of Thessaly, Greece.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
26/10/2022 11.09.26,180,2021,Yes,It was accurately labelled,"Abstract
Explainability is a key feature for computer-aided diagnosis systems. This property not only helps doctors understand their decisions, but also allows less experienced practitioners to improve their knowledge. Skin cancer diagnosis is a field where explainability is of critical importance, as lesions of different classes often exhibit confounding characteristics. This work proposes a deep neural network (DNN) for skin cancer diagnosis that provides explainability through content-based image retrieval. We explore several state-of-the-art approaches to improve the feature space learned by the DNN, namely contrastive, distillation, and triplet losses. We demonstrate that the combination of these regularization losses with the categorical cross-entropy leads to the best performances on melanoma classification, and results in a hybrid DNN that simultaneously: i) classifies the images; and ii) retrieves similar images justifying the diagnosis. The code is available at https://​github.​com/​catarina-barata/​CBIR_​Explainability_​Skin_​Cancer.","Explainability is a key feature for computer-aided diagnosis systems. This property not only helps doctors understand their decisions, but also allows less experienced practitioners to improve their knowledge. Skin cancer diagnosis is a field where explainability is of critical importance, as lesions of different classes often exhibit confounding characteristics. This work proposes a deep neural network (DNN) for skin cancer diagnosis that provides explainability through content-based image retrieval. We explore several state-of-the-art approaches to improve the feature space learned by the DNN, namely contrastive, distillation, and triplet losses.","Explainability is a key feature for computer-aided diagnosis systems. This property not only helps doctors understand their decisions, but also allows less experienced practitioners to improve their knowledge. Skin cancer diagnosis is a field where explainability is of critical importance, as lesions of different classes often exhibit confounding characteristics. This work proposes a deep neural network (DNN) for skin cancer diagnosis that provides explainability through content-based image retrieval. We explore several state-of-the-art approaches to improve the feature space learned by the DNN, namely contrastive, distillation, and triplet losses.",Neural network,"Precision, Recall",No,Nothing is mentioned,Yes,"ISIC 2018 dermoscopy dataset [19], which contains 10,015 images for training and 1,512 for testing, comprising seven skin lesion classes: nevu (NV), melanoma (MEL), actinic (AKIEC), basal cell carciona (BCC), dermatofibroma (DF), benign keratosis (BKL), and vascular (VASC). ","ISIC 2018 dermoscopy dataset [19], which contains 10,015 images for training and 1,512 for testing, comprising seven skin lesion classes: nevu (NV), melanoma (MEL), actinic (AKIEC), basal cell carciona (BCC), dermatofibroma (DF), benign keratosis (BKL), and vascular (VASC). ",Public,its the ham10000,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Institute for Systems and Robotics, Instituto Superior Técnico, Lisbon, Portugal

This work was supported by the FCT project and multi-year funding [CEECIND/ 00326/2017] and LARSyS - FCT Plurianual funding 2020–2023; and by a Google Research Award’21. The Titan Xp used in this project were donated by the NVIDIA Corporation.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"The code is available at https://​github.​com/​catarina-barata/​CBIR_​Explainability_​Skin_​Cancer.

Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87199-4_​52) contains supplementary material, which is available to authorized users.",
26/10/2022 11.22.59,69,2021,Yes,It was accurately labelled,"Abstract
Nowadays, there is an urgent requirement of self-supervised learning (SSL) on whole slide pathological images (WSIs) to relieve the demand of finely expert annotations. However, the performance of SSL algorithms on WSIs has long lagged behind their supervised counterparts. To close this gap, in this paper, we fully explore the intrinsic characteristics of WSIs and propose SSLP: Spatial Guided Self-supervised Learning on Pathological Images. We argue the patch-wise spatial proximity is a significant characteristic of WSIs, if properly employed, shall provide abundant supervision for free. Specifically, we explore three semantic invariance from 1) self-invariance: the same patch of different augmented views, 2) intra-invariance: the patches within spatial neighbors and 3) inter-invariance: their corresponding neighbors in the feature space. As a result, our SSLP model achieves $$82.9\%$$ accuracy and $$85.7\%$$ AUC on CAMELYON linear classification and $$95.2\%$$ accuracy fine-tuning on cross-disease classification on NCTCRC, which outperforms previous state-of-the-art algorithm and matches the performance of a supervised counterpart.","Abstract
Nowadays, there is an urgent requirement of self-supervised learning (SSL) on whole slide pathological images (WSIs) to relieve the demand of finely expert annotations. However, the performance of SSL algorithms on WSIs has long lagged behind their supervised counterparts. To close this gap, in this paper, we fully explore the intrinsic characteristics of WSIs and propose SSLP: Spatial Guided Self-supervised Learning on Pathological Images. We argue the patch-wise spatial proximity is a significant characteristic of WSIs, if properly employed, shall provide abundant supervision for free. Specifically, we explore three semantic invariance from 1) self-invariance: the same patch of different augmented views, 2) intra-invariance: the patches within spatial neighbors and 3) inter-invariance: their corresponding neighbors in the feature space. As a result, our SSLP model achieves $$82.9\%$$ accuracy and $$85.7\%$$ AUC on CAMELYON linear classification and $$95.2\%$$ accuracy fine-tuning on cross-disease classification on NCTCRC, which outperforms previous state-of-the-art algorithm and matches the performance of a supervised counterpart.","Abstract
Nowadays, there is an urgent requirement of self-supervised learning (SSL) on whole slide pathological images (WSIs) to relieve the demand of finely expert annotations. However, the performance of SSL algorithms on WSIs has long lagged behind their supervised counterparts. To close this gap, in this paper, we fully explore the intrinsic characteristics of WSIs and propose SSLP: Spatial Guided Self-supervised Learning on Pathological Images. We argue the patch-wise spatial proximity is a significant characteristic of WSIs, if properly employed, shall provide abundant supervision for free. Specifically, we explore three semantic invariance from 1) self-invariance: the same patch of different augmented views, 2) intra-invariance: the patches within spatial neighbors and 3) inter-invariance: their corresponding neighbors in the feature space. As a result, our SSLP model achieves $$82.9\%$$ accuracy and $$85.7\%$$ AUC on CAMELYON linear classification and $$95.2\%$$ accuracy fine-tuning on cross-disease classification on NCTCRC, which outperforms previous state-of-the-art algorithm and matches the performance of a supervised counterpart.",self supervised learning,"AUC, Accuracy",No,Nothing is mentioned,Yes,"Datasets. We study SSL algorithms performed in: 1) NCTCRC: dataset contains pathological image patches with $$224\times 224$$ pixels at 0.5 microns per pixel (MPP) from 9 tissue classes, and 2) CAMELYON: dataset contains pathological image patches with $$256\times 256$$ pixels at 0.5 MPP from 2 tissue classes. The details of the datasets are in Appendix A in the supplementary material. All these datasets will be publicly available sooner.","Datasets. We study SSL algorithms performed in: 1) NCTCRC: dataset contains pathological image patches with $$224\times 224$$ pixels at 0.5 microns per pixel (MPP) from 9 tissue classes, and 2) CAMELYON: dataset contains pathological image patches with $$256\times 256$$ pixels at 0.5 MPP from 2 tissue classes. The details of the datasets are in Appendix A in the supplementary material. All these datasets will be publicly available sooner.",Public,"Datasets. We study SSL algorithms performed in: 1) NCTCRC: dataset contains pathological image patches with $$224\times 224$$ pixels at 0.5 microns per pixel (MPP) from 9 tissue classes, and 2) CAMELYON: dataset contains pathological image patches with $$256\times 256$$ pixels at 0.5 MPP from 2 tissue classes. The details of the datasets are in Appendix A in the supplementary material. All these datasets will be publicly available sooner.",No,Nothing is mentioned,Yes,"Maybe at least:
 The details of the datasets are in Appendix A in the supplementary material. All these datasets will be publicly available sooner.",No,Nothing is mentioned,Yes,"Shanghai Jiao Tong University, Shanghai, China
(2)
MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China

This work was supported in part by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), 111 project (BP0719010), Shanghai Science and Technology Committee (18DZ2270700) and Shanghai Jiao Tong University Science and Technology Innovation Special Fund (ZH2018ZDA17).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87196-3_​1) contains supplementary material, which is available to authorized users.",
26/10/2022 11.27.43,72,2021,Yes,It was accurately labelled,"Radiomics can quantify the properties of regions of interest in medical image data. Classically, they account for pre-defined statistics of shape, texture, and other low-level image features. Alternatively, deep learning-based representations are derived from supervised learning but require expensive annotations and often suffer from overfitting and data imbalance issues. In this work, we address the challenge of learning the representation of a 3D medical image for an effective quantification under data imbalance. We propose a self-supervised representation learning framework to learn high-level features of 3D volumes as a complement to existing radiomics features. Specifically, we demonstrate how to learn image representations in a self-supervised fashion using a 3D Siamese network. More importantly, we deal with data imbalance by exploiting two unsupervised strategies: a) sample re-weighting, and b) balancing the composition of training batches. When combining the learned self-supervised feature with traditional radiomics, we show significant improvement in brain tumor classification and lung cancer staging tasks covering MRI and CT imaging modalities. Codes are available in https://​github.​com/​hongweilibran/​imbalanced-SSL.","Radiomics can quantify the properties of regions of interest in medical image data. Classically, they account for pre-defined statistics of shape, texture, and other low-level image features. Alternatively, deep learning-based representations are derived from supervised learning but require expensive annotations and often suffer from overfitting and data imbalance issues. In this work, we address the challenge of learning the representation of a 3D medical image for an effective quantification under data imbalance. We propose a self-supervised representation learning framework to learn high-level features of 3D volumes as a complement to existing radiomics features. Specifically, we demonstrate how to learn image representations in a self-supervised fashion using a 3D Siamese network. More importantly, we deal with data imbalance by exploiting two unsupervised strategies: a) sample re-weighting, and b) balancing the composition of training batches. When combining the learned self-supervised feature with traditional radiomics, we show significant improvement in brain tumor classification and lung cancer staging tasks covering MRI and CT imaging modalities. Codes are available in https://​github.​com/​hongweilibran/​imbalanced-SSL.","Great advances have been achieved in supervised deep learning, reaching expert-level performance on some considerably challenging applications [11]. However, supervised methods for image classification commonly require relatively large-scale datasets with ground-truth labels which is time- and resource-consuming in the medical field. ",self supervised learning,"Specificity, Accuracy, Sensitivity",Yes,"For NSCLC-radiomics, we get the lung mask by a recent public lung segmentation tool ",Yes,"The evaluation of our approach is performed on two public datasets: 1) a multi-center MRI dataset (BraTS) [4, 19] including 326 patients with brain tumor. The MRI modalities include FLAIR, T1, T2 and T1-c with a uniform voxel size 1 $$\times $$ 1 $$\times $$ 1 mm$$^3$$. Only FLAIR is used in our experiment for simplicity of comparisons. 2) a lung CT dataset with 420 non-small cell lung cancer patients (NSCLC-radiomics) [1, 9]1. ","The evaluation of our approach is performed on two public datasets: 1) a multi-center MRI dataset (BraTS) [4, 19] including 326 patients with brain tumor. The MRI modalities include FLAIR, T1, T2 and T1-c with a uniform voxel size 1 $$\times $$ 1 $$\times $$ 1 mm$$^3$$. Only FLAIR is used in our experiment for simplicity of comparisons. 2) a lung CT dataset with 420 non-small cell lung cancer patients (NSCLC-radiomics) [1, 9]1. ",Public,"The evaluation of our approach is performed on two public datasets: 1) a multi-center MRI dataset (BraTS) [4, 19] including 326 patients with brain tumor. The MRI modalities include FLAIR, T1, T2 and T1-c with a uniform voxel size 1 $$\times $$ 1 $$\times $$ 1 mm$$^3$$. Only FLAIR is used in our experiment for simplicity of comparisons. 2) a lung CT dataset with 420 non-small cell lung cancer patients (NSCLC-radiomics) [1, 9]1. ",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Department of Computer Science, Technical University of Munich, Munich, Germany
(2)
Department of Quantitative Biomedicine, University of Zurich, Zürich, Switzerland
(3)
ETH Zurich, Zürich, Switzerland
(4)
Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China
(5)
Faculty of Information Technology, Macau University of Science and Technology, Macao, China
(6)
Klinikum rechts der Isar, Technical University of Munich, Munich, Germany 

This work was supported by Helmut Horten Foundation. I. E. was supported by the TRABIT network under the EU Marie Sklodowska-Curie program (Grant ID: 765148). S. L. was supported by the Faculty Research Grant (NO. FRG-18-020-FI) at Macau University of Science and Technology. B. W. and B. M. were supported through the DFG, SFB-824, subproject B12. K. C. was supported by Clinical Research Priority Program (CRPP) Grant on Artificial Intelligence in Oncological Imaging Network, University of Zurich.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Codes are available in https://​github.​com/​hongweilibran/​imbalanced-SSL.

Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87196-3_​4) contains supplementary material, which is available to authorized users.


We build a 3D convolutional neural network with two bottleneck blocks as the encoder for all experiments (details in Supplementary).",
26/10/2022 11.31.24,73,2021,Yes,It was accurately labelled,"Self-supervised learning provides a possible solution to extract effective visual representations from unlabeled histopathological images. However, existing methods either fail to make good use of domain-specific knowledge, or rely on side information like spatial proximity and magnification. In this paper, we propose CS-CO, a hybrid self-supervised visual representation learning method tailored for histopathological images, which integrates advantages of both generative and discriminative models. The proposed method consists of two self-supervised learning stages: cross-stain prediction (CS) and contrastive learning (CO), both of which are designed based on domain-specific knowledge and do not require side information. A novel data augmentation approach, stain vector perturbation, is specifically proposed to serve contrastive learning. Experimental results on the public dataset NCT-CRC-HE-100K demonstrate the superiority of the proposed method for histopathological image visual representation. Under the common linear evaluation protocol, our method achieves 0.915 eight-class classification accuracy with only 1,000 labeled data, which is about 1.3% higher than the fully-supervised ResNet18 classifier trained with the whole 89,434 labeled training data. Our code is available at https://​github.​com/​easonyang1996/​CS-CO.","Self-supervised learning provides a possible solution to extract effective visual representations from unlabeled histopathological images. However, existing methods either fail to make good use of domain-specific knowledge, or rely on side information like spatial proximity and magnification. In this paper, we propose CS-CO, a hybrid self-supervised visual representation learning method tailored for histopathological images, which integrates advantages of both generative and discriminative models. The proposed method consists of two self-supervised learning stages: cross-stain prediction (CS) and contrastive learning (CO), both of which are designed based on domain-specific knowledge and do not require side information. A novel data augmentation approach, stain vector perturbation, is specifically proposed to serve contrastive learning. Experimental results on the public dataset NCT-CRC-HE-100K demonstrate the superiority of the proposed method for histopathological image visual representation. Under the common linear evaluation protocol, our method achieves 0.915 eight-class classification accuracy with only 1,000 labeled data, which is about 1.3% higher than the fully-supervised ResNet18 classifier trained with the whole 89,434 labeled training data. Our code is available at https://​github.​com/​easonyang1996/​CS-CO.","Self-supervised learning provides a possible solution to extract effective visual representations from unlabeled histopathological images. However, existing methods either fail to make good use of domain-specific knowledge, or rely on side information like spatial proximity and magnification. In this paper, we propose CS-CO, a hybrid self-supervised visual representation learning method tailored for histopathological images, which integrates advantages of both generative and discriminative models. The proposed method consists of two self-supervised learning stages: cross-stain prediction (CS) and contrastive learning (CO), both of which are designed based on domain-specific knowledge and do not require side information. A novel data augmentation approach, stain vector perturbation, is specifically proposed to serve contrastive learning. Experimental results on the public dataset NCT-CRC-HE-100K demonstrate the superiority of the proposed method for histopathological image visual representation. Under the common linear evaluation protocol, our method achieves 0.915 eight-class classification accuracy with only 1,000 labeled data, which is about 1.3% higher than the fully-supervised ResNet18 classifier trained with the whole 89,434 labeled training data. Our code is available at https://​github.​com/​easonyang1996/​CS-CO.",self supervised learning,Accuracy,No,Nothing is mentioned,Yes,"We evaluate our proposed CS-CO method on the public dataset NCT-CRC-HE-100K [17]. The dataset contains nine classes of histopathological images of human colorectal cancer and healthy tissue. The predefined training set contains 100,000 images and the test set contains 7180 images.","We evaluate our proposed CS-CO method on the public dataset NCT-CRC-HE-100K [17]. The dataset contains nine classes of histopathological images of human colorectal cancer and healthy tissue. The predefined training set contains 100,000 images and the test set contains 7180 images.",Public,"We evaluate our proposed CS-CO method on the public dataset NCT-CRC-HE-100K [17]. The dataset contains nine classes of histopathological images of human colorectal cancer and healthy tissue. The predefined training set contains 100,000 images and the test set contains 7180 images.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"
Ministry of Education Key Laboratory of Bioinformatics, Bioinformatics Division, Beijing National Research Center for Information Science and Technology, Department of Automation, Tsinghua University, Beijing, 100084, China
(2)
Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, China
(3)
Department of Hepatobiliary and Pancreatic Surgery, The Affiliated Hospital of Qingdao University, Qingdao, 266000, Shandong, China

This work was partially supported by the National Key Research and Development Program of China (No. 2018YFC0910404), the National Natural Science Foundation of China (Nos. 61873141, 61721003), the Shanghai Municipal Science and Technology Major Project (No. 2017SHZDZX01), the Tsinghua-Fuzhou Institute for Data Technology, the Taishan Scholars Program of Shandong Province (No. 2019010668), and the Shandong Higher Education Young Science and Technology Support Program (No. 2020KJL005).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,Our code is available at https://​github.​com/​easonyang1996/​CS-CO.,
27/10/2022 10.28.48,78,2021,Yes,It was accurately labelled,"Abstract
Contrastive learning is a key technique of modern self-supervised learning. The broader accessibility of earlier approaches is hindered by the need of heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which accommodate for large-scale negative samples or momentum. The more recent SimSiam approach addresses such key limitations via stop-gradient without momentum encoders. In medical image analysis, multiple instances can be achieved from the same patient or tissue. Inspired by these advances, we propose a simple triplet representation learning (SimTriplet) approach on pathological images. The contribution of the paper is three-fold: (1) The proposed SimTriplet method takes advantage of the multi-view nature of medical images beyond self-augmentation; (2) The method maximizes both intra-sample and inter-sample similarities via triplets from positive pairs, without using negative samples; and (3) The recent mix precision training is employed to advance the training by only using a single GPU with 16 GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58% better performance compared with supervised learning. It also achieved 2.13% better performance compared with SimSiam. Our proposed SimTriplet can achieve decent performance using only 1% labeled data. The code and data are available at https://​github.​com/​hrlblab/​SimTriplet.","Abstract
Contrastive learning is a key technique of modern self-supervised learning. The broader accessibility of earlier approaches is hindered by the need of heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which accommodate for large-scale negative samples or momentum. The more recent SimSiam approach addresses such key limitations via stop-gradient without momentum encoders. In medical image analysis, multiple instances can be achieved from the same patient or tissue. Inspired by these advances, we propose a simple triplet representation learning (SimTriplet) approach on pathological images. The contribution of the paper is three-fold: (1) The proposed SimTriplet method takes advantage of the multi-view nature of medical images beyond self-augmentation; (2) The method maximizes both intra-sample and inter-sample similarities via triplets from positive pairs, without using negative samples; and (3) The recent mix precision training is employed to advance the training by only using a single GPU with 16 GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58% better performance compared with supervised learning. It also achieved 2.13% better performance compared with SimSiam. Our proposed SimTriplet can achieve decent performance using only 1% labeled data. The code and data are available at https://​github.​com/​hrlblab/​SimTriplet.","Abstract
Contrastive learning is a key technique of modern self-supervised learning. The broader accessibility of earlier approaches is hindered by the need of heavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which accommodate for large-scale negative samples or momentum. The more recent SimSiam approach addresses such key limitations via stop-gradient without momentum encoders. In medical image analysis, multiple instances can be achieved from the same patient or tissue. Inspired by these advances, we propose a simple triplet representation learning (SimTriplet) approach on pathological images. The contribution of the paper is three-fold: (1) The proposed SimTriplet method takes advantage of the multi-view nature of medical images beyond self-augmentation; (2) The method maximizes both intra-sample and inter-sample similarities via triplets from positive pairs, without using negative samples; and (3) The recent mix precision training is employed to advance the training by only using a single GPU with 16 GB memory. By learning from 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58% better performance compared with supervised learning. It also achieved 2.13% better performance compared with SimSiam. Our proposed SimTriplet can achieve decent performance using only 1% labeled data. The code and data are available at https://​github.​com/​hrlblab/​SimTriplet.

We extracted image patches from seven melanoma skin cancer ","Neural network, self supervised learning","Accuracy, F1 score",No,Nothing is mentioned,No," We extracted image patches from seven melanoma skin cancer Whole Slide Images (WSIs) from the Cancer Genome Atlas (TCGA) Datasets (TCGA Research Network: https://​www.​cancer.​gov/​tcga). From the seven annotated WSIs, 4698 images from 5 WSIs were obtained for training and validation, while 1,921 images from 2 WSIs were used for testing
Unlabeled Data. Beyond the 7 annotated WSIs, additional 79 WSIs without annotations were used for training contrastive learning models. The 79 WSIs were all available and usable melanoma cases from TCGA. The number and size of image patches used for different contrastive learning strategies are described in $$\mathsection $$Experiment."," We extracted image patches from seven melanoma skin cancer Whole Slide Images (WSIs) from the Cancer Genome Atlas (TCGA) Datasets (TCGA Research Network: https://​www.​cancer.​gov/​tcga). From the seven annotated WSIs, 4698 images from 5 WSIs were obtained for training and validation, while 1,921 images from 2 WSIs were used for testing
Unlabeled Data. Beyond the 7 annotated WSIs, additional 79 WSIs without annotations were used for training contrastive learning models. The 79 WSIs were all available and usable melanoma cases from TCGA. The number and size of image patches used for different contrastive learning strategies are described in $$\mathsection $$Experiment.",Public, We extracted image patches from seven melanoma skin cancer Whole Slide Images (WSIs) from the Cancer Genome Atlas (TCGA) Datasets (TCGA Research Network: https://​www.​cancer.​gov/​tcga). ,No,Nothing is mentioned,No,Nothing is mentioned,Yes,For this article (though plucked from an existing dataset,Yes,"

Vanderbilt University, Nashville, TN 37215, USA
(2)
Vanderbilt University Medical Center, Nashville, TN 37215, USA

Dr. Wheless is funded by grants from the Skin Cancer Foundation and the Dermatology Foundation.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,The code and data are available at https://​github.​com/​hrlblab/​SimTriplet.,
27/10/2022 10.44.50,92,2021,Yes,It was accurately labelled,"Tumor classification is important for decision support of precision medicine. Computer-aided diagnosis by convolutional neural networks relies on a large amount of annotated dataset, which is costly sometimes. To solve the poor predictive ability caused by tumor heterogeneity and inadequate labeled image data, a self-supervised learning method combined with radiomics is proposed to learn rich visual representation about tumors without human supervision. A self-supervised pretext task, namely “Radiomics-Deep Feature Correspondence”, is formulated to maximize agreement between radiomics view and deep learning view of the same sample in the latent space. The presented self-supervised model is evaluated on two public medical image datasets of thyroid nodule and kidney tumor and achieves high score on linear evaluations. Furthermore, fine-tuning the pre-trained network leads to a better score than the train-from-scratch models on the tumor classification task and shows label-efficient performance using small training datasets. This shows injecting radiomics prior knowledge about tumors into the representation space can build a more powerful self-supervised method.","Tumor classification is important for decision support of precision medicine. Computer-aided diagnosis by convolutional neural networks relies on a large amount of annotated dataset, which is costly sometimes. To solve the poor predictive ability caused by tumor heterogeneity and inadequate labeled image data, a self-supervised learning method combined with radiomics is proposed to learn rich visual representation about tumors without human supervision. A self-supervised pretext task, namely “Radiomics-Deep Feature Correspondence”, is formulated to maximize agreement between radiomics view and deep learning view of the same sample in the latent space. The presented self-supervised model is evaluated on two public medical image datasets of thyroid nodule and kidney tumor and achieves high score on linear evaluations. Furthermore, fine-tuning the pre-trained network leads to a better score than the train-from-scratch models on the tumor classification task and shows label-efficient performance using small training datasets. This shows injecting radiomics prior knowledge about tumors into the representation space can build a more powerful self-supervised method.","Deep convolutional neural networks (CNNs) have made major breakthroughs in the past few years, largely driven by increased computing power and massive labeled datasets. Benefitting from the huge advances of deep learning in image classification, computer-aided medical diagnostics has achieved great success [3]. Precise prediction of tumor type can help doctors recognize and interpret the subtle difference between different kinds of medical images. Moreover, it is critical to decision support of personalized cancer treatment for patients.","Neural network, self supervised learning","Accuracy, F1 score",No,"Nothing is mentioned, other than one of two datasets already being segmented, but as nothing is mentioned about segmenting the other, must not be necessary",Yes,"Thyroid Nodule Classification in Ultrasound Images. The challenge of Thyroid Nodule Segmentation and Classification in Ultrasound Images (TN-SCUI2020) [14] provide a public 2D dataset of thyroid nodule with over 3,644 patient cases from different ages, genders, and were collected in different sites using various ultrasound machines (e.g. Mindray DC-8, Philips-cx50, TOSHIBA Aplio300). Each ultrasound image is provided with its annotated class (benign or malignant) and a detailed delineation of the nodule. For pre-processing, we crop nodule areas from images and resize the areas to $$196\times 160$$ in dimension. The dataset is randomly split to a train set (2,916 cases) and a test set (728 cases) by the ratio of 80 : 20, while preserving the percentage of samples for each class. In self-supervised learning, the train set is used for the pretext task.

Multiclass Kidney Tumor Classification in CT. The challenge of 2019 Kidney and Kidney Tumor Segmentation (KiTS19) [22] released 210 3D abdominal CT images with kidney tumor subtypes and segmentations of kidney and kidney tumor. These CT images are from more than 50 institutions and scaned with different CT scanners and acquisition protocols.","Thyroid Nodule Classification in Ultrasound Images. The challenge of Thyroid Nodule Segmentation and Classification in Ultrasound Images (TN-SCUI2020) [14] provide a public 2D dataset of thyroid nodule with over 3,644 patient cases from different ages, genders, and were collected in different sites using various ultrasound machines (e.g. Mindray DC-8, Philips-cx50, TOSHIBA Aplio300). Each ultrasound image is provided with its annotated class (benign or malignant) and a detailed delineation of the nodule. For pre-processing, we crop nodule areas from images and resize the areas to $$196\times 160$$ in dimension. The dataset is randomly split to a train set (2,916 cases) and a test set (728 cases) by the ratio of 80 : 20, while preserving the percentage of samples for each class. In self-supervised learning, the train set is used for the pretext task.

Multiclass Kidney Tumor Classification in CT. The challenge of 2019 Kidney and Kidney Tumor Segmentation (KiTS19) [22] released 210 3D abdominal CT images with kidney tumor subtypes and segmentations of kidney and kidney tumor. These CT images are from more than 50 institutions and scaned with different CT scanners and acquisition protocols.",Public,"Thyroid Nodule Classification in Ultrasound Images. The challenge of Thyroid Nodule Segmentation and Classification in Ultrasound Images (TN-SCUI2020) [14] provide a public 2D dataset of thyroid nodule with over 3,644 patient cases from different ages, genders, and were collected in different sites using various ultrasound machines (e.g. Mindray DC-8, Philips-cx50, TOSHIBA Aplio300). Each ultrasound image is provided with its annotated class (benign or malignant) and a detailed delineation of the nodule. For pre-processing, we crop nodule areas from images and resize the areas to $$196\times 160$$ in dimension. The dataset is randomly split to a train set (2,916 cases) and a test set (728 cases) by the ratio of 80 : 20, while preserving the percentage of samples for each class. In self-supervised learning, the train set is used for the pretext task.

Multiclass Kidney Tumor Classification in CT. The challenge of 2019 Kidney and Kidney Tumor Segmentation (KiTS19) [22] released 210 3D abdominal CT images with kidney tumor subtypes and segmentations of kidney and kidney tumor. These CT images are from more than 50 institutions and scaned with different CT scanners and acquisition protocols.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"LIST, Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, Nanjing, China
(2)
Centre de Recherche en Information Biomédicale Sino-Français (CRIBs), Rennes, France

This research was supported by National Natural Science Foundation under grants (31571001, 61828101). We thank the Big Data Center of Southeast University for providing the GPUs to support the numerical calculations in this paper.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
27/10/2022 10.49.16,94,2021,Yes,It was accurately labelled,"Abstract
We propose and demonstrate a representation learning approach by maximizing the mutual information between local features of images and text. The goal of this approach is to learn useful image representations by taking advantage of the rich information contained in the free text that describes the findings in the image. Our method trains image and text encoders by encouraging the resulting representations to exhibit high local mutual information. We make use of recent advances in mutual information estimation with neural network discriminators. We argue that the sum of local mutual information is typically a lower bound on the global mutual information. Our experimental results in the downstream image classification tasks demonstrate the advantages of using local features for image-text representation learning.","Abstract
We propose and demonstrate a representation learning approach by maximizing the mutual information between local features of images and text. The goal of this approach is to learn useful image representations by taking advantage of the rich information contained in the free text that describes the findings in the image. Our method trains image and text encoders by encouraging the resulting representations to exhibit high local mutual information. We make use of recent advances in mutual information estimation with neural network discriminators. We argue that the sum of local mutual information is typically a lower bound on the global mutual information. Our experimental results in the downstream image classification tasks demonstrate the advantages of using local features for image-text representation learning.","We present a novel approach for image-text representation learning by maximizing the mutual information between local features of the images and the text. In the context of medical imaging, the images could be, for example, radiographs and the text could be radiology reports that capture radiologists’ impressions of the images. A large number of such image-text pairs are generated in the clinical workflow every day [7, 13]. Jointly learning from images and raw text can support a leap in the quality of medical vision models by taking advantage of existing expert descriptions of the images.","Neural network, self supervised learning",AUC,No,Nothing is mentioned,Yes, We demonstrate our approach on the MIMIC-CXR dataset v2.0 [13] that includes around 250K frontal-view chest radiographs with their associated radiology reports. , We demonstrate our approach on the MIMIC-CXR dataset v2.0 [13] that includes around 250K frontal-view chest radiographs with their associated radiology reports. ,Public,Looking at the reference,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"(1)
CSAIL, Massachusetts Institute of Technology, Cambridge, MA, USA
(2)
MIT Lincoln Laboratory, Lexington, MA, USA
(3)
Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, USA
(4)
Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA

This work was supported in part by NIH NIBIB NAC P41EB015902, Wistron, IBM Watson, MIT Deshpande Center, MIT J-Clinic, MIT Lincoln Lab, and US Air Force.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
27/10/2022 10.57.15,102,2021,Yes,It was accurately labelled,"Abstract
Given a population longitudinal neuroimaging measurements defined on a brain network, exploiting temporal dependencies within the sequence of data and corresponding latent variables defined on the graph (i.e., network encoding relationships between regions of interest (ROI)) can highly benefit characterizing the brain. Here, it is important to distinguish time-variant (e.g., longitudinal measures) and time-invariant (e.g., gender) components to analyze them individually. For this, we propose an innovative and ground-breaking Disentangled Sequential Graph Autoencoder which leverages the Sequential Variational Autoencoder (SVAE), graph convolution and semi-supervising framework together to learn a latent space composed of time-variant and time-invariant latent variables to characterize disentangled representation of the measurements over the entire ROIs. Incorporating target information in the decoder with a supervised loss let us achieve more effective representation learning towards improved classification. We validate our proposed method on the longitudinal cortical thickness data from Alzheimer’s Disease Neuroimaging Initiative (ADNI) study. Our method outperforms baselines with traditional techniques demonstrating benefits for effective longitudinal data representation for predicting labels and longitudinal data generation.","Abstract
Given a population longitudinal neuroimaging measurements defined on a brain network, exploiting temporal dependencies within the sequence of data and corresponding latent variables defined on the graph (i.e., network encoding relationships between regions of interest (ROI)) can highly benefit characterizing the brain. Here, it is important to distinguish time-variant (e.g., longitudinal measures) and time-invariant (e.g., gender) components to analyze them individually. For this, we propose an innovative and ground-breaking Disentangled Sequential Graph Autoencoder which leverages the Sequential Variational Autoencoder (SVAE), graph convolution and semi-supervising framework together to learn a latent space composed of time-variant and time-invariant latent variables to characterize disentangled representation of the measurements over the entire ROIs. Incorporating target information in the decoder with a supervised loss let us achieve more effective representation learning towards improved classification. We validate our proposed method on the longitudinal cortical thickness data from Alzheimer’s Disease Neuroimaging Initiative (ADNI) study. Our method outperforms baselines with traditional techniques demonstrating benefits for effective longitudinal data representation for predicting labels and longitudinal data generation.","Abstract
Given a population longitudinal neuroimaging measurements defined on a brain network, exploiting temporal dependencies within the sequence of data and corresponding latent variables defined on the graph (i.e., network encoding relationships between regions of interest (ROI)) can highly benefit characterizing the brain. Here, it is important to distinguish time-variant (e.g., longitudinal measures) and time-invariant (e.g., gender) components to analyze them individually. For this, we propose an innovative and ground-breaking Disentangled Sequential Graph Autoencoder which leverages the Sequential Variational Autoencoder (SVAE), graph convolution and semi-supervising framework together to learn a latent space composed of time-variant and time-invariant latent variables to characterize disentangled representation of the measurements over the entire ROIs. Incorporating target information in the decoder with a supervised loss let us achieve more effective representation learning towards improved classification. We validate our proposed method on the longitudinal cortical thickness data from Alzheimer’s Disease Neuroimaging Initiative (ADNI) study. Our method outperforms baselines with traditional techniques demonstrating benefits for effective longitudinal data representation for predicting labels and longitudinal data generation.",Graph analysis,"Accuracy, Precision, Recall, rmse",No,Nothing is mentioned,Yes,"We conducted experiments on structural brain connectivity from DTI in ADNI. DTI images were processed by tractography, which extracted neuron fiber tracts and longitudinal cortical thickness measures registered at Destrieux atlas [8] with 148 ROIs. The dataset had five labels; we merged control (CN), Significant Memory Concern (SMC) and Early Mild Cognitive Impairment (EMCI) groups as Pre-clinical AD group, and Late Mild Cognitive Impairment (LMCI) and Alzheimer’s Disease (AD) as Prodromal AD group to ensure sufficient sample size. The dataset included N = 140 subjects with the Pre-AD group (93 subjects/330 records) and the Pro-AD group (47 subjects/170 records). The mean (std) of ages and sex ratio (Male:Famale) in Pre-AD group and Pro-AD group are 74.02(6.72)/(185:145) and 74.87(6.92)/(95:75), respectively","We conducted experiments on structural brain connectivity from DTI in ADNI. DTI images were processed by tractography, which extracted neuron fiber tracts and longitudinal cortical thickness measures registered at Destrieux atlas [8] with 148 ROIs. The dataset had five labels; we merged control (CN), Significant Memory Concern (SMC) and Early Mild Cognitive Impairment (EMCI) groups as Pre-clinical AD group, and Late Mild Cognitive Impairment (LMCI) and Alzheimer’s Disease (AD) as Prodromal AD group to ensure sufficient sample size. The dataset included N = 140 subjects with the Pre-AD group (93 subjects/330 records) and the Pro-AD group (47 subjects/170 records). The mean (std) of ages and sex ratio (Male:Famale) in Pre-AD group and Pro-AD group are 74.02(6.72)/(185:145) and 74.87(6.92)/(95:75), respectively",Public,ADNI,No,Nothing is mentioned,Yes,"age and sex:
The dataset included N = 140 subjects with the Pre-AD group (93 subjects/330 records) and the Pro-AD group (47 subjects/170 records). The mean (std) of ages and sex ratio (Male:Famale) in Pre-AD group and Pro-AD group are 74.02(6.72)/(185:145) and 74.87(6.92)/(95:75), respectively",No,nothing is mentioned,Yes,"University of Texas at Arlington, Arlington, USA
(2)
Lawrence Berkeley National Laboratory, Berkeley, USA
(3)
Pohang University of Science and Technology, Pohang, South Korea
(4)
University of North Carolina, Chapel Hill, Chapel Hill, USA

This work was supported by GAANN Doctoral Fellowships in Computer Science and Engineering at UTA sponsored by the U.S. Department of Education, NSF IIS CRII 1948510, NIH RF1 AG059312, NIH R03 AG070701, and IITP-2019-0-01906 funded by MSIT (AI Graduate School Program at POSTECH).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
20/10/2022 09.14.36,308,2021,Yes,It was accurately labelled,Image anomaly detection methods that learn normal appearance from only healthy data have shown promising results recently. We propose an alternative to image reconstruction-based and image embedding-based methods and propose a new self-supervised method to tackle pathological anomaly detection.,Image anomaly detection methods that learn normal appearance from only healthy data have shown promising results recently. We propose an alternative to image reconstruction-based and image embedding-based methods and propose a new self-supervised method to tackle pathological anomaly detection.,"Doctors such as radiologists and cardiologists, along with allied imaging specialists such as sonographers shoulder the heavy responsibility of making complex diagnoses. Their decisions often determine patient treatment. Unfortunately, diagnostic errors lead to death or disability almost twice as often as any other medical error

However, detecting arbitrary irregularities, without having any predefined target classes, remains an unsolved problem.",Neural network,Precision,No,Nothing is mentioned,No,"Data: Our first dataset is ChestX-ray14 [27], a public chest X-ray dataset with 108,948 images from 32,717 patients showing 14 pathological classes as well as a normal class. From this large dataset, we extract 43,322 posteroanterior (PA) views of adult patients (over 18) and split them into male (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figa_HTML.gif ) and female (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figb_HTML.gif ) partitions. All X-ray images are resized to $$256 \times 256$$ (down from $$1024 \times 1024$$) and normalized to have zero mean and unit standard deviation. The training/test split is summarized in Table 1.

The second dataset consists of a total of 13380 frames from 108 patients acquired during routine fetal ultrasound screening. This is an application where automated anomaly detection in screening services would be of most use. We use cardiac standard view planes [8], specifically 4-chamber heart (4CH) and 3-vessel and trachea (3VT) views, from a private and de-identified dataset of ultrasound videos. For each selected standard plane, 20 consecutive frames (10 before and 9 after) are extracted from the ultrasound videos. Images are $$224 \times 288$$ and are normalized to zero mean, unit standard deviation. Normal samples consist of healthy images from a single view (4CH/3VT) and anomalous images are composed of alternate views (3VT/4CH) as well as pathological hearts of the same view (4CH/3VT). For pathology we use cases of hypoplastic left heart syndrome (HLHS), a condition that affects the development of the left side of the heart [23]. The training/test split is outlined in Table 1. The scans are of volunteers at 18–24 weeks gestation (Ethics: anonymous during review), in a fetal cardiology clinic, where patients are referred to from primary screening and secondary care sites.","Data: Our first dataset is ChestX-ray14 [27], a public chest X-ray dataset with 108,948 images from 32,717 patients showing 14 pathological classes as well as a normal class. From this large dataset, we extract 43,322 posteroanterior (PA) views of adult patients (over 18) and split them into male (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figa_HTML.gif ) and female (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figb_HTML.gif ) partitions. All X-ray images are resized to $$256 \times 256$$ (down from $$1024 \times 1024$$) and normalized to have zero mean and unit standard deviation. The training/test split is summarized in Table 1.

The second dataset consists of a total of 13380 frames from 108 patients acquired during routine fetal ultrasound screening. This is an application where automated anomaly detection in screening services would be of most use. We use cardiac standard view planes [8], specifically 4-chamber heart (4CH) and 3-vessel and trachea (3VT) views, from a private and de-identified dataset of ultrasound videos. For each selected standard plane, 20 consecutive frames (10 before and 9 after) are extracted from the ultrasound videos. Images are $$224 \times 288$$ and are normalized to zero mean, unit standard deviation. Normal samples consist of healthy images from a single view (4CH/3VT) and anomalous images are composed of alternate views (3VT/4CH) as well as pathological hearts of the same view (4CH/3VT). For pathology we use cases of hypoplastic left heart syndrome (HLHS), a condition that affects the development of the left side of the heart [23]. The training/test split is outlined in Table 1. The scans are of volunteers at 18–24 weeks gestation (Ethics: anonymous during review), in a fetal cardiology clinic, where patients are referred to from primary screening and secondary care sites.","Public, Private","Data: Our first dataset is ChestX-ray14 [27], a public chest X-ray dataset with 108,948 images from 32,717 patients showing 14 pathological classes as well as a normal class. From this large dataset, we extract 43,322 posteroanterior (PA) views of adult patients (over 18) and split them into male (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figa_HTML.gif ) and female (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figb_HTML.gif ) partitions. All X-ray images are resized to $$256 \times 256$$ (down from $$1024 \times 1024$$) and normalized to have zero mean and unit standard deviation. The training/test split is summarized in Table 1.

The second dataset consists of a total of 13380 frames from 108 patients acquired during routine fetal ultrasound screening. This is an application where automated anomaly detection in screening services would be of most use. We use cardiac standard view planes [8], specifically 4-chamber heart (4CH) and 3-vessel and trachea (3VT) views, from a private and de-identified dataset of ultrasound videos. For each selected standard plane, 20 consecutive frames (10 before and 9 after) are extracted from the ultrasound videos. Images are $$224 \times 288$$ and are normalized to zero mean, unit standard deviation. Normal samples consist of healthy images from a single view (4CH/3VT) and anomalous images are composed of alternate views (3VT/4CH) as well as pathological hearts of the same view (4CH/3VT). For pathology we use cases of hypoplastic left heart syndrome (HLHS), a condition that affects the development of the left side of the heart [23]. The training/test split is outlined in Table 1. The scans are of volunteers at 18–24 weeks gestation (Ethics: anonymous during review), in a fetal cardiology clinic, where patients are referred to from primary screening and secondary care sites.",No,Nothing is mentioned,Yes,"Our first dataset is ChestX-ray14 [27], a public chest X-ray dataset with 108,948 images from 32,717 patients showing 14 pathological classes as well as a normal class. From this large dataset, we extract 43,322 posteroanterior (PA) views of adult patients (over 18) and split them into male (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figa_HTML.gif ) and female (  ../images/521400_1_En_56_Chapter/521400_1_En_56_Figb_HTML.gif ) partitions

For the second: The scans are of volunteers at 18–24 weeks gestation (Ethics: anonymous during review), in a fetal cardiology clinic, where patients are referred to from primary screening and secondary care sites",No,Nothing is mentioned,Yes,"(1)
Imperial College London, SW7 2AZ London, UK
(2)
King’s College London, St Thomas’ Hospital, SE1 7EH London, UK
(3)
Friedrich–Alexander University Erlangen–Nürnberg, Erlangen, Germany

Support from Wellcome Trust IEH Award iFind project [102431] and UK Research and Innovation London Medical Imaging and Artificial Intelligence Centre for Value Based Healthcare. JT was supported by the ICL President’s Scholarship.",Yes," The scans are of volunteers at 18–24 weeks gestation (Ethics: anonymous during review), in a fetal cardiology clinic, where patients are referred to from primary screening and secondary care sites 
(so only for the second dataset used)",No,nothing is mentioned,No,nothing is mentioned,Yes,Code available at https://​github.​com/​jemtan/​PII.,
11/10/2022 09.08.14,297,2021,Yes,It was accurately labelled,Data Augmentation in Logit Space for Medical Image Classification with Limited Training Data,"a novel data augmentation method is proposed to effectively alleviate the over-fitting issue, not in the input space but in the logit space. ","Successful application of deep learning often depends on large amount of training data. However in practical medical image analysis, available training data are often limited, often causing over-fitting during model training. In this paper, a novel data augmentation method is proposed to effectively alleviate the over-fitting issue, not in the input space but in the logit space. ",Neural network,Accuracy,No,Nothing is mentioned (and segmentation doesn't seem to be used with neural networks?),Yes,"The proposed method was extensively evaluated on three medical image classification datasets, Skin40, Skin8, and Xray6.",Mixed sizes,"Public, Private","At least one is public, think the two others are private",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"The final dataset is created for this article, the others no:
Xray6 is a subset of ChestXray14 dataset [23], containing six diseases of X-ray images (Atelectasis, Cardiomegaly, Emphysema, Hernia, Mass, Effusion). Based on the smallest class (i.e., Hernia) which has only 110 images, the same number of images were randomly sampled from every other class, forming the small-sample Xray6 dataset. ",Yes,"(1)
School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China
(2)
Key Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, China
(3)
Pazhou Lab, Guangzhou, China

This work is supported by the National Natural Science Foundation of China (No. 62071502, U1811461), the Guangdong Key Research and Development Program (No. 2020B1111190001, 2019B020228001), and the Meizhou Science and Technology Program (No. 2019A0102005).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Really hard to find the performance measure!
29/09/2022 09.20.10,266,2021,Yes,It was accurately labelled,"Unsupervised anomaly detection (UAD) learns one-class classifiers exclusively with normal (i.e., healthy) images to detect any abnormal (i.e., unhealthy) samples that do not conform to the expected normal patterns","we propose a novel self-supervised representation learning method, called Constrained Contrastive Distribution learning for anomaly detection (CCD), which learns fine-grained feature representations by simultaneously predicting the distribution of augmented data and image contexts using contrastive learning with pretext constraints. ","UAD has two main advantages over its fully supervised counterpart. Firstly, it is able to directly leverage large datasets available from health screening programs that contain mostly normal image samples, avoiding the costly manual labelling of abnormal samples and the subsequent issues involved in training with extremely class-imbalanced data. Further, UAD approaches can potentially detect and localise any type of lesions that deviate from the normal patterns. One significant challenge faced by UAD methods is how to learn effective low-dimensional image representations to detect and localise subtle abnormalities, generally consisting of small lesions. ",Constrained Contrastive Distribution learning,AUC,No,Nothing is mentioned,Yes,"They use three different datasets: Hyper-Kvasir, LAG, and Liu et al's colonoscopy dataset",Different sizes,"Public, Private","But mixed, some public, some private",No,None of the used datasets are described in great detail,No,Nothing is mentioned,No,Nothing is mentioned,No,"Medical Imaging, Robotics, Analytic Computing Laboratory/Engineering (MIRACLE), School of Biomedical Engineering & Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China
(2)
Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China
(3)
School of Electronic, Electrical and Communication Engineering, University of the Chinese Academy of Science, Beijing, China
(4)
Healthcare Intelligence, AIC, DAMO Academy, Alibaba Group, Hangzhou, China  Nothing is mentioned",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Their code is freely available on github, link in abstract",
27/09/2022 11.21.06,257,2021,Yes,It was accurately labelled,"Learning disease-related representations plays a critical role in image-based cancer diagnosis, due to its trustworthy, interpretable and good generalization power.","Learning disease-related representations plays a critical role in image-based cancer diagnosis, due to its trustworthy, interpretable and good generalization power. A good representation should not only be disentangled from the disease-irrelevant features, but also incorporate the information of lesion’s attributes (e.g., shape, margin) that are often identified first during cancer diagnosis clinically. To learn such a representation, we propose a Disentangle Auto-Encoder with Graph Convolutional Network (DAE-GCN), which adopts a disentangling mechanism with the guidance of a GCN model in the AE-based framework. ","For better representation learning, the disentanglement mechanism has been proved to be an effective way [1, 3, 12], since such a mechanism prompts different independent latent units to encode different independent ground truth generation factors that vary in the data [1]. Based on the above, to capture the disease-related features without mixing other irrelevant information, in this paper we propose a Disentangle Auto-Encoder with Graph Convolutional Network (DAE-GCN), which incorporates a disentangling mechanism into an AE framework, equipped with attribution data during training stage (the attributes are not provided during the test). ","Graph analysis, Supervised learning, Neural network","AUC, Accuracy",No,"Our dataset contains $$\{x_i,A_i,y_i\}_{i \in \{1,...,n\}}$$, in which x, A, y respectively denote the patch-level mass image, attributes (e.g., circumscribed-margin, round-shape, irregular-shape), and the binary disease label. ",Yes," DDSM 
But also uses 3 ""in house"" datasets","We consider both the public dataset DDSM [2] and three in-house datasets: Inhouse1, Inhouse2 and Inhouse3. For each dataset, the region of interests (ROIs) (malignant/benign masses) are cropped based on the annotations of radiologists the same as [9]1. For all datasets, we randomly2 divide the whole set into training, validation and testing as 8:1:1 in patient-wise. ","Public, Private","We consider both the public dataset DDSM [2] and three in-house datasets: Inhouse1, Inhouse2 and Inhouse3.",No,"To evaluate the effectiveness of our DAE-GCN, we verify it on the patch-level mammogram mass benign/malignant classification. We consider both the public dataset DDSM [2] and three in-house datasets: Inhouse1, Inhouse2 and Inhouse3. For each dataset, the region of interests (ROIs) (malignant/benign masses) are cropped based on the annotations of radiologists the same as [9]1. For all datasets, we randomly2 divide the whole set into training, validation and testing as 8:1:1 in patient-wise.",No,Nothing is mentioend,No,Nothing is mentioned,Yes,"(1)
Center for Data Science, Peking University, Beijing, China
(2)
Peking University, Beijing, China
(3)
Department of Computer Science and Technology, Peking University, Beijing, China
(4)
Deepwise AI Lab, Beijing, China
(5)
The University of Hong Kong, Pokfulam, Hong Kong   This work was supported by MOST-2018AAA0102004, NSFC-61625201 and ZheJiang Province Key Research & Development Program (No. 2020C03073).",No,Nothing is mentioned,Yes,"From the introduction: 
For image-based disease benign/malignant diagnosis, it is crucial to learn the disease-related representation for prediction, due to the necessity of trustworthy (to patients), explainable (to clinicians) and good generalization ability in healthcare.",No,Nothing is mentioned,Yes,"To provide convenience for latter works, we publish our spitted test set of DDSM [2] in supplementary.",
20/10/2022 09.04.51,307,2021,No,Other medical imaging task,"Modeling 3D context is essential for high-performance 3D medical image analysis. Although 2D networks benefit from large-scale 2D supervised pretraining, it is weak in capturing 3D context. 3D networks are strong in 3D context yet lack supervised pretraining. As an emerging technique, 3D context fusion operator, which enables conversion from 2D pretrained networks, leverages the advantages of both and has achieved great success. Existing 3D context fusion operators are designed to be spatially symmetric, i.e., performing identical operations on each 2D slice like convolutions. However, these operators are not truly equivariant to translation, especially when only a few 3D slices are used as inputs. In this paper, we propose a novel asymmetric 3D context fusion operator (A3D), which uses different weights to fuse 3D context from different 2D slices. Notably, A3D is NOT translation-equivariant while it significantly outperforms existing symmetric context fusion operators without introducing large computational overhead. We validate the effectiveness of the proposed method by extensive experiments on DeepLesion benchmark, a large-scale public dataset for universal lesion detection from computed tomography (CT). The proposed A3D consistently outperforms symmetric context fusion operators by considerable margins, and establishes a new state of the art on DeepLesion. To facilitate open research, our code and model in PyTorch is available at https://​github.​com/​M3DV/​AlignShift.",,,,,,,,,,,,,,,,,,,,,,,,,,,,Universal lesion detection
11/10/2022 08.55.30,292,2021,No,Other medical imaging task,Vessel centerline extraction is fundamental for plentiful medical applications. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 13.11.03,289,2021,No,Other medical imaging task,"Is about a dataset, which is good, also for classification, but not a classification method on its own",,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 12.55.00,285,2021,No,,Retina-Match: Ipsilateral Mammography Lesion Matching in a Single Shot Detection Pipeline,,,,,,,,,,,,,,,,,,,,,,,,,,,,
04/10/2022 11.21.01,281,2021,No,Other medical imaging task,Precise localization of polyp is crucial for early cancer screening in gastrointestinal endoscopy.,,,,,,,,,,,,,,,,,,,,,,,,,,,,
04/10/2022 11.14.57,279,2021,No,Other medical imaging task,Accurate localization and identification of vertebrae from CT images is a fundamental step in clinical spine diagnosis and treatment. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,
03/10/2022 09.19.11,275,2021,No,Segmentation,ASC-Net: Adversarial-Based Selective Network for Unsupervised Anomaly Segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
03/10/2022 09.18.24,273,2021,No,Other medical imaging task,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
03/10/2022 09.17.03,272,2021,No,Other medical imaging task,"In the experiments, we use three widely used metrics [20] to evaluate the performance of different retrieval methods. These evaluation metrics are: average cumulative gain (ACG), normalized discounted cumulative gain (NDCG), and weighted mean average precision (mAP$$_w$$).",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/09/2022 11.29.27,259,2021,No,Other medical imaging task,"Transfer learning is an important step to extract meaningful features and overcome the data limitation in the medical Visual Question Answering (VQA) task. However, most of the existing medical VQA methods rely on external data for transfer learning, while the meta-data within the dataset is not fully utilized. In this paper, we present a new multiple meta-model quantifying method that effectively learns meta-annotation and leverages meaningful features to the medical VQA task. Our proposed method is designed to increase meta-data by auto-annotation, deal with noisy labels, and output meta-models which provide robust features for medical VQA tasks. Extensively experimental results on two public medical VQA datasets show that our approach achieves superior accuracy in comparison with other state-of-the-art methods, while does not require external data to train meta-models.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 10.31.32,30,2021,No,Segmentation,Multi-compound Transformer for Accurate Biomedical Image Segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 10.30.02,25,2021,No,,Partially-Supervised Learning for Vessel Segmentation in Ocular Images,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 10.28.27,18,2021,No,Segmentation,CarveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 10.27.16,17,2021,No,Segmentation,Improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 10.25.31,8,2021,No,Segmentation,Consistent Segmentation of Longitudinal Brain MR Images with Spatio-Temporal Constrained Networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21/10/2022 09.00.59,326,2021,No,Other medical imaging task,Beyond Non-maximum Suppression - Detecting Lesions in Digital Breast Tomosynthesis Volumes,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21/10/2022 09.01.54,327,2021,No,Other medical imaging task,"Precision medicine involves answering counterfactual questions such as “Would this patient respond better to treatment A or treatment B?” These types of questions are causal in nature and require the tools of causal inference to be answered, e.g., with a structural causal model (SCM). In this work, we develop an SCM that models the interaction between demographic information, disease covariates, and magnetic resonance (MR) images of the brain for people with multiple sclerosis. Inference in the SCM generates counterfactual images that show what an MR image of the brain would look like if demographic or disease covariates are changed. These images can be used for modeling disease progression or used for downstream image processing tasks where controlling for confounders is necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21/10/2022 09.03.00,328,2021,No,Other medical imaging task,"Data auditing is a process to verify whether certain data have been removed from a trained model. A recently proposed method [10] uses Kolmogorov-Smirnov (KS) distance for such data auditing. However, it fails under certain practical conditions. In this paper, we propose a new method called Ensembled Membership Auditing ($$\mathsf {EMA}$$) for auditing data removal to overcome these limitations. We compare both methods using benchmark datasets (MNIST and SVHN) and Chest X-ray datasets with multi-layer perceptrons (MLP) and convolutional neural networks (CNN). Our experiments show that $$\mathsf {EMA}$$ is robust under various conditions, including the failure cases of the previously proposed method. Our code is available at: https://​github.​com/​Hazelsuko07/​EMA.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
21/10/2022 09.10.47,330,2021,No,Other medical imaging task,,"Confounding bias is a crucial problem when applying machine learning to practice, especially in clinical practice. We consider the problem of learning representations independent to multiple biases. In literature, this is mostly solved by purging the bias information from learned representations. We however expect this strategy to harm the diversity of information in the representation, and thus limiting its prospective usage (e.g., interpretation). Therefore, we propose to mitigate the bias while keeping almost all information in the latent representations, which enables us to observe and interpret them as well. To achieve this, we project latent features onto a learned vector direction, and enforce the independence between biases and projected features rather than all learned features. To interpret the mapping between projected features and input data, we propose projection-wise disentangling: a sampling and reconstruction along the learned vector direction. The proposed method was evaluated on the analysis of 3D facial shape and patient characteristics (N = 5011). Experiments showed that this conceptually simple method achieved state-of-the-art fair prediction performance and interpretability, showing its great potential for clinical application",,,,,,,,,,,,,,,,,,,,,,,,,,,
25/10/2022 13.37.59,264,2021,No,Other medical imaging task,"Developing a Universal Lesion Detector (ULD) that can detect various types of lesions from the whole body is of great importance for early diagnosis and timely treatment. Recently, deep neural networks have been applied for the ULD task, and existing methods assume that all the training samples are well-annotated. However, the partial label problem is unavoidable when curating large-scale datasets, where only a part of instances are annotated in each image. To address this issue, we propose a novel segmentation-assisted model, where an additional semantic segmentation branch with superpixel-guided selective loss is introduced to assist the conventional detection branch. The segmentation branch and the detection branch help each other to find unlabeled lesions with a mutual-mining strategy, and then the mined suspicious lesions are ignored for fine-tuning to reduce their negative impacts. Evaluation experiments on the DeepLesion dataset demonstrate that our proposed method allows the baseline detector to boost its average precision by 13%, outperforming the previous state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
25/10/2022 13.38.58,323,2021,No,Other medical imaging task,"Intracerebral hemorrhage (ICH) is the deadliest type of stroke. Early prediction of stroke lesion growth is crucial in assisting physicians towards better stroke assessments. Existing stroke lesion prediction methods are mainly for ischemic stroke. In ICH, most methods only focus on whether the hematoma will expand but not how it will develop. This paper explored a new, unknown topic of predicting ICH growth at the image-level based on the baseline non-contrast computerized tomography (NCCT) image and its hematoma mask. We propose a novel end-to-end prediction framework based on the displacement vector fields (DVF) with the following advantages. 1) It can simultaneously predict CT image and hematoma mask at follow-up, providing more clinical assessment references and surgery indication. 2) The DVF regularization enforces a smooth spatial deformation, limiting the degree of the stroke lesion changes and lowering the requirement of large data. 3) A multi-modal fusion module learns high-level associations between global clinical features and spatial image features. Experiments on a multi-center dataset demonstrate improved performance compared to several strong baselines. Detailed ablation experiments are conducted to highlight the contributions of various components.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 08.25.19,131,2021,No,Other medical imaging task,"Abstract
A convolutional neural network (ConvNet) is usually trained and then tested using images drawn from the same distribution. To generalize a ConvNet to various tasks often requires a complete training dataset that consists of images drawn from different tasks. In most scenarios, it is nearly impossible to collect every possible representative dataset as a priori. The new data may only become available after the ConvNet is deployed in clinical practice. ConvNet, however, may generate artifacts on out-of-distribution testing samples. In this study, we present Targeted Gradient Descent (TGD), a novel fine-tuning method that can extend a pre-trained network to a new task without revisiting data from the previous task while preserving the knowledge acquired from previous training. To a further extent, the proposed method also enables online learning of patient-specific data. The method is built on the idea of reusing a pre-trained ConvNet’s redundant kernels to learn new knowledge. We compare the performance of TGD to several commonly used training approaches on the task of Positron emission tomography (PET) image denoising. Results from clinical images show that TGD generated results on par with training-from-scratch while significantly reducing data preparation and network training time. More importantly, it enables online learning on the testing study to enhance the network’s generalization capability in real-world applications.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 08.27.01,132,2021,No,Other medical imaging task,"Abstract
Deep neural networks for medical images are extremely vulnerable to adversarial examples (AEs), which poses security concerns on clinical decision-making. Recent findings have shown that existing medical AEs are easy to detect in feature space. To better understand this phenomenon, we thoroughly investigate the characteristic of traditional medical AEs in feature space. Specifically, we first perform a stress test to reveal the vulnerability of medical images and compare them to natural images. Then, we theoretically prove that the existing adversarial attacks manipulate the prediction by continuously optimizing the vulnerable representations in a fixed direction, leading to outlier representations in feature space. Interestingly, we find this vulnerability is a double-edged sword that can be exploited to help hide AEs in the feature space. We propose a novel hierarchical feature constraint (HFC) as an add-on to existing white-box attacks, which encourages hiding the adversarial representation in the normal feature distribution. We evaluate the proposed method on two public medical image datasets, namely Fundoscopy and Chest X-Ray. Experimental results demonstrate the superiority of our HFC as it bypasses an array of state-of-the-art adversarial medical AEs detector more efficiently than competing adaptive attacks. Our code is available at https://​github.​com/​qsyao/​Hierarchical_​Feature_​Constraint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 08.28.54,135,2021,No,Other medical imaging task,"Abstract
Recently, medical report generation, which aims to automatically generate a long and coherent descriptive paragraph of a given medical image, has received growing research interests. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias: the normal visual regions dominate the dataset over the abnormal visual regions, and 2) the very long sequence. To alleviate above two problems, we propose an AlignTransformer framework, which includes the Align Hierarchical Attention (AHA) and the Multi-Grained Transformer (MGT) modules: 1) AHA module first predicts the disease tags from the input image and then learns the multi-grained visual features by hierarchically aligning the visual regions and disease tags. The acquired disease-grounded visual features can better represent the abnormal regions of the input image, which could alleviate data bias problem; 2) MGT module effectively uses the multi-grained features and Transformer framework to generate the long medical report. The experiments on the public IU-Xray and MIMIC-CXR datasets show that the AlignTransformer can achieve results competitive with state-of-the-art methods on the two datasets. Moreover, the human evaluation conducted by professional radiologists further proves the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 08.29.54,136,2021,No,Other medical imaging task,"Abstract
The ability to estimate how a tumor might evolve in the future could have tremendous clinical benefits, from improved treatment decisions to better dose distribution in radiation therapy. Recent work has approached the glioma growth modeling problem via deep learning and variational inference, thus learning growth dynamics entirely from a real patient data distribution. So far, this approach was constrained to predefined image acquisition intervals and sequences of fixed length, which limits its applicability in more realistic scenarios. We overcome these limitations by extending Neural Processes, a class of conditional generative models for stochastic time series, with a hierarchical multi-scale representation encoding including a spatio-temporal attention mechanism. The result is a learned growth model that can be conditioned on an arbitrary number of observations, and that can produce a distribution of temporally consistent growth trajectories on a continuous time axis. On a dataset of 379 patients, the approach successfully captures both global and finer-grained variations in the images, exhibiting superior performance compared to other learned growth models.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 08.30.32,137,2021,No,Other medical imaging task,"In this paper, we address the problem of automatic detection and localization of vertebrae in arbitrary Field-Of-View (FOV) Spine CT. We propose a novel transformers-based 3D object detection method that views automatic detection of vertebrae in arbitrary FOV CT scans as an one-to-one set prediction problem. The main components of the new framework, called Spine-Transformers, are an one-to-one set based global loss that forces unique predictions and a light-weighted transformer architecture equipped with skip connections and learnable positional embeddings for encoder and decoder, respectively. It reasons about the relations of different levels of vertebrae and the global volume context to directly output all vertebrae in parallel. We additionally propose an inscribed sphere-based object detector to replace the regular box-based object detector for a better handling of volume orientation variation. Comprehensive experiments are conducted on two public datasets and one in-house dataset. The experimental results demonstrate the efficacy of the present approach. A reference implementation of our method can be found at: https://​github.​com/​gloriatao/​Spine-Transformers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 08.44.39,141,2021,No,Other medical imaging task,"Abstract
Self-training based unsupervised domain adaptation (UDA) has shown great potential to address the problem of domain shift, when applying a trained deep learning model in a source domain to unlabeled target domains. However, while the self-training UDA has demonstrated its effectiveness on discriminative tasks, such as classification and segmentation, via the reliable pseudo-label selection based on the softmax discrete histogram, the self-training UDA for generative tasks, such as image synthesis, is not fully investigated. In this work, we propose a novel generative self-training (GST) UDA framework with continuous value prediction and regression objective for cross-domain image synthesis. Specifically, we propose to filter the pseudo-label with an uncertainty mask, and quantify the predictive confidence of generated images with practical variational Bayes learning. The fast test-time adaptation is achieved by a round-based alternative optimization scheme. We validated our framework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis problem, where datasets in the source and target domains were acquired from different scanners or centers. Extensive validations were carried out to verify our framework against popular adversarial training UDA methods. Results show that our GST, with tagged MRI of test subjects in new target domains, improved the synthesis quality by a large margin, compared with the adversarial training UDA methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 09.01.10,150,2021,No,Other medical imaging task,"Abstract
The difficulty of acquiring resting-state fMRI of early developing children under the same condition leads to a dedicated protocol, i.e., scanning younger infants during sleep and older children during being awake, respectively. However, the obviously different brain activities of sleep and awake states arouse a new challenge of awake-to-sleep connectome prediction/translation, which remains unexplored despite its importance in the longitudinally-consistent delineation of brain functional development. Due to the data scarcity and huge differences between natural images and geometric data (e.g., brain connectome), existing methods tailored for image translation generally fail in predicting functional connectome from awake to sleep. To fill this critical gap, we unprecedentedly propose a novel reference-relation guided autoencoder with deep CCA restriction (R2AE-dCCA) for awake-to-sleep connectome prediction. Specifically, 1) A reference-autoencoder (RAE) is proposed to realize a guided generation from the source domain to the target domain. The limited paired data are thus greatly augmented by including the combinations of all the age-restricted neighboring subjects as the references, while the target-specific pattern is fully learned; 2) A relation network is then designed and embedded into RAE, which utilizes the similarity in the source domain to determine the belief-strength of the reference during prediction; 3) To ensure that the learned relation in the source domain can effectively guide the generation in the target domain, a deep CCA restriction is further employed to maintain the neighboring relation during translation; 4) New validation metrics dedicated for connectome prediction are also proposed. Experimental results showed that our proposed R2AE-dCCA produces better prediction accuracy and well maintains the modular structure of brain functional connectome in comparison with state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 09.27.38,162,2021,No,Segmentation,Federated Whole Prostate Segmentation in MRI with Personalized Neural Architectures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 09.28.02,163,2021,No,Segmentation,Federated Contrastive Learning for Volumetric Medical Image Segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 09.35.58,165,2021,No,Other medical imaging task,"Abstract
Neural networks have demonstrated remarkable performance in classification and regression tasks on chest X-rays. In order to establish trust in the clinical routine, the networks’ prediction mechanism needs to be interpretable. One principal approach to interpretation is feature attribution. Feature attribution methods identify the importance of input features for the output prediction. Building on Information Bottleneck Attribution (IBA) method, for each prediction we identify the chest X-ray regions that have high mutual information with the network’s output. Original IBA identifies input regions that have sufficient predictive information. We propose Inverse IBA to identify all informative regions. Thus all predictive cues for pathologies are highlighted on the X-rays, a desirable property for chest X-ray diagnosis. Moreover, we propose Regression IBA for explaining regression models. Using Regression IBA we observe that a model trained on cumulative severity score labels implicitly learns the severity of different X-ray regions. Finally, we propose Multi-layer IBA to generate higher resolution and more detailed attribution/saliency maps. We evaluate our methods using both human-centric (ground-truth-based) interpretability metrics, and human-agnostic feature importance metrics on NIH Chest X-ray8 and BrixIA datasets. The code (https://​github.​com/​CAMP-eXplain-AI/​CheXplain-IBA) is publicly available.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 09.36.43,166,2021,No,Other medical imaging task,"Abstract
Recent development of image-to-image translation techniques has enabled the generation of rare medical images (e.g., PET) from common ones (e.g., MRI). Beyond the potential benefits of the reduction in scanning time, acquisition cost, and radiation exposure risks, the translation models in themselves are inscrutable black boxes. In this work, we propose two approaches to demystify the image translation process, where we particularly focus on the T1-MRI to PET translation. First, we adopt the representational similarity analysis and discover that the process of T1-MR to PET image translation includes the stages of brain tissue segmentation and brain region recognition, which unravels the relationship between the structural and functional neuroimaging data. Second, based on our findings, an Explainable and Simplified Image Translation (ESIT) model is proposed to demonstrate the capability of deep learning models for extracting gray matter volume information and identifying brain regions related to normal aging and Alzheimer’s disease, which untangles the biological plausibility hidden in deep learning models.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 09.38.33,168,2021,No,Other medical imaging task,"Abstract
Pelvic ring disruptions result from blunt injury mechanisms and are often found in patients with multi-system trauma. To grade pelvic fracture severity in trauma victims based on whole-body CT, the Tile AO/OTA classification is frequently used. Due to the high volume of whole-body trauma CTs generated in busy trauma centers, an automated approach to Tile classification would provide substantial value, e. g., to prioritize the reading queue of the attending trauma radiologist. In such scenario, an automated method should perform grading based on a transparent process and based on interpretable features to enable interaction with human readers and lower their workload by offering insights from a first automated read of the scan. This paper introduces an automated yet interpretable pelvic trauma decision support system to assist radiologists in fracture detection and Tile grade classification. The method operates similarly to human interpretation of CT scans and first detects distinct pelvic fractures on CT with high specificity using a Faster-RCNN model that are then interpreted using a structural causal model based on clinical best practices to infer an initial Tile grade. The Bayesian causal model and finally, the object detector are then queried for likely co-occurring fractures that may have been rejected initially due to the highly specific operating point of the detector, resulting in an updated list of detected fractures and corresponding final Tile grade. Our method is transparent in that it provides finding location and type using the object detector, as well as information on important counterfactuals that would invalidate the system’s recommendation and achieves an AUC of 83.3%/85.1% for translational/rotational instability. Despite being designed for human-machine teaming, our approach does not compromise on performance compared to previous black-box approaches.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 09.39.19,169,2021,No,Other medical imaging task,"Abstract
Deep Neural Networks (DNNs) have an enormous potential to learn from complex biomedical data. In particular, DNNs have been used to seamlessly fuse heterogeneous information from neuroanatomy, genetics, biomarkers, and neuropsychological tests for highly accurate Alzheimer’s disease diagnosis. On the other hand, their black-box nature is still a barrier for the adoption of such a system in the clinic, where interpretability is absolutely essential. We propose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for explaining the Alzheimer’s diagnosis made by a DNN from the 3D point cloud of the neuroanatomy and tabular biomarkers. Our explanations are based on the Shapley value, which is the unique method that satisfies all fundamental axioms for local explanations previously established in the literature. Thus, SVEHNN has many desirable characteristics that previous work on interpretability for medical decision making is lacking. To avoid the exponential time complexity of the Shapley value, we propose to transform a given DNN into a Lightweight Probabilistic Deep Network without re-training, thus achieving a complexity only quadratic in the number of features. In our experiments on synthetic and real data, we show that we can closely approximate the exact Shapley value with a dramatically reduced runtime and can reveal the hidden knowledge the network has learned from the data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 10.58.51,175,2021,No,Other medical imaging task,"Convolutional neural networks are showing promise in the automatic diagnosis of thoracic pathologies on chest x-rays. Their black-box nature has sparked many recent works to explain the prediction via input feature attribution methods (aka saliency methods). However, input feature attribution methods merely identify the importance of input regions for the prediction and lack semantic interpretation of model behavior. In this work, we first identify the semantics associated with internal units (feature maps) of the network. We proceed to investigate the following questions; Does a regression model that is only trained with COVID-19 severity scores implicitly learn visual patterns associated with thoracic pathologies? Does a network that is trained on weakly labeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover, we investigate the effect of pretraining and data imbalance on the interpretability of learned features. In addition to the analysis, we propose semantic attribution to semantically explain each prediction. We present our findings using publicly available chest pathologies (CheXpert [5], NIH ChestX-ray8 [25]) and COVID-19 datasets (BrixIA [20], and COVID-19 chest X-ray segmentation dataset [4]). The Code (https://​github.​com/​CAMP-eXplain-AI/​CheXplain-Dissection) is publicly available.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 11.02.41,176,2021,No,Other medical imaging task,"Abstract
Machine learning models commonly exhibit unexpected failures post-deployment due to either data shifts or uncommon situations in the training environment. Domain experts typically go through the tedious process of inspecting the failure cases manually, identifying failure modes and then attempting to fix the model. In this work, we aim to standardise and bring principles to this process through answering two critical questions: (i) how do we know that we have identified meaningful and distinct failure types?; (ii) how can we validate that a model has, indeed, been repaired? We suggest that the quality of the identified failure types can be validated through measuring the intra- and inter-type generalisation after fine-tuning and introduce metrics to compare different subtyping methods. Furthermore, we argue that a model can be considered repaired if it achieves high accuracy on the failure types while retaining performance on the previously correct data. We combine these two ideas into a principled framework for evaluating the quality of both the identified failure subtypes and model repairment. We evaluate its utility on a classification and an object detection tasks. Our code is available at https://​github.​com/​Rokken-lab6/​Failure-Analysis-and-Model-Repairment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 11.03.48,177,2021,No,Other medical imaging task,"Abstract
Model explainability is essential for the creation of trustworthy Machine Learning models in healthcare. An ideal explanation resembles the decision-making process of a domain expert and is expressed using concepts or terminology that is meaningful to the clinicians. To provide such explanation, we first associate the hidden units of the classifier to clinically relevant concepts. We take advantage of radiology reports accompanying the chest X-ray images to define concepts. We discover sparse associations between concepts and hidden units using a linear sparse logistic regression. To ensure that the identified units truly influence the classifier’s outcome, we adopt tools from Causal Inference literature and, more specifically, mediation analysis through counterfactual interventions. Finally, we construct a low-depth decision tree to translate all the discovered concepts into a straightforward decision rule, expressed to the radiologist. We evaluated our approach on a large chest x-ray dataset, where our model produces a global explanation consistent with clinical knowledge.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 11.04.36,179,2021,No,Other medical imaging task,"Abstract
Being accountable for the signed reports, pathologists may be wary of high-quality deep learning outcomes if the decision-making is not understandable. Applying off-the-shelf methods with default configurations such as Local Interpretable Model-Agnostic Explanations (LIME) is not sufficient to generate stable and understandable explanations. This work improves the application of LIME to histopathology images by leveraging nuclei annotations, creating a reliable way for pathologists to audit black-box tumor classifiers. The obtained visualizations reveal the sharp, neat and high attention of the deep classifier to the neoplastic nuclei in the dataset, an observation in line with clinical decision making. Compared to standard LIME, our explanations show improved understandability for domain-experts, report higher stability and pass the sanity checks of consistency to data or initialization changes and sensitivity to network parameters. This represents a promising step in giving pathologists tools to obtain additional information on image classification models. The code and trained models are available on GitHub.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 11.10.55,181,2021,No,Other medical imaging task,"Abstract
Application of deep neural networks to medical imaging tasks has in some sense become commonplace. Still, a “thorn in the side” of the deep learning movement is the argument that deep networks are prone to overfitting and are thus unable to generalize well when datasets are small (as is common in medical imaging tasks). One way to bolster confidence is to provide mathematical guarantees, or bounds, on network performance after training which explicitly quantify the possibility of overfitting. In this work, we explore recent advances using the PAC-Bayesian framework to provide bounds on generalization error for large (stochastic) networks. While previous efforts focus on classification in larger natural image datasets (e.g., MNIST and CIFAR-10), we apply these techniques to both classification and segmentation in a smaller medical imagining dataset: the ISIC 2018 challenge set. We observe the resultant bounds are competitive compared to a simpler baseline, while also being more explainable and alleviating the need for holdout sets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 11.11.47,185,2021,No,Other medical imaging task,"Abstract
In healthcare applications, predictive uncertainty has been used to assess predictive accuracy. In this paper, we demonstrate that predictive uncertainty estimated by the current methods does not highly correlate with prediction error by decomposing the latter into random and systematic errors, and showing that the former is equivalent to the variance of the random error. In addition, we observe that current methods unnecessarily compromise performance by modifying the model and training loss to estimate the target and uncertainty jointly. We show that estimating them separately without modifications improves performance. Following this, we propose a novel method that estimates the target labels and magnitude of the prediction error in two steps. We demonstrate this method on a large-scale MRI reconstruction task, and achieve significantly better results than the state-of-the-art uncertainty estimation methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 11.12.29,187,2021,No,Other medical imaging task,"Automating report generation for medical imaging promises to reduce workload and assist diagnosis in clinical practice. Recent work has shown that deep learning models can successfully caption natural images. However, learning from medical data is challenging due to the diversity and uncertainty inherent in the reports written by different radiologists with discrepant expertise and experience. To tackle these challenges, we propose variational topic inference for automatic report generation. Specifically, we introduce a set of topics as latent variables to guide sentence generation by aligning image and language modalities in a latent space. The topics are inferred in a conditional variational inference framework, with each topic governing the generation of a sentence in the report. Further, we adopt a visual attention module that enables the model to attend to different locations in the image and generate more informative descriptions. We conduct extensive experiments on two benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results demonstrate that our proposed variational topic inference method can generate novel reports rather than mere copies of reports used in training, while still achieving comparable performance to state-of-the-art methods in terms of standard language generation criteria.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/10/2022 11.13.12,188,2021,No,Other medical imaging task,"Deep reinforcement learning (DRL) is a promising technique for anatomical landmark detection in 3D medical images and a useful first step in automated medical imaging pathology detection. However, deployment of landmark detection in a pathology detection pipeline requires a self-assessment process to identify out-of-distribution images for manual review. We therefore propose a novel method derived from the full-width-half-maxima of q-value probability distributions for estimating the uncertainty of a distributional deep q-learning (dist-DQN) landmark detection agent. We trained two dist-DQN models targeting the locations of knee fibular styloid and intercondylar eminence of the tibia, using 1552 MR sequences (Sagittal PD, PDFS and T2FS) with an approximate 75%, 5%, 20% training, validation, and test split. Error for the two landmarks was 3.25 ± 0.12 mm and 3.06 ± 0.10 mm respectively (mean ± standard error). Mean error for the two landmarks was 28% lower than a non-distributional DQN baseline (3.16 ± 0.11 mm vs 4.36 ± 0.27 mm). Additionally, we demonstrate that the dist-DQN derived uncertainty metric has an AUC of 0.91 for predicting out-of-distribution images with a specificity of 0.77 at sensitivity 0.90, illustrating the double benefit of improved error rate and the ability to defer reviews to experts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.23.05,76,2021,No,Other medical imaging task,"Abstract
Longitudinal MRIs are often used to capture the gradual deterioration of brain structure and function caused by aging or neurological diseases. Analyzing this data via machine learning generally requires a large number of ground-truth labels, which are often missing or expensive to obtain. Reducing the need for labels, we propose a self-supervised strategy for representation learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts in contrastive learning, LNE explicitly models the similarity between trajectory vectors across different subjects. We do so by building a graph in each training iteration defining neighborhoods in the latent space so that the progression direction of a subject follows the direction of its neighbors. This results in a smooth trajectory field that captures the global morphological change of the brain while maintaining the local continuity. We apply LNE to longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274 healthy subjects, and Alzheimer’s Disease Neuroimaging Initiative (ADNI, $$N=632$$). The visualization of the smooth trajectory vector field and superior performance on downstream tasks demonstrate the strength of the proposed method over existing self-supervised methods in extracting information associated with normal aging and in revealing the impact of neurodegenerative disorders. The code is available at https://​github.​com/​ouyangjiahong/​longitudinal-neighbourhood-embedding.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.29.43,79,2021,No,Other medical imaging task,"Abstract
Manually annotating medical images is extremely expensive, especially for large-scale datasets. Self-supervised contrastive learning has been explored to learn feature representations from unlabeled images. However, unlike natural images, the application of contrastive learning to medical images is relatively limited. In this work, we propose a self-supervised framework, namely lesion-based contrastive learning for automated diabetic retinopathy (DR) grading. Instead of taking entire images as the input in the common contrastive learning scheme, lesion patches are employed to encourage the feature extractor to learn representations that are highly discriminative for DR grading. We also investigate different data augmentation operations in defining our contrastive prediction task. Extensive experiments are conducted on the publicly-accessible dataset EyePACS, demonstrating that our proposed framework performs outstandingly on DR grading in terms of both linear evaluation and transfer capacity evaluation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.30.12,80,2021,No,Segmentation,"Automatic and accurate tumor segmentation on medical images is in high demand to assist physicians with diagnosis and treatment. However, it is difficult to obtain massive amounts of annotated training data required by the deep-learning models as the manual delineation process is often tedious and expertise required. Although self-supervised learning (SSL) scheme has been widely adopted to address this problem, most SSL methods focus only on global structure information, ignoring the key distinguishing features of tumor regions: local intensity variation and large size distribution. In this paper, we propose Scale-Aware Restoration (SAR), a SSL method for 3D tumor segmentation. Specifically, a novel proxy task, i.e. scale discrimination, is formulated to pre-train the 3D neural network combined with the self-restoration task. Thus, the pre-trained model learns multi-level local representations through multi-scale inputs. Moreover, an adversarial learning module is further introduced to learn modality invariant representations from multiple unlabeled source datasets. We demonstrate the effectiveness of our methods on two downstream tasks: i) Brain tumor segmentation, ii) Pancreas tumor segmentation. Compared with the state-of-the-art 3D SSL methods, our proposed approach can significantly improve the segmentation accuracy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.34.56,83,2021,No,Other medical imaging task,"Abstract
Deep learning networks have shown promising performance for object localization in medical images, but require large amount of annotated data for supervised training. To address this problem, we propose: 1) A novel contrastive learning method which embeds the anatomical structure by predicting the Relative Position Regression (RPR) between any two patches from the same volume; 2) An one-shot framework for organ and landmark localization in volumetric medical images. Our main idea comes from that tissues and organs from different human bodies own similar relative position and context. Therefore, we could predict the relative positions of their non-local patches, thus locate the target organ. Our one-shot localization framework is composed of three parts: 1) A deep network trained to project the input patch into a 3D latent vector, representing its anatomical position; 2) A coarse-to-fine framework contains two projection networks, providing more accurate localization of the target; 3) Based on the coarse-to-fine model, we transfer the organ bounding-box (B-box) detection to locating six extreme points along x, y and z directions in the query volume. Experiments on multi-organ localization from head-and-neck (HaN) and abdominal CT volumes showed that our method acquired competitive performance in real time, which is more accurate and $$10^5$$ times faster than template matching methods with the same setting for one-shot localization in 3D medical images. Code is available at https://​github.​com/​HiLab-git/​RPR-Loc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.35.45,84,2021,No,Other medical imaging task,"Abstract
A long-standing challenge in multimodal brain network analyses is to integrate topologically different brain networks obtained from diffusion and functional MRI in a coherent statistical framework. Existing multimodal frameworks will inevitably destroy the topological difference of the networks. In this paper, we propose a novel topological learning framework that integrates networks of different topology through persistent homology. Such challenging task is made possible through the introduction of a new topological loss that bypasses intrinsic computational bottlenecks and thus enables us to perform various topological computations and optimizations with ease. We validate the topological loss in extensive statistical simulations with ground truth to assess its effectiveness of discriminating networks. Among many possible applications, we demonstrate the versatility of topological loss in the twin imaging study where we determine the extend to which brain networks are genetically heritable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.36.47,86,2021,No,Other medical imaging task,"Abstract
We propose a novel unsupervised out-of-distribution detection method for medical images based on implicit fields image representations. In our approach, an auto-decoder feed-forward neural network learns the distribution of healthy images in the form of a mapping between spatial coordinates and probabilities over a proxy for tissue types. At inference time, the learnt distribution is used to retrieve, from a given test image, a restoration, i.e. an image maximally consistent with the input one but belonging to the healthy distribution. Anomalies are localized using the voxel-wise probability predicted by our model for the restored image. We tested our approach in the task of unsupervised localization of gliomas on brain MR images and compared it to several other VAE-based anomaly detection methods. Results show that the proposed technique substantially outperforms them (average DICE 0.640 vs 0.518 for the best performing VAE-based alternative) while also requiring considerably less computing time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.37.16,87,2021,No,Segmentation,Dual-Consistency Semi-supervised Learning with Uncertainty Quantification for COVID-19 Lesion Segmentation from CT Images,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.38.26,88,2021,No,Other medical imaging task,"Abstract
One of the primary challenges facing medical visual question answering (Med-VQA) is the lack of large-scale well-annotated datasets for training. To overcome this challenge, this paper proposes a two-stage pre-training framework by learning transferable feature representations of radiology images and distilling a lightweight visual feature extractor for Med-VQA. Specifically, we leverage large amounts of unlabeled radiology images to train three teacher models for the body regions of brain, chest, and abdomen respectively via contrastive learning. Then, we distill the teacher models to a lightweight student model that can be used as a universal visual feature extractor for any Med-VQA system. The lightweight feature extractor can be readily fine-tuned on the training radiology images of any Med-VQA dataset, saving the annotation effort while preventing overfitting to small-scale training data. The effectiveness and advantages of the pre-trained model are demonstrated by extensive experiments with state-of-the-art Med-VQA methods on existing benchmarks. The source code and the pre-training dataset can be downloaded from https://​github.​com/​awenbocc/​cprd.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.38.53,89,2021,No,Segmentation,Positional Contrastive Learning for Volumetric Medical Image Segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.39.46,90,2021,No,Other medical imaging task,"Abstract
The problem of building disease progression models with longitudinal data has long been addressed with parametric mixed-effect models. They provide interpretable models at the cost of modeling assumptions on the progression profiles and their variability across subjects. Their deep learning counterparts, on the other hand, strive on flexible data-driven modeling, and additional interpretability - or, as far as generative models are involved, disentanglement of latent variables with respect to generative factors - comes from additional constraints. In this work, we propose a deep longitudinal model designed to disentangle inter-patient variability from an estimated disease progression timeline. We do not seek for an explicit mapping between age and disease stage, but to learn the latter solely from the ordering between visits using a differentiable ranking loss. Furthermore, we encourage inter-patient variability to be encoded in a separate latent space, where for each patient a single representation is learned from its set of visits, with a constraint of invariance under permutation of the visits. The modularity of the network architecture allows us to apply our model on various data types: a synthetic image dataset with known generative factors, cognitive assessments and neuroimaging data. We show that, combined with our patient encoder, the ranking loss for visits helps to exceed models with supervision, in particular in terms of disease staging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.40.43,91,2021,No,Segmentation,"Abstract
Vessel segmentation is an essential task in many clinical applications. Although supervised methods have achieved state-of-art performance, acquiring expert annotation is laborious and mostly limited for two-dimensional datasets with a small sample size. On the contrary, unsupervised methods rely on handcrafted features to detect tube-like structures such as vessels. However, those methods require complex pipelines involving several hyper-parameters and design choices rendering the procedure sensitive, dataset-specific, and not generalizable. We propose a self-supervised method with a limited number of hyper-parameters that is generalizable across modalities. Our method uses tube-like structure properties, such as connectivity, profile consistency, and bifurcation, to introduce inductive bias into a learning algorithm. To model those properties, we generate a vector field that we refer to as a flow. Our experiments on various public datasets in 2D and 3D show that our method performs better than unsupervised methods while learning useful transferable features from unlabeled data. Unlike generic self-supervised methods, the learned features learn vessel-relevant features that are transferable for supervised approaches, which is essential when the number of annotated data is limited.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.50.22,95,2021,No,Other medical imaging task,"Abstract
In recent studies, we have witnessed the applicability of deep learning methods on resting-state functional magnetic resonance image (rs-fMRI) analysis and its use for brain disease diagnosis, e.g., autism spectrum disorder (ASD). However, it still remains challenging to learn discriminative representations from raw BOLD signals or functional connectivity (FC) with a limited number of samples. In this paper, we propose a simple but efficient representation learning method for FC in a self-supervised learning manner. Specifically, we devise a proxy task of estimating the randomly masked seed-based functional networks from the remaining ones in FC, to discover the complex high-level relations among brain regions, which are not directly observable from an input FC. Thanks to the random masking strategy in our proxy task, it also has the effect of augmenting training samples, thus allowing for robust training. With the pretrained feature representation network in a self-supervised manner, we then construct a decision network for the downstream task of ASD diagnosis. In order to validate the effectiveness of our proposed method, we used the ABIDE dataset that collected subjects from multiple sites and our proposed method showed superiority to the comparative methods in various metrics",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.51.12,100,2021,No,Segmentation,"Abstract
Measuring lesion size is an important step to assess tumor growth and monitor disease progression and therapy response in oncology image analysis. Although it is tedious and highly time-consuming, radiologists have to work on this task by using RECIST criteria (Response Evaluation Criteria In Solid Tumors) routinely and manually. Even though lesion segmentation may be the more accurate and clinically more valuable means, physicians can not manually segment lesions as now since much more heavy laboring will be required. In this paper, we present a prior-guided dual-path network (PDNet) to segment common types of lesions throughout the whole body and predict their RECIST diameters accurately and automatically. Similar to [23], a click guidance from radiologists is the only requirement. There are two key characteristics in PDNet: 1) Learning lesion-specific attention matrices in parallel from the click prior information by the proposed prior encoder, named click-driven attention; 2) Aggregating the extracted multi-scale features comprehensively by introducing top-down and bottom-up connections in the proposed decoder, named dual-path connection. Experiments show the superiority of our proposed PDNet in lesion segmentation and RECIST diameter prediction using the DeepLesion dataset and an external test set. PDNet learns comprehensive and representative deep image features for our tasks and produces more accurate results on both lesion segmentation and RECIST diameter prediction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 10.51.41,101,2021,No,Segmentation,"Abstract
Semi-supervised learning has been recently employed to solve problems from medical image segmentation due to challenges in acquiring sufficient manual annotations, which is an important prerequisite for building high-performance deep learning methods. Since unlabeled data is generally abundant, most existing semi-supervised approaches focus on how to make full use of both limited labeled data and abundant unlabeled data. In this paper, we propose a novel semi-supervised strategy called reciprocal learning for medical image segmentation, which can be easily integrated into any CNN architecture. Concretely, the reciprocal learning works by having a pair of networks, one as a student and one as a teacher. The student model learns from pseudo label generated by the teacher. Furthermore, the teacher updates its parameters autonomously according to the reciprocal feedback signal of how well student performs on the labeled set. Extensive experiments on two public datasets show that our method outperforms current state-of-the-art semi-supervised segmentation methods, demonstrating the potential of our strategy for the challenging semi-supervised problems. The code is publicly available at https://​github.​com/​XYZach/​RLSSS.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 11.52.07,104,2021,No,Other medical imaging task,"Abstract
Minimally invasive surgery (MIS) has many documented advantages, but the surgeon’s limited visual contact with the scene can be problematic. Hence, systems that can help surgeons navigate, such as a method that can produce a 3D semantic map, can compensate for the limitation above. In theory, we can borrow 3D semantic mapping techniques developed for robotics, but this requires finding solutions to the following challenges in MIS: 1) semantic segmentation, 2) depth estimation, and 3) pose estimation. In this paper, we propose the first 3D semantic mapping system from knee arthroscopy that solves the three challenges above. Using out-of-distribution non-human datasets, where pose could be labeled, we jointly train depth+pose estimators using self-supervised and supervised losses. Using an in-distribution human knee dataset, we train a fully-supervised semantic segmentation system to label arthroscopic image pixels into femur, ACL, and meniscus. Taking testing images from human knees, we combine the results from these two systems to automatically create 3D semantic maps of the human knee. The result of this work opens the pathway to the generation of intra-operative 3D semantic mapping, registration with pre-operative data, and robotic-assisted arthroscopy. Source code: https://​github.​com/​YJonmo/​EndoMapNet.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 11.58.58,109,2021,No,Segmentation,"Abstract
Due to the difficulty in accessing a large amount of labeled data, semi-supervised learning is becoming an attractive solution in medical image segmentation. To make use of unlabeled data, current popular semi-supervised methods (e.g., temporal ensembling, mean teacher) mainly impose data-level and model-level consistency on unlabeled data. In this paper, we argue that in addition to these strategies, we could further utilize auxiliary tasks and consider task-level consistency to better leverage unlabeled data for segmentation. Specifically, we introduce two auxiliary tasks, i.e., a foreground and background reconstruction task for capturing semantic information and a signed distance field (SDF) prediction task for imposing shape constraint, and explore the mutual promotion effect between the two auxiliary and the segmentation tasks based on mean teacher architecture. Moreover, to handle the potential bias of the teacher model caused by annotation scarcity, we develop a tripled-uncertainty guided framework to encourage the three tasks in the teacher model to generate more reliable pseudo labels. When calculating uncertainty, we propose an uncertainty weighted integration (UWI) strategy for yielding the segmentation predictions of the teacher. Extensive experiments on public 2017 ACDC dataset and PROMISE12 dataset have demostrated the effectiveness of our method. Code is available at https://​github.​com/​DeepMedLab/​Tri-U-MT.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 12.02.32,113,2021,No,Segmentation,"Abstract
The success of deep learning methods in medical image segmentation tasks heavily depends on a large amount of labeled data to supervise the training. On the other hand, the annotation of biomedical images requires domain knowledge and can be laborious. Recently, contrastive learning has demonstrated great potential in learning latent representation of images even without any label. Existing works have explored its application to biomedical image segmentation where only a small portion of data is labeled, through a pre-training phase based on self-supervised contrastive learning without using any labels followed by a supervised fine-tuning phase on the labeled portion of data only. In this paper, we establish that by including the limited label information in the pre-training phase, it is possible to boost the performance of contrastive learning. We propose a supervised local contrastive loss that leverages limited pixel-wise annotation to force pixels with the same label to gather around in the embedding space. Such loss needs pixel-wise computation which can be expensive for large images, and we further propose two strategies, downsampling and block division, to address the issue. We evaluate our methods on two public biomedical image datasets of different modalities. With different amounts of labeled data, our methods consistently outperform the state-of-the-art contrast-based methods and other semi-supervised learning techniques.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 12.03.36,114,2021,No,Other medical imaging task,"Abstract
Advances in computational cognitive neuroimaging research are related to the availability of large amounts of labeled brain imaging data, but such data are scarce and expensive to generate. While powerful data generation mechanisms, such as Generative Adversarial Networks (GANs), have been designed in the last decade for computer vision, such improvements have not yet carried over to brain imaging. A likely reason is that GANs training is ill-suited to the noisy, high-dimensional and small-sample data available in functional neuroimaging. In this paper, we introduce Conditional Independent Components Analysis (Conditional ICA): a fast functional Magnetic Resonance Imaging (fMRI) data augmentation technique, that leverages abundant resting-state data to create images by sampling from an ICA decomposition. We then propose a mechanism to condition the generator on classes observed with few samples. We first show that the generative mechanism is successful at synthesizing data indistinguishable from observations, and that it yields gains in classification accuracy in brain decoding problems. In particular it outperforms GANs while being much easier to optimize and interpret. Lastly, Conditional ICA enhances classification accuracy in eight datasets without further parameters tuning.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/10/2022 12.09.12,121,2021,No,Other medical imaging task,Improving Pneumonia Localization via Cross-Attention on Medical Images and Reports,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17/10/2022 09.17.50,248,2012,Yes,It was accurately labelled,"In addition to segmentation,
since the feature vectors capture the information on the phase retardation
caused by cells, they can be used for cell stage classification between
intermitotic and mitotic/apoptotic stages. Experiments on three image
sequences demonstrate that the dictionary-based restoration method can
restore phase contrast images containing cells with different optical natures
and provide promising results on cell stage classification.","the
authors analyze the image formation process of phase contrast images
and propose an image restoration method based on the dictionary representation
of diffraction patterns.","Computer-aided image analysis of phase contrast microscopy [1] has attracted
increasing attention since it enables long-term monitoring of the proliferation
and migration processes of live cells.",K-means clustering,"Precision, Recall",Yes,The sample results of cell segmentation.,,"The proposed approach was tested on three different sets of phase contrast
images of 1040× 1392 pixels. The specifications of the datasets are summarized
in Table 1.","The proposed approach was tested on three different sets of phase contrast
images of 1040× 1392 pixels. The specifications of the datasets are summarized
in Table 1.",Private,"The proposed approach was tested on three different sets of phase contrast
images of 1040× 1392 pixels. The specifications of the datasets are summarized
in Table 1.",No,Nothing is mentioned,No,"Nothing is mentioned - also not relevant, cells?",No,Nothing is mentioned,Yes,"Department of EE, Shanghai Jiaotong University
2 Department of CS, Missouri University of Science and Technology
3 The Robotics Institute, Carnegie Mellon University

This research is supported by funds from Cell Image Analysis Consortium of Carnegie
Mellon University and University of Missouri Research Board.",No,"Nothing is mentioned, not relevant, cells?",No,Nothing is mentioned,No,"Nothing is mentioned, not relevant, cells?",Yes,The article includes pseudocode,
17/10/2022 09.10.32,242,2012,Yes,It was accurately labelled,"Distortion correction is applied to endoscopic duodenal imagery to
improve automated classification of celiac disease affected mucosa patches.","Distortion correction is applied to endoscopic duodenal imagery to
improve automated classification of celiac disease affected mucosa patches.","Computer-aided decision support systems relying on automated analysis of endoscopic
imagery receive increasing attention [1].
A specific type of degradation, present in all endoscopic images, is a barrel-type
distortion. This type of degradation is caused by the wide-angle (fish eye) nature of the
optics used in endoscopes.
The aim of correcting this distortion in endoscopy is manifold","SVM, Discriminant analysis, knn","Specificity, Accuracy, Sensitivity",No,Nothing is mentioned,No,"The image test set used in this work (see [7] for example images) stems from three
pediatric gastroscopes without magnification, types GIF-Q165 and GIF-N180, Olympus,
with two of the first type and one of the latter type, respectively. The patients
presented in the pediatric Department because of celiac-like symptoms.","From the acquired images, an experienced endoscopist extracted 128 × 128 pixels
patches significant for diagnosis. The images and patients were pre-classified by the diagnostic
outcome of the biopsy of the significant region at the hospital into the modified
Marsh classification as shown in Table 1.",Private,"The image test set used in this work (see [7] for example images) stems from three
pediatric gastroscopes without magnification, types GIF-Q165 and GIF-N180, Olympus,
with two of the first type and one of the latter type, respectively. The patients
presented in the pediatric Department because of celiac-like symptoms.",No,"The image test set used in this work (see [7] for example images) stems from three
pediatric gastroscopes without magnification, types GIF-Q165 and GIF-N180, Olympus,
with two of the first type and one of the latter type, respectively. The patients
presented in the pediatric Department because of celiac-like symptoms.",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"1 Department of Computer Sciences
University of Salzburg, Austria
2 St.Anna Children’s Hospital, Dept. Pediatrics
Medical University, Vienna

This work has been partially supported by the Austrian Science Fund project no. 24366.",No,"They only mention that patients were there because they showed symptoms..:
The patients
presented in the pediatric Department because of celiac-like symptoms.",No,Nothing is mentioned,No,Nothing is mentioned,Yes,There are several links to implementations used during the preprocessing stage,
17/10/2022 08.55.02,240,2012,Yes,It was accurately labelled,"In this study, we propose a computational diagnosis system
for detecting the colorectal cancer from histopathological slices.The computational
analysis was usually performed on patch level where only
a small part of the slice is covered. However, slice-based classification
is more realistic for histopathological diagnosis.","In this study, we propose a computational diagnosis system
for detecting the colorectal cancer from histopathological slices.The computational
analysis was usually performed on patch level where only
a small part of the slice is covered. However, slice-based classification
is more realistic for histopathological diagnosis.","Colorectal cancer is the third most common cancer in both men and women
world-wide and is the third leading cause of cancer-related deaths in the Western
world [1]. For 2012, 103,000 colon cancer cases and 51,000 colon cancer
related deaths are predicted for the United States. Like for many other types
of cancer, histopathological analysis is accepted as the gold standard for malignancy
diagnosis [2]. The analysis of these data, however, is performed visually
and the detection and grading of the suspect tissue may show variability depending
on the experience and awareness of the experts. Therefore, various studies
have been performed into the development of computer-aided diagnosis systems
(CAD) to improve the ability of pathologists at discriminating between malignant
and benign tissue.",logistic classifier,"AUC, Accuracy",Yes,"The nuclei in patches could be better segmented
using more advanced techniques.",No,"A total of 120 H&E stained colon biopsy slices from 96 different patients were collected
and scanned by high resolution camera at AtriumMedical Center, Heerlen.
The original slices (sized about 70,000x120,000) consist of equal sized patches of
1024x1024 pixels. Each slice may include different numbers of patches that display
actual tissue (between 200 and 6000) depending on the size of the original
biopsy etc. Slices were labeled at two different levels: either at slice-level or at
patch-level. 55 of the slices are used for patch-based labeling where each individual
patch was assigned to one of the four primary (normal, cancer, adenomatous,
inflamed) and two secondary classes (unknown, inappropriate) by a pathologist
(see Fig.2). The unknown class is for the patches which the pathologist is not sure
and inappropriate is for the patches which are not appropriate for analysis due to
the imaging problems such as camera focus.We consider the primary classes only,
which include 6134, 2503, 2261 and 2967 patches, respectively.","A total of 120 H&E stained colon biopsy slices from 96 different patients were collected
and scanned by high resolution camera at AtriumMedical Center, Heerlen.
The original slices (sized about 70,000x120,000) consist of equal sized patches of
1024x1024 pixels. Each slice may include different numbers of patches that display
actual tissue (between 200 and 6000) depending on the size of the original
biopsy etc. Slices were labeled at two different levels: either at slice-level or at
patch-level. 55 of the slices are used for patch-based labeling where each individual
patch was assigned to one of the four primary (normal, cancer, adenomatous,
inflamed) and two secondary classes (unknown, inappropriate) by a pathologist
(see Fig.2). The unknown class is for the patches which the pathologist is not sure
and inappropriate is for the patches which are not appropriate for analysis due to
the imaging problems such as camera focus.We consider the primary classes only,
which include 6134, 2503, 2261 and 2967 patches, respectively.",Private,"A total of 120 H&E stained colon biopsy slices from 96 different patients were collected
and scanned by high resolution camera at AtriumMedical Center, Heerlen.
The original slices (sized about 70,000x120,000) consist of equal sized patches of
1024x1024 pixels. Each slice may include different numbers of patches that display
actual tissue (between 200 and 6000) depending on the size of the original
biopsy etc. Slices were labeled at two different levels: either at slice-level or at
patch-level. 55 of the slices are used for patch-based labeling where each individual
patch was assigned to one of the four primary (normal, cancer, adenomatous,
inflamed) and two secondary classes (unknown, inappropriate) by a pathologist
(see Fig.2). The unknown class is for the patches which the pathologist is not sure
and inappropriate is for the patches which are not appropriate for analysis due to
the imaging problems such as camera focus.We consider the primary classes only,
which include 6134, 2503, 2261 and 2967 patches, respectively.",No,"A total of 120 H&E stained colon biopsy slices from 96 different patients were collected
and scanned by high resolution camera at AtriumMedical Center, Heerlen.
The original slices (sized about 70,000x120,000) consist of equal sized patches of
1024x1024 pixels. Each slice may include different numbers of patches that display
actual tissue (between 200 and 6000) depending on the size of the original
biopsy etc. Slices were labeled at two different levels: either at slice-level or at
patch-level. 55 of the slices are used for patch-based labeling where each individual
patch was assigned to one of the four primary (normal, cancer, adenomatous,
inflamed) and two secondary classes (unknown, inappropriate) by a pathologist
(see Fig.2). The unknown class is for the patches which the pathologist is not sure
and inappropriate is for the patches which are not appropriate for analysis due to
the imaging problems such as camera focus.We consider the primary classes only,
which include 6134, 2503, 2261 and 2967 patches, respectively.",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"1 Pattern Recognition Laboratory, Delft University of Technology, The Netherlands
2 Atrium Medical Center, Heerlen, The Netherlands
3 Department of Computer Engineering, Suleyman Demirel University, Turkey

Habil Kalkan is supported by TUBITAK-BIDEB 2219 program.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
17/10/2022 08.40.16,235,2012,Yes,It was accurately labelled,"Twenty-eight features are computed for each fracture
line and sent to a committee of support vector machines for classification.","We propose a
fully automated method to detect acute vertebral body fractures on trauma CT
studies.","Assessment of trauma patients with multiple injuries, particularly in the setting of
multiple trauma patients presenting to the hospital concurrently, can be one of the
most clinically challenging situations dealt with by the radiologist. Traumatic injury
of the spine is a subset of the spectrum of blunt trauma pathology, and is common and
potentially devastating. Previous reports estimate the number of vertebral fractures
each year in the United States at more than 140,000, with 19%-50% of fractures of the
thoracolumbar spine associated with neurological injury [1]. Rapid and accurate
assessment is essential for determination of an acceptable management algorithm, and
delay in detection and management of",SVM,Sensitivity,Yes,"The cortical shell of vertebral
body is then segmented using deformable dual-surface models",No,"Our cohort includes 18 trauma patients admitted to UC Irvine Medical Center between
June 2009 and July 2010. The mean patient age was 51±11 yrs (18-86yrs). There were
13 men and 5 women. All patients were scanned on a Siemens Sensation 64 scanner.
The scanning parameters were: 2mm slice thickness, 120 kvp, no intravenous contrast
administration, and convolution kernel B40f (16 patients) or B60f (2 patients). The CT
data covered the thoracic and lumbar spines, and included 14 vertebrae on average. An
expert radiologist examined the cases and manually marked the fracture sites. Ten
patients were positive for vertebral body fractures. The total number of spatially distinct
fracture sites was 21, among the 10 patients. The remaining 8 patients had no evidence
of vertebral body fracture. The average running time is 5.6 minutes.","Our cohort includes 18 trauma patients admitted to UC Irvine Medical Center between
June 2009 and July 2010. The mean patient age was 51±11 yrs (18-86yrs). There were
13 men and 5 women. All patients were scanned on a Siemens Sensation 64 scanner.
The scanning parameters were: 2mm slice thickness, 120 kvp, no intravenous contrast
administration, and convolution kernel B40f (16 patients) or B60f (2 patients). The CT
data covered the thoracic and lumbar spines, and included 14 vertebrae on average. An
expert radiologist examined the cases and manually marked the fracture sites. Ten
patients were positive for vertebral body fractures. The total number of spatially distinct
fracture sites was 21, among the 10 patients. The remaining 8 patients had no evidence
of vertebral body fracture. The average running time is 5.6 minutes.",Private,"Our cohort includes 18 trauma patients admitted to UC Irvine Medical Center between
June 2009 and July 2010. The mean patient age was 51±11 yrs (18-86yrs). There were
13 men and 5 women. All patients were scanned on a Siemens Sensation 64 scanner.
The scanning parameters were: 2mm slice thickness, 120 kvp, no intravenous contrast
administration, and convolution kernel B40f (16 patients) or B60f (2 patients). The CT
data covered the thoracic and lumbar spines, and included 14 vertebrae on average. An
expert radiologist examined the cases and manually marked the fracture sites. Ten
patients were positive for vertebral body fractures. The total number of spatially distinct
fracture sites was 21, among the 10 patients. The remaining 8 patients had no evidence
of vertebral body fracture. The average running time is 5.6 minutes.",Yes,"Our cohort includes 18 trauma patients admitted to UC Irvine Medical Center between
June 2009 and July 2010. The mean patient age was 51±11 yrs (18-86yrs). There were
13 men and 5 women. All patients were scanned on a Siemens Sensation 64 scanner.
The scanning parameters were: 2mm slice thickness, 120 kvp, no intravenous contrast
administration, and convolution kernel B40f (16 patients) or B60f (2 patients). The CT
data covered the thoracic and lumbar spines, and included 14 vertebrae on average. An
expert radiologist examined the cases and manually marked the fracture sites. Ten
patients were positive for vertebral body fractures. The total number of spatially distinct
fracture sites was 21, among the 10 patients. The remaining 8 patients had no evidence
of vertebral body fracture. The average running time is 5.6 minutes.",Yes,"Sex and age(mean and range are reported): 

Our cohort includes 18 trauma patients admitted to UC Irvine Medical Center between
June 2009 and July 2010. The mean patient age was 51±11 yrs (18-86yrs). There were
13 men and 5 women. All patients were scanned on a Siemens Sensation 64 scanner.
The scanning parameters were: 2mm slice thickness, 120 kvp, no intravenous contrast
administration, and convolution kernel B40f (16 patients) or B60f (2 patients). The CT
data covered the thoracic and lumbar spines, and included 14 vertebrae on average. An
expert radiologist examined the cases and manually marked the fracture sites. Ten
patients were positive for vertebral body fractures. The total number of spatially distinct
fracture sites was 21, among the 10 patients. The remaining 8 patients had no evidence
of vertebral body fracture. The average running time is 5.6 minutes.",No,Nothing is mentioned,No,"1 Radiology and Imaging Sciences Department, Clinical Center, The National Institutes
of Health, Bethesda, MD 20892
2 Department of Radiological Sciences, University of California, Irvine School of Medicine",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
17/10/2022 08.36.09,234,2012,Yes,It was accurately labelled,"In clinical practice, physicians often exploit previously observed
patterns in coronary angiograms from similar patients to quickly
assess the state of the disease in a current patient. These assessments
involve visually observed features such as the distance of a junction from
the root and the tortuosity of the arteries. In this paper, we show how
these visual features can be automatically extracted from coronary artery
images and used for finding similar coronary angiograms from a database.
Testing on a large collection has shown the method finds clinically similar
coronary angiograms from patients with similar clinical history.","we show how
these visual features can be automatically extracted from coronary artery
images and used for finding similar coronary angiograms from a database.","X-ray Coronary angiography is a commonly used technique to assess the state
of coronary artery disease (CAD). During assessment, clinicians look for characteristic
visual features, taking into account the overall disease burden, the
complexity of individual lesions, and placing more weight on proximal stenoses
of the coronary arteries. Even though there are quantitative assessment scores
such as the Syntax Score[12], they require manual input of angiographic information.
Thus the clinicians still characterize the disease by ’eyeballing’ on salient
visual features such as lumen variation or the relative thickness of arteries (see
Fig. 1a-c)[5], the distance of the junctions from the root, the number of trifurcations,
etc.","Supervised learning, Relevant Component Analysis","Accuracy, Precision",Yes,Extracting Coronary Artery Segments,No,"From a collection of 1600 runs of X-ray angiography videos from 70 patients,
we applied a keyframe detection method [13] to retain the top 10 key frames
from each run, generating a ground truth test set of 600 images drawn across
multiple patients, viewpoints and coronary arteries.","From a collection of 1600 runs of X-ray angiography videos from 70 patients,
we applied a keyframe detection method [13] to retain the top 10 key frames
from each run, generating a ground truth test set of 600 images drawn across
multiple patients, viewpoints and coronary arteries.",Private,"From a collection of 1600 runs of X-ray angiography videos from 70 patients,
we applied a keyframe detection method [13] to retain the top 10 key frames
from each run, generating a ground truth test set of 600 images drawn across
multiple patients, viewpoints and coronary arteries.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,"1 IBM Almaden Research Center, San Jose, CA, USA
2 Kaiser San Francisco Medical Center, San Francisco, CA, USA",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
16/10/2022 11.09.08,230,2012,Yes,It was accurately labelled,"A classifier capable of handling subjects
with unequal numbers of modalities prevents discarding any subjects, as
is traditionally done, thereby broadening the scope of the classifier to
more severe pathology.","The paper presents a method for learning multimodal classifiers
from datasets in which not all subjects have data from all modalities.","Pattern classification techniques are generating increasing interest in the neuroimaging
community as they are powerful in learning the patterns of pathology from a population, assign a probabilistic score to each subject which characterizes
pathology on an individual basis and aid in assessing treatment in conjunction
with other clinical scores",ensemble based classification framework,"AUC, Accuracy",No,Nothing is mentioned,No,"Our dataset consisted of 138 subjects (42 TD
and 96 ASD), out of which 55 subjects had complete data while others had some
feature missing (60.1% subjects with partial data). 30% of subjects were missing
MMF, 15.7% were missing M100 and 38% were missing DTI. We randomly
picked 112 subjects (51 complete and 61 subjects with partial data, making
54.4% missing data) for training and the other 26 (4 complete and 22 partial
data) as test data.","Our dataset consisted of 138 subjects (42 TD
and 96 ASD), out of which 55 subjects had complete data while others had some
feature missing (60.1% subjects with partial data). 30% of subjects were missing
MMF, 15.7% were missing M100 and 38% were missing DTI. We randomly
picked 112 subjects (51 complete and 61 subjects with partial data, making
54.4% missing data) for training and the other 26 (4 complete and 22 partial
data) as test data.",Private,"Our dataset consisted of 138 subjects (42 TD
and 96 ASD), out of which 55 subjects had complete data while others had some
feature missing (60.1% subjects with partial data). 30% of subjects were missing
MMF, 15.7% were missing M100 and 38% were missing DTI. We randomly
picked 112 subjects (51 complete and 61 subjects with partial data, making
54.4% missing data) for training and the other 26 (4 complete and 22 partial
data) as test data.",Yes,"Dataset and Preprocessing. Our dataset consisted of 138 subjects (42 TD
and 96 ASD), out of which 55 subjects had complete data while others had some
feature missing (60.1% subjects with partial data). 30% of subjects were missing
MMF, 15.7% were missing M100 and 38% were missing DTI. We randomly
picked 112 subjects (51 complete and 61 subjects with partial data, making
54.4% missing data) for training and the other 26 (4 complete and 22 partial
data) as test data.
The MEG recordings were performed using a CTF 275-channel biomagnetometer
with the following protocol: (i) binaural auditory presentation of brief
sinusoidal tone stimuli at 45dB SL.M100 latency was determined from the source
modeled peak of the stimulus-locked average of 100 trials of each token (ii) binaural
auditory presentation of interleaved standard and deviant tone and vowel
tokens (/a/, /u/). Mismatch field (MMF) latency was determined from the subtraction
of superior temporal gyrus (STG) source-modeled responses for each
token as deviant vs. standard. The DTI data were acquired on Siemens 3T VerioTM scanner using the Stejkal Tanner diffusion weighted imaging sequence
(2mm isotropic resolution) with b=1000 mm/s2 and 30 gradient directions.",No,Nothing is mentioned,Yes,For the article,Yes,"1 Section of Biomedical Image Analysis, University of Pennsylvania,
Philadelphia, PA, USA
{Madhura.Ingalhalikar,Ragini.Verma}@uphs.upenn.edu
2 Lurie Family Foundations MEG Imaging Center, Department of Radiology,
Children’s Hospital of Philadelphia, Philadelphia, PA, USA

The authors would like to acknowledge support from the NIH grants: MH092862,
MH079938 and DC008871.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
16/10/2022 10.06.04,227,2012,Yes,It was accurately labelled,"In this work, for the first time, we propose
an automatic 3D SNE detection approach based on random forests,
with a novel formulation of SNE probability that relies on visual context
and anatomical priors. On a 3D-TCUS dataset of 11 PD patients and 11
healthy controls, we demonstrate that our SNE detection approach yields
promising results with a sensitivity and specificity of around 83%.","In this work, for the first time, we propose
an automatic 3D SNE detection approach based on random forests,
with a novel formulation of SNE probability that relies on visual context
and anatomical priors. On a 3D-TCUS dataset of 11 PD patients and 11
healthy controls, we demonstrate that our SNE detection approach yields
promising results with a sensitivity and specificity of around 83%.","Parkinson’s disease (PD) is a neurodegenerative movement
disorder caused by decay of dopaminergic cells in the substantia nigra
(SN), which are basal ganglia residing within the midbrain area. In the
past two decades, transcranial B-mode sonography (TCUS) has emerged
as a viable tool in differential diagnosis of PD and recently has been shown
to have promising potential as a screening technique for early detection of
PD, even before onset of motor symptoms. In TCUS imaging, the degeneration
of SN cells becomes visible as bright and hyper-echogenic speckle
patches (SNE) in the midbrain. Recent research proposes the usage of 3D
ultrasound imaging in order to make the application of the TCUS technique
easier and more objective.",random forest,"Specificity, F1 score, Sensitivity",Yes,"We further assume that we are given a segmentation of the midbrain
M ⊂ Ω, either from a manual expert segmentation or alternatively from the
output of a ROI detection algorithm",No,"In this section, we evaluate our SNE detection approach on the bi-lateral 3DTCUS
dataset volume of 22 subjects, consisting of 11 PD patients and 11
healthy controls.","In this section, we evaluate our SNE detection approach on the bi-lateral 3DTCUS
dataset volume of 22 subjects, consisting of 11 PD patients and 11
healthy controls.",Private,"In this section, we evaluate our SNE detection approach on the bi-lateral 3DTCUS
dataset volume of 22 subjects, consisting of 11 PD patients and 11
healthy controls.",No,"In this section, we evaluate our SNE detection approach on the bi-lateral 3DTCUS
dataset volume of 22 subjects, consisting of 11 PD patients and 11
healthy controls.",No,"In this section, we evaluate our SNE detection approach on the bi-lateral 3DTCUS
dataset volume of 22 subjects, consisting of 11 PD patients and 11
healthy controls.",No,"In this section, we evaluate our SNE detection approach on the bi-lateral 3DTCUS
dataset volume of 22 subjects, consisting of 11 PD patients and 11
healthy controls.",No,"1 Institute of Biomathematics and Biometry, Helmholtz Zentrum M¨unchen, Germany
2 Computer Aided Medical Procedures, Technische Universit¨at M¨unchen, Germany
3 Department of Neurology, Ludwig-Maximilians-University of Munich, Germany

",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
16/10/2022 09.51.56,213,2012,Yes,It was accurately labelled,"We propose a novel approach to detect fusion
and undocking events by first searching for docked vesicles that ‘disappear’ from
the field of view, and then using a diffusion model to classify them as either fusion
or undocking events.","We propose a novel approach to detect fusion
and undocking events by first searching for docked vesicles that ‘disappear’ from
the field of view, and then using a diffusion model to classify them as either fusion
or undocking events.","Fluorescently-tagged proteins located on vesicles can fuse with the
surface membrane (visualised as a ‘puff’) or undock and return back into the bulk
of the cell. Detection and quantitative measurement of these events from timelapse
videos has proven difficult.",own algorithm - cannot find any keywords I relate,"Accuracy, Precision, Recall",No,Nothing is mentioned,No,"Our data
set comprises of four videos1.","Our data
set comprises of four videos1.",Private,"Our data
set comprises of four videos1.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,"1 Department of Computer Science, University of Oxford, UK
2 Department of Computer Science, University of Bristol, UK
3 Department of Biochemistry, University of Bristol, UK",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
16/10/2022 09.42.23,206,2012,Yes,It was accurately labelled,The criterion function of SFFS was the accuracy of the SVM classifier,"We applied the feature analysis method
to a large TCS dataset that is relevant for clinical practice and includes
the variability that is present under real conditions. In order to decrease
the influence to the image properties from the different settings of ultrasound
machine, we propose a local image analysis method using an
invariant scale blob detection for the hyperechogenicity estimation. The
local features are extracted from the detected blobs and the watershed
regions in half of mesencephalon area. The performance of these features
is evaluated by a feature-selection method. The cross validation results
show that the local features could be used for PD detection.","Transcranial sonography (TCS) is a new tool for the diagnosis
of Parkinson’s disease (PD) according to a distinct hyperechogenic
pattern in the substantia nigra (SN) region. However a procedure including
rating scale of SN hyperechogenicity was required for a standard clinical
setting with increased use.",SVM,"Accuracy, Confusion matrix",Yes,"Secondly, based on the manually segmented HoM images which were
marked by the physicians, the suspicious hyperechogenicity areas were localized
by the invariant scale blob detection method.",No,"The experiments were based on three data sets which were obtained with Philips
SONOS 5500 by different examiners. Dataset 1 includes 42 TCS images from 23
PD patients and 36 TCS images from 21 healthy controls. Dataset 2 includes
15 PD TCS images from ten PD patients and eight control images from four
controls. The last dataset consisted of ten PD TCS images from five PD patients
and 27 TCS from 14 controls. Totally, this large dataset includes 67 PD images
from 38 PD patients and 71 control images from 39 healthy subjects.","The experiments were based on three data sets which were obtained with Philips
SONOS 5500 by different examiners. Dataset 1 includes 42 TCS images from 23
PD patients and 36 TCS images from 21 healthy controls. Dataset 2 includes
15 PD TCS images from ten PD patients and eight control images from four
controls. The last dataset consisted of ten PD TCS images from five PD patients
and 27 TCS from 14 controls. Totally, this large dataset includes 67 PD images
from 38 PD patients and 71 control images from 39 healthy subjects.",Private,"The experiments were based on three data sets which were obtained with Philips
SONOS 5500 by different examiners. Dataset 1 includes 42 TCS images from 23
PD patients and 36 TCS images from 21 healthy controls. Dataset 2 includes
15 PD TCS images from ten PD patients and eight control images from four
controls. The last dataset consisted of ten PD TCS images from five PD patients
and 27 TCS from 14 controls. Totally, this large dataset includes 67 PD images
from 38 PD patients and 71 control images from 39 healthy subjects.",No,"The experiments were based on three data sets which were obtained with Philips
SONOS 5500 by different examiners. Dataset 1 includes 42 TCS images from 23
PD patients and 36 TCS images from 21 healthy controls. Dataset 2 includes
15 PD TCS images from ten PD patients and eight control images from four
controls. The last dataset consisted of ten PD TCS images from five PD patients
and 27 TCS from 14 controls. Totally, this large dataset includes 67 PD images
from 38 PD patients and 71 control images from 39 healthy subjects.",No,Nothing is mentioned,No,"Possibly for this article, but hard to tell...

The experiments were based on three data sets which were obtained with Philips
SONOS 5500 by different examiners. Dataset 1 includes 42 TCS images from 23
PD patients and 36 TCS images from 21 healthy controls. Dataset 2 includes
15 PD TCS images from ten PD patients and eight control images from four
controls. The last dataset consisted of ten PD TCS images from five PD patients
and 27 TCS from 14 controls. Totally, this large dataset includes 67 PD images
from 38 PD patients and 71 control images from 39 healthy subjects.",No,"1 Institute for Signal Processing, University of Luebeck, Germany
2 Department of Neurology, University Hospital Schleswig-Holstein, Germany
3 Graduate School, University of Luebeck, Germany",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
16/10/2022 09.34.12,203,2012,Yes,It was accurately labelled,"The benefits of our approach are twofold. First it uses the
reference database for prediction, i.e. to provide potential biomarkers in
a clinical setting. Second it increases statistical power on the new task.
We demonstrate on a set of 18 pairs of functional MRI experimental
conditions that our approach gives good prediction.","we propose to use a database
of images, rather than coordinates, and frame the problem as transfer
learning: learning a discriminant model on a reference task to apply it
to a different but related new task. To facilitate statistical analysis of
small cohorts, we use a sparse discriminant model that selects predictive
voxels on the reference task and thus provides a principled procedure to
define ROIs.","Typical cohorts in brain imaging studies are not large enough
for systematic testing of all the information contained in the images. To
build testable working hypotheses, investigators thus rely on analysis
of previous work, sometimes formalized in a so-called meta-analysis. In
brain imaging, this approach underlies the specification of regions of interest
(ROIs) that are usually selected on the basis of the coordinates of
previously detected effects. In this paper, we propose to use a database
of images, rather than coordinates, and frame the problem as transfer
learning:",logistic regression,Accuracy,No,Nothing is mentioned,No,"We use 3 studies for this meta-analysis. The first study (E1) [11] is composed
of 322 subjects and was designed to assess the inter-subject variability in some
language, calculation, and sensorimotor tasks. The second study (E2) is similar
to the first one in terms of stimuli, but its data was acquired on 35 pairs of twinsubjects.
The last study (E3) [12] characterizes brain regions in charge of the
syntactic and the semantic processing for the language. It was performed with
40 subjects, 20 of which were stimulated by pseudowords (jabberwocky stimuli)
instead of actual meaningful sentences. All the studies were pre-processed and
analyzed with the standard fMRI analysis software SPM5. The data used for this
work are the statistical images resulting from the intra-subject analyses across
the 3 studies. E1 has 34 contrasts images available, E2 56, and E3 28. The raw
images were not always acquired on the same scanner. E1 has data from a 3T
SIEMENS Trio, and a 3T Brucker scanner; E2 data were acquired on a 1.5T
GE Signa; and E3 images come from the same 3T SIEMENS Trio.","We use 3 studies for this meta-analysis. The first study (E1) [11] is composed
of 322 subjects and was designed to assess the inter-subject variability in some
language, calculation, and sensorimotor tasks. The second study (E2) is similar
to the first one in terms of stimuli, but its data was acquired on 35 pairs of twinsubjects.
The last study (E3) [12] characterizes brain regions in charge of the
syntactic and the semantic processing for the language. It was performed with
40 subjects, 20 of which were stimulated by pseudowords (jabberwocky stimuli)
instead of actual meaningful sentences. All the studies were pre-processed and
analyzed with the standard fMRI analysis software SPM5. The data used for this
work are the statistical images resulting from the intra-subject analyses across
the 3 studies. E1 has 34 contrasts images available, E2 56, and E3 28. The raw
images were not always acquired on the same scanner. E1 has data from a 3T
SIEMENS Trio, and a 3T Brucker scanner; E2 data were acquired on a 1.5T
GE Signa; and E3 images come from the same 3T SIEMENS Trio.",Private,"We use 3 studies for this meta-analysis. The first study (E1) [11] is composed
of 322 subjects and was designed to assess the inter-subject variability in some
language, calculation, and sensorimotor tasks. The second study (E2) is similar
to the first one in terms of stimuli, but its data was acquired on 35 pairs of twinsubjects.
The last study (E3) [12] characterizes brain regions in charge of the
syntactic and the semantic processing for the language. It was performed with
40 subjects, 20 of which were stimulated by pseudowords (jabberwocky stimuli)
instead of actual meaningful sentences. All the studies were pre-processed and
analyzed with the standard fMRI analysis software SPM5. The data used for this
work are the statistical images resulting from the intra-subject analyses across
the 3 studies. E1 has 34 contrasts images available, E2 56, and E3 28. The raw
images were not always acquired on the same scanner. E1 has data from a 3T
SIEMENS Trio, and a 3T Brucker scanner; E2 data were acquired on a 1.5T
GE Signa; and E3 images come from the same 3T SIEMENS Trio.",No,"We use 3 studies for this meta-analysis. The first study (E1) [11] is composed
of 322 subjects and was designed to assess the inter-subject variability in some
language, calculation, and sensorimotor tasks. The second study (E2) is similar
to the first one in terms of stimuli, but its data was acquired on 35 pairs of twinsubjects.
The last study (E3) [12] characterizes brain regions in charge of the
syntactic and the semantic processing for the language. It was performed with
40 subjects, 20 of which were stimulated by pseudowords (jabberwocky stimuli)
instead of actual meaningful sentences. All the studies were pre-processed and
analyzed with the standard fMRI analysis software SPM5. The data used for this
work are the statistical images resulting from the intra-subject analyses across
the 3 studies. E1 has 34 contrasts images available, E2 56, and E3 28. The raw
images were not always acquired on the same scanner. E1 has data from a 3T
SIEMENS Trio, and a 3T Brucker scanner; E2 data were acquired on a 1.5T
GE Signa; and E3 images come from the same 3T SIEMENS Trio.",No,"We use 3 studies for this meta-analysis. The first study (E1) [11] is composed
of 322 subjects and was designed to assess the inter-subject variability in some
language, calculation, and sensorimotor tasks. The second study (E2) is similar
to the first one in terms of stimuli, but its data was acquired on 35 pairs of twinsubjects.
The last study (E3) [12] characterizes brain regions in charge of the
syntactic and the semantic processing for the language. It was performed with
40 subjects, 20 of which were stimulated by pseudowords (jabberwocky stimuli)
instead of actual meaningful sentences. All the studies were pre-processed and
analyzed with the standard fMRI analysis software SPM5. The data used for this
work are the statistical images resulting from the intra-subject analyses across
the 3 studies. E1 has 34 contrasts images available, E2 56, and E3 28. The raw
images were not always acquired on the same scanner. E1 has data from a 3T
SIEMENS Trio, and a 3T Brucker scanner; E2 data were acquired on a 1.5T
GE Signa; and E3 images come from the same 3T SIEMENS Trio.",No,Nothing is mentioned,No,"1 Parietal Team, INRIA Saclay-ˆIle-de-France, Saclay, France
yannick.schwartz@inria.fr
2 CEA, DSV, I2BM, Neurospin bˆat 145, 91191 Gif-Sur-Yvette, France
3 INSERM, CEA, Cognitive Neuroimaging Unit, Neurospin Center, France

",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
11/10/2022 12.19.48,194,2012,Yes,It was accurately labelled,"In this paper we propose an auditory stimulation and Near Infra-Red
Spectroscopy (NIRS) hemodynamic changes acquisition protocol for preterm
neonates. This study is designed to assess the specific characteristics of
neurovascular coupling to auditory stimuli in healthy and ill neonate brains. The
method could lead to clinical application in Intra-Ventricular Hemorrhage
(IVH) diagnosis along with other techniques such as EEG. We propose a
realistic head model creation with all useful head structures and brain tissues
including the neonate fontanel for more accurate results from NIRS signals
modeling. We also design a 3D imaging tool for dynamic mapping and analysis
of brain activation onto the cortex surface. Results show significant differences
in oxy-hemoglobin between healthy neonates and subjects with IVH.","We propose a
realistic head model creation with all useful head structures and brain tissues
including the neonate fontanel for more accurate results from NIRS signals
modeling. We also design a 3D imaging tool for dynamic mapping and analysis
of brain activation onto the cortex surface. Results show significant differences
in oxy-hemoglobin between healthy neonates and subjects with IVH.","Low arterial blood oxygenation and abnormal cerebral
blood flow is believed to influence the function of the neonatal brain [1]. Preterm
neonates are at high-risk of IVH because of their lack of ability to regulate cerebral blood
flow and pressure [2].",Multimodality atlas,AUC,Yes,"The fontanel was segmented from the CT-Scan using a
variational level-set method",No,"This study was carried out on two groups; the first one is composed of 12 healthy control
subjects and the second one of 7 ill subjects with IVH of grade III-IV. All subjects of
both groups are preterm neonates of gestational age from 28 to 32 weeks; tested during
their sleep between 2 and 4 days after birth. Subjects were submitted to auditory stimuli
which consist of two digitized syllables /ba/ and /ga/ as in a previous EEG study [9].
Three stimulations conditions were used: the standard one (ST: four /ba/ male); deviant
voice (DV: three /ba/ male, one /ba/ female); and deviant phoneme (DP: three /ba/ male,
one /ga/ male). The four syllables block duration is 4s and total stimulation (20s) is
composed of five consecutive blocks. A newborn special NIRS probe showed in Fig. 1(a)
was designed and consists of two patches containing two detectors and sixteen light
sources in each of them (8 to λ=690nm; 8 to λ=830nm wavelengths). Twenty acquisition
channels, ten per hemisphere, are measured in the configuration showed in Fig. 1(b). We
used a multi-channel frequency domain based optical imaging system (Imagent, ISS Inc.)
for the acquisition of oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (Hb)
changes during auditory stimuli. Values of HbO and Hb and their changes were obtained
using the relation between absorption spectroscopic coefficients of the environment and
chromophore concentrations according to the modified Beer-Lambert law [10] used in
NIRS studies.","This study was carried out on two groups; the first one is composed of 12 healthy control
subjects and the second one of 7 ill subjects with IVH of grade III-IV. All subjects of
both groups are preterm neonates of gestational age from 28 to 32 weeks; tested during
their sleep between 2 and 4 days after birth. Subjects were submitted to auditory stimuli
which consist of two digitized syllables /ba/ and /ga/ as in a previous EEG study [9].
Three stimulations conditions were used: the standard one (ST: four /ba/ male); deviant
voice (DV: three /ba/ male, one /ba/ female); and deviant phoneme (DP: three /ba/ male,
one /ga/ male). The four syllables block duration is 4s and total stimulation (20s) is
composed of five consecutive blocks. A newborn special NIRS probe showed in Fig. 1(a)
was designed and consists of two patches containing two detectors and sixteen light
sources in each of them (8 to λ=690nm; 8 to λ=830nm wavelengths). Twenty acquisition
channels, ten per hemisphere, are measured in the configuration showed in Fig. 1(b). We
used a multi-channel frequency domain based optical imaging system (Imagent, ISS Inc.)
for the acquisition of oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (Hb)
changes during auditory stimuli. Values of HbO and Hb and their changes were obtained
using the relation between absorption spectroscopic coefficients of the environment and
chromophore concentrations according to the modified Beer-Lambert law [10] used in
NIRS studies.",Private,"This study was carried out on two groups; the first one is composed of 12 healthy control
subjects and the second one of 7 ill subjects with IVH of grade III-IV. All subjects of
both groups are preterm neonates of gestational age from 28 to 32 weeks; tested during
their sleep between 2 and 4 days after birth. Subjects were submitted to auditory stimuli
which consist of two digitized syllables /ba/ and /ga/ as in a previous EEG study [9].
Three stimulations conditions were used: the standard one (ST: four /ba/ male); deviant
voice (DV: three /ba/ male, one /ba/ female); and deviant phoneme (DP: three /ba/ male,
one /ga/ male). The four syllables block duration is 4s and total stimulation (20s) is
composed of five consecutive blocks. A newborn special NIRS probe showed in Fig. 1(a)
was designed and consists of two patches containing two detectors and sixteen light
sources in each of them (8 to λ=690nm; 8 to λ=830nm wavelengths). Twenty acquisition
channels, ten per hemisphere, are measured in the configuration showed in Fig. 1(b). We
used a multi-channel frequency domain based optical imaging system (Imagent, ISS Inc.)
for the acquisition of oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (Hb)
changes during auditory stimuli. Values of HbO and Hb and their changes were obtained
using the relation between absorption spectroscopic coefficients of the environment and
chromophore concentrations according to the modified Beer-Lambert law [10] used in
NIRS studies.",Yes,"This study was carried out on two groups; the first one is composed of 12 healthy control
subjects and the second one of 7 ill subjects with IVH of grade III-IV. All subjects of
both groups are preterm neonates of gestational age from 28 to 32 weeks; tested during
their sleep between 2 and 4 days after birth. Subjects were submitted to auditory stimuli
which consist of two digitized syllables /ba/ and /ga/ as in a previous EEG study [9].
Three stimulations conditions were used: the standard one (ST: four /ba/ male); deviant
voice (DV: three /ba/ male, one /ba/ female); and deviant phoneme (DP: three /ba/ male,
one /ga/ male). The four syllables block duration is 4s and total stimulation (20s) is
composed of five consecutive blocks. A newborn special NIRS probe showed in Fig. 1(a)
was designed and consists of two patches containing two detectors and sixteen light
sources in each of them (8 to λ=690nm; 8 to λ=830nm wavelengths). Twenty acquisition
channels, ten per hemisphere, are measured in the configuration showed in Fig. 1(b). We
used a multi-channel frequency domain based optical imaging system (Imagent, ISS Inc.)
for the acquisition of oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (Hb)
changes during auditory stimuli. Values of HbO and Hb and their changes were obtained
using the relation between absorption spectroscopic coefficients of the environment and
chromophore concentrations according to the modified Beer-Lambert law [10] used in
NIRS studies.",No,Nothing is mentioned,Yes,For this article,No,"1 GRAMFC, Inserm U1105, CHU Amiens, University of Picardie Jules Verne, Amiens, France
{marc.fournier,mahdi.mahmoudzadeh,guy.kongolo,reinhard.grebe,
fabrice.wallois}@u-picardie.fr
2 Dept. of Electrical and Electronics Engineering, Shiraz University of Technology, Shiraz, Iran
kazemi@sutech.ac.ir
3 NeuroSpin, CEA, Inserm U992, Cognitive Neuroimaging, University Paris XI, Paris, France
ghislaine.dehaene@cea.fr",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
11/10/2022 10.47.27,163,2012,Yes,It was accurately labelled,"Feature Classification
for Tracking Articulated Surgical Tools","we describe a tracking system which
learns visual feature descriptors as class-specific landmarks on an articulated tool.","Tool tracking is an accepted capability for computer-aided surgical intervention
which has numerous applications, both in robotic and manual
minimally-invasive procedures.",Randomized Trees,Accuracy,Yes,"Although the integral images afford efficient extractions of the covariances, we can
reduce the computations further by initially segmenting the pixels of the image to identify
areas of interest to classify.",No,We experimented on previously collected porcine data from a da Vinci R  surgical robot.,"Overall, we use ∼ 15,000 training samples across the 7 classes.
Couldn't find the size of the test set",Private,Nothing public is referenced,No,Nothing is mentioned,No,"Not relevant, porcine data",Yes,For this article: We begin by collecting data to train our classifier,No,"1 Columbia University, New York NY USA
{areiter,allen}@cs.columbia.edu
2 Intuitive Surgical, Inc, Sunnyvale CA USA
tao.zhao@intusurg.com",No,"Not relevant, porcine data",No,Nothing is mentioned,No,"Not relevant, porcine data",No,Nothing is mentioned,
11/10/2022 10.29.37,152,2012,Yes,It was accurately labelled,"a neural network-based classifier is implemented to test the
effectiveness of the proposed morphological analysis approach","By exploiting the recent developments in Multirow-Detector
Computed Tomography (MDCT) scanner technology, the complex endocardial
surface morphology of the left ventricle is studied and the cardiac segments
affected by coronary arterial stenosis localized via analysis of Computed
Tomography (CT) image data obtained from a 320-MDCT scanner. The nonrigid
endocardial surface data is analyzed using an isometry-invariant Bag-of-
Words (BOW) feature-based approach. The clinical significance of the analysis
in identifying, localizing and quantifying the incidence and extent of coronary
artery disease is investigated. Specifically, the association between the
incidence and extent of coronary artery disease and the alterations in the
endocardial surface morphology is studied. The results of the proposed
approach on 15 normal data sets, and 12 abnormal data sets exhibiting coronary
artery disease with varying levels of severity are presented. Based on the
characterization of the endocardial surface morphology using the Bag-of-Words
features, a neural network-based classifier is implemented to test the
effectiveness of the proposed morphological analysis approach.","The complex morphological structure of the left ventricular
endocardial surface and its relation to the severity of arterial stenosis has not yet
been thoroughly investigated due to the limitations of conventional imaging
techniques.


Since CAD is a leading cause of morbidity and
mortality worldwide, techniques that improve diagnostic and prognostic effectiveness
have a potentially significant clinical impact.",Neural network,Accuracy,Yes,Left Ventricle Segmentation and Meshing,No,"We employed the proposed methods for segmentation, meshing and endocardial
surface shape description on 27 MDCT data sets consisting of 12 data sets from
cardiac patients and 15 data sets from normal subjects","We employed the proposed methods for segmentation, meshing and endocardial
surface shape description on 27 MDCT data sets consisting of 12 data sets from
cardiac patients and 15 data sets from normal subjects",Private,"We employed the proposed methods for segmentation, meshing and endocardial
surface shape description on 27 MDCT data sets consisting of 12 data sets from
cardiac patients and 15 data sets from normal subjects",No,Nothing is mentioned,No,"We employed the proposed methods for segmentation, meshing and endocardial
surface shape description on 27 MDCT data sets consisting of 12 data sets from
cardiac patients and 15 data sets from normal subjects",No,"We employed the proposed methods for segmentation, meshing and endocardial
surface shape description on 27 MDCT data sets consisting of 12 data sets from
cardiac patients and 15 data sets from normal subjects",No,"1 Department of Computer Science, The University of Georgia, Athens, GA, USA
2 Piedmont Heart Institute, Atlanta, GA, USA
3 Stony Brook University Medical Center, Stony Brook, NY, USA",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
11/10/2022 10.23.19,137,2012,Yes,It was accurately labelled,"The experimental results of applying the HCRF classifier on real, multi-centre
clinical trial images acquired from 122 patients with Relapsing Remitting MS
(RRMS) yields a 98% sensitivity rate, 0.66 positive predictive value (PPV) and
an average of 1.55 FP counts per patient when compared to a set of “silver
standard” manual labels attained by expert consensus1","we present an automatic, probabilistic Hierarchical
Conditional Random Field (HCRF) framework for detection of gadenhancing
lesions in brain images of patients with MS.","The detection of gad-enhancing lesions in brain MRI of Multiple
Sclerosis (MS) patients is of great interest since they are important
markers of disease activity. However, many of the enhancing voxels are
associated with normal structures (i.e. blood vessels) or noise in the
MRI, making the detection of gad-enhancing lesions a challenging task.",Conditional Random Fields,Sensitivity,No,"A Conditional Random
Fields (CRF) [1] classifier was recently developed for this task, without relying
on the pre-segmentation of T2w lesions and using only commonly acquired
MRI sequences (",No,"The data was acquired from 122 patients with RRMS as part of a multi center
clinical trial (31 centers). The patients had varying levels of gad-enhancing lesion
loads, located in different areas of the brain WM, and showed varying amounts
of brain atrophy.","The HCRF classifier is trained on 92 randomly selected MRI volumes and
tested on the remaining 30 cases.",Private,"The data was acquired from 122 patients with RRMS as part of a multi center
clinical trial (31 centers). The patients had varying levels of gad-enhancing lesion
loads, located in different areas of the brain WM, and showed varying amounts
of brain atrophy.",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"For this article: The data was acquired from 122 patients with RRMS as part of a multi center
clinical trial (31 centers).",No,"1 Centre for Intelligent Machines, McGill University, Canada
2 Montreal Neurological Institute, McGill University, Canada
3 NeuroRx Research, Montreal, Canada",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
07/10/2022 12.13.48,122,2012,Yes,It was accurately labelled,"This paper presents a method for unsupervised cluster analysis using
multi-edge similarity graphs that combine information from different
modalities. The method alleviates the issues with traditional supervised
classification methods that use diagnostic labels and are therefore unable
to exploit or elucidate the underlying heterogeneity of the dataset under
analysis.","This paper presents a method for unsupervised cluster analysis using
multi-edge similarity graphs that combine information from different
modalities. The method alleviates the issues with traditional supervised
classification methods that use diagnostic labels and are therefore unable
to exploit or elucidate the underlying heterogeneity of the dataset under
analysis.","Classifying subjects based on their underlying pathology, brain structure, behavior
and cognition is an important step towards creating biomarkers. However,
pathologies like ASD and other neuropsychiatric disorders are defined over a
spectrum and the severity of the disease may vary within a population thus
making the data highly heterogeneous. Different modalities, like imaging, neurocognitive
scores etc., may characterize different aspects of this heterogeneity
to different degrees. This paper presents a method for unsupervised cluster analysis
of populations using multi-edge similarity graphs that combine information
of population heterogeneity from different modalities, producing classes that are
more representative of population variability.
Traditional superivised classification methods, utilize predefined diagnostic
labels for the subjects for training [1], [2], and hence new subjects can only be
classified into one of these diagnostic categories, thereby overlooking the underlying
heterogeneity of the pathology.","Graph analysis, Unsupervised learning",Accuracy,Yes,"Cortical parcellation and sub-cortical
segmentation of all the subjects was obtained using Freesurfer [11] on structural
T1 images, and a total of 78 ROI’s were extracted to represent the nodes of the
structural network.",No,"Two separate datasets were used in the unsupervised clustering:
– The SCZ dataset consisted of 29 female controls (CNT) and 23 female age
matched patients with schizophrenia. The DWI images were acquired on
Siemens 3T scanner with b=1000 s/mm2 and 64 gradient directions. Neurocognitive
testing was carried out on all the subjects and the speed and
accuracy of memory, emotion, reasoning, and executive functioning were
recorded.
– The ASD dataset consisted of 33 participants with ASD and 21 age matched
typically developing controls (TD’s). The DWI images were acquired on
Siemens 3T scanner with b=1000 s/mm2 and 30 gradient directions. The
cognitive and psychological tests included verbal IQ, Social Responsiveness
Scale (SRS), Social communication questionnaire (SCQ), Clinical evaluation
of language fundamentals (CELF), Full scale IQ and Autism diagnostic
observation schedule (ADOS) and perceptual reasoning index (PRI).","Two separate datasets were used in the unsupervised clustering:
– The SCZ dataset consisted of 29 female controls (CNT) and 23 female age
matched patients with schizophrenia. The DWI images were acquired on
Siemens 3T scanner with b=1000 s/mm2 and 64 gradient directions. Neurocognitive
testing was carried out on all the subjects and the speed and
accuracy of memory, emotion, reasoning, and executive functioning were
recorded.
– The ASD dataset consisted of 33 participants with ASD and 21 age matched
typically developing controls (TD’s). The DWI images were acquired on
Siemens 3T scanner with b=1000 s/mm2 and 30 gradient directions. The
cognitive and psychological tests included verbal IQ, Social Responsiveness
Scale (SRS), Social communication questionnaire (SCQ), Clinical evaluation
of language fundamentals (CELF), Full scale IQ and Autism diagnostic
observation schedule (ADOS) and perceptual reasoning index (PRI).",Private,"Two separate datasets were used in the unsupervised clustering:
– The SCZ dataset consisted of 29 female controls (CNT) and 23 female age
matched patients with schizophrenia. The DWI images were acquired on
Siemens 3T scanner with b=1000 s/mm2 and 64 gradient directions. Neurocognitive
testing was carried out on all the subjects and the speed and
accuracy of memory, emotion, reasoning, and executive functioning were
recorded.
– The ASD dataset consisted of 33 participants with ASD and 21 age matched
typically developing controls (TD’s). The DWI images were acquired on
Siemens 3T scanner with b=1000 s/mm2 and 30 gradient directions. The
cognitive and psychological tests included verbal IQ, Social Responsiveness
Scale (SRS), Social communication questionnaire (SCQ), Clinical evaluation
of language fundamentals (CELF), Full scale IQ and Autism diagnostic
observation schedule (ADOS) and perceptual reasoning index (PRI).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Section of Biomedical Image Analysis,
University of Pennsylvania, Philadelphia, PA, USA
{Madhura.Ingalhalikar,Ragini.Verma}@uphs.upenn.edu
2 Brain Behavior Laboratory, University of Pennsylvania, Philadelphia, PA, USA
3 Lurie Family Foundation’s MEG Imaging Center, Department of Radiology,
Children’s Hospital of Philadelphia, Philadelphia, PA, USA

The authors would like to acknowledge support from the NIH grants: MH092862,
MH079938 and DC008871.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
07/10/2022 12.05.24,117,2012,Yes,It was accurately labelled,"Our results demonstrate that the constrained sparse network gives better classification
performance than the conventional correlation-based network, indicating
its greater sensitivity to early stage brain pathologies.","we consider a constrained sparse linear regression model associated
with the least absolute shrinkage and selection operator (LASSO). Specifically,
we introduced sparsity into brain connectivity via l1-norm penalization, and ensured
consistent non-zero connections across subjects via l2-norm penalization.
Our results demonstrate that the constrained sparse network gives better classification
performance than the conventional correlation-based network, indicating
its greater sensitivity to early stage brain pathologies.","Mild cognitive impairment (MCI) is difficult to diagnose due to its
subtlety. Recent emergence of advanced network analysis techniques utilizing
resting-state functional Magnetic Resonance Imaging (rs-fMRI) has made the
understanding of neurological disorders more comprehensively at a whole-brain
connectivity level. However, inferring effective brain connectivity from fMRI
data is a challenging task, particularly when the ultimate goal is to obtain good
control-patient classification performance. Incorporating sparsity into connectivity
modeling can potentially produce results that are biologically more meaningful
since most biologically networks are formed by a relatively few number
of connections. However, this constraint, when applied at an individual level, will
degrade classification performance due to inter-subject variability. To address this
problem,",SVM,"AUC, Specificity, Accuracy, Sensitivity",Yes,"The images were then masked with their respective graymatter (GM)
masks, created by segmenting the GM regions from their T1-weighted images to eliminate
the physiological noise caused by cardiac and respiratory cycles in white matter
and cerebrospinal fluid [17].",No,"Resting-state fMRI (rs-fMRI) scans of 12 MCI patients and 25 healthy controls were
acquired using a 3 Tesla (Signa EXCITE, GE) scanner with the following parameters:
TR/TE = 2000/32 ms, flip angle = 77◦, imaging matrix = 64 × 64, FOV = 256 ×
256 mm2, 34 slices, 150 volumes, and voxel thickness = 4 mm.","Resting-state fMRI (rs-fMRI) scans of 12 MCI patients and 25 healthy controls were
acquired using a 3 Tesla (Signa EXCITE, GE) scanner with the following parameters:
TR/TE = 2000/32 ms, flip angle = 77◦, imaging matrix = 64 × 64, FOV = 256 ×
256 mm2, 34 slices, 150 volumes, and voxel thickness = 4 mm.",Private,"Resting-state fMRI (rs-fMRI) scans of 12 MCI patients and 25 healthy controls were
acquired using a 3 Tesla (Signa EXCITE, GE) scanner with the following parameters:
TR/TE = 2000/32 ms, flip angle = 77◦, imaging matrix = 64 × 64, FOV = 256 ×
256 mm2, 34 slices, 150 volumes, and voxel thickness = 4 mm.",Yes,"Resting-state fMRI (rs-fMRI) scans of 12 MCI patients and 25 healthy controls were
acquired using a 3 Tesla (Signa EXCITE, GE) scanner with the following parameters:
TR/TE = 2000/32 ms, flip angle = 77◦, imaging matrix = 64 × 64, FOV = 256 ×
256 mm2, 34 slices, 150 volumes, and voxel thickness = 4 mm.",Yes,"Informed consent was obtained from all subjects, and the
experimental protocols were approved by the institutional ethics board. Confirmation
of diagnosis for all subjects was made via expert consensus panels. Demographic and
clinical information of the participants is provided in Table 1.
214 C.-Y. Wee et al.
Table 1. Demographic and clinical information of the participants
Group MCI Control p-value
No. of subjects (Male/Female) 6/6 9/16 -
Age (mean ± SD) 75.0 ± 8.0 72.9 ± 7.9 0.3598a
Years of education (mean ± SD) 18.0 ± 4.1 15.8 ± 2.4 0.0491a
MMSE (mean ± SD) 28.5 ± 1.5b 29.3 ± 1.1 0.1201a",Yes,For this article,No,"Department of Radiology and BRIC,
University of North Carolina at Chapel Hill, NC, USA
2 Brain Imaging and Analysis Center,
Duke University Medical Center, Durham, NC, USA",Yes,"Informed consent was obtained from all subjects, and the
experimental protocols were approved by the institutional ethics board. ",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Informed consent was obtained from all subjects, and the
experimental protocols were approved by the institutional ethics board. ","WOW, that's a first!"
07/10/2022 11.52.29,97,2012,Yes,It was accurately labelled,"We then measure the accuracy score of our encoding
by training a linear classifier, which outperforms the same classifier based
on volumetric measurements.","we adapt spectral signatures for capturing morphological
changes over time. Advanced techniques for capturing temporal shape changes
frequently rely on first registering the sequence of shapes and then analyzing the
corresponding set of high dimensional deformation maps. Instead, we propose a
simple encoding motivated by the observation that small shape deformations lead
to minor refinements in the spectral signature composed of the eigenvalues of
the Laplace operator. The proposed encoding does not require registration, since
spectral signatures are invariant to pose changes. We apply our representation to
the shapes of the ventricles extracted from 22 cine MR scans of healthy controls
and Tetralogy of Fallot patients. We then measure the accuracy score of our encoding
by training a linear classifier, which outperforms the same classifier based
on volumetric measurements.","Capturing the shape and function of anatomy through volumetric measurements extracted
from 4D medical scans has become of central importance in diagnosing diseases.
For example, cardiologists rely on ejection fraction extracted from ultrasound
or cine MR scans to assess patients. These volumetric measurements, however, are not
sensitive enough to aid the diagnosis of many focal or diffuse cardiac diseases. In this
paper, we introduce a new encoding of the shape and its temporal changes based on
the spectral signature and show that this encoding is more sensitive for comparing two
shapes and their temporal dynamics than volumetric measurements.",SVM,"Accuracy, Precision, Recall",Yes,"A
medical expert then semi-automatically segmented the blood pool of the right ventricle
andmyocardium of the left ventricle at the end-diastole (ED) timepoint using ‘Segment’
[15] with manual corrections of the results.",No,"Our dataset includes the cine MR scans of 11 TOF
cases and 11 healthy volunteers (K=22)","Our dataset includes the cine MR scans of 11 TOF
cases and 11 healthy volunteers (K=22)",Private,"Our dataset includes the cine MR scans of 11 TOF
cases and 11 healthy volunteers (K=22)",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"For this article: We would like to thank DongHye Ye for his help on generating
the cardiac dataset. ",Yes,"Dept. of Radiology, University of Pennsylvania, Philadelphia, PA 19104, USA
2 Microsoft Research, Cambridge, CB3 0FB, UK
3 Dept. of Computer Science, Rutgers University, Piscataway, NJ 08854, USA

We would like to thank Dong Hye Ye for his help on generating
the cardiac dataset. This project was supported in part by Grant Number UL1RR024134
and by the Institute for Translational Medicine and Therapeutics’ (ITMAT) Transdisciplinary
Program.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
07/10/2022 11.37.42,88,2012,Yes,It was accurately labelled,"Population based pattern analysis and classification for
quantifying structural and functional differences between diverse groups
has been shown to be a powerful tool for the study of a number of
diseases, and is quite commonly used especially in neuroimaging.","we show that null distributions ordinarily obtained
by permutation tests using SVMs can be analytically approximated from
the data. The analytical computation takes a small fraction of the time
it takes to do an actual permutation test, thereby rendering it possible
to quickly create statistical significance maps derived from SVMs. Such
maps are critical for understanding imaging patterns of group differences
and interpreting which anatomical regions are important in determining
the classifier’s decision.","The dominant approach
addressing this problem involves performing independent statistical testing either
pixel/voxel-wise [1] or regions of interest (ROI-wise) in the image. It has
been argued that such univariate analysis might miss group difference patterns
that span multiple voxels or regions [2]. Hence, replacing univariate methods by
multivariate methods such as SVMs [3] [4][5] has been discussed in literature.
However, unlike univariate methods [1], SVMs do not naturally provide statistical
tests (and corresponding p-values) associated with every voxel/region of
an image. Permutation testing has been suggested for interpreting SVM output
for such high dimensional data [6]. However, performing these tests is time
consuming and computationally costly",SVM,Accuracy,No,Nothing is mentioned,No,"Simulated data was generated as follows 1) grey matter tissue
density maps were generated from brain images of 152 normal subjects 2)simulated
brain shrinkage was introduced in the right frontal lobe of half of these
images by a localized reduction in intensity of the corresponding TDMs. The
vectorized TDM corresponding to each subject forms the superlong vector xi of
section 2.1.
Experiment 2: 278 TDMs were generated. This dataset contained 152 controls
and 126 Alzheimer’s patients.","Simulated data was generated as follows 1) grey matter tissue
density maps were generated from brain images of 152 normal subjects 2)simulated
brain shrinkage was introduced in the right frontal lobe of half of these
images by a localized reduction in intensity of the corresponding TDMs. The
vectorized TDM corresponding to each subject forms the superlong vector xi of
section 2.1.
Experiment 2: 278 TDMs were generated. This dataset contained 152 controls
and 126 Alzheimer’s patients.",Private,"Simulated data was generated as follows 1) grey matter tissue
density maps were generated from brain images of 152 normal subjects 2)simulated
brain shrinkage was introduced in the right frontal lobe of half of these
images by a localized reduction in intensity of the corresponding TDMs. The
vectorized TDM corresponding to each subject forms the superlong vector xi of
section 2.1.
Experiment 2: 278 TDMs were generated. This dataset contained 152 controls
and 126 Alzheimer’s patients.",No,"Simulated data was generated as follows 1) grey matter tissue
density maps were generated from brain images of 152 normal subjects 2)simulated
brain shrinkage was introduced in the right frontal lobe of half of these
images by a localized reduction in intensity of the corresponding TDMs. The
vectorized TDM corresponding to each subject forms the superlong vector xi of
section 2.1.
Experiment 2: 278 TDMs were generated. This dataset contained 152 controls
and 126 Alzheimer’s patients.",No,"Simulated data was generated as follows 1) grey matter tissue
density maps were generated from brain images of 152 normal subjects 2)simulated
brain shrinkage was introduced in the right frontal lobe of half of these
images by a localized reduction in intensity of the corresponding TDMs. The
vectorized TDM corresponding to each subject forms the superlong vector xi of
section 2.1.
Experiment 2: 278 TDMs were generated. This dataset contained 152 controls
and 126 Alzheimer’s patients.",Yes,For this article,No,"Section for Biomedical Image Analysis,
University of Pennsylvania, Philadelphia, PA 19104, USA",No,"Nothing is mentioned, also perhaps not relevant, as simulated data",No,"Nothing is mentioned, also perhaps not relevant, as simulated data",No,"Nothing is mentioned, also perhaps not relevant, as simulated data",No,Nothing is mentioned,"They don't really use accuracy as such, but that is the closest I could come to a performance measure. Also first article I can recall that doesn't have a conclusion...
Simulated data, so the ethics questions 1-3 also a bit fishy"
03/10/2022 09.45.04,53,2012,Yes,It was accurately labelled,"Our method features a novel completely
data-driven approach to breast shape prediction that does not necessitate
prior knowledge about biomechanical properties and parameters of the
breast tissue.","We therefore propose a method for 3D breast decompression
and associated lesion mapping from 3D DBT data.","According to theWorld Cancer Report 2008 (globocan.iarc.fr, 2012/01/23) breast
cancer is the most frequent cancer diagnosis in women among all specifiable
kinds of cancer. Early detection is assumed to significantly improve outcomes.",Random Forest ,Accuracy,Yes,"For this we first segment the breast
tissue area by thresholding and region growing,",No,"Nipple detection has been trained and evaluated on 122 annotated training
data sets, i.e., DBT scans.","Nipple detection has been trained and evaluated on 122 annotated training
data sets, i.e., DBT scans.",Private,"Nipple detection has been trained and evaluated on 122 annotated training
data sets, i.e., DBT scans.",No,Nothing is mentioned,No,Nothing is mentioned,Yes,I believe it was collected for this article,No,"1 Siemens AG, Corporate Technology, Erlangen, Germany
2 University Hospital Erlangen, Department of Radiology, Germany
3 Siemens AG, Healthcare, Erlangen, Germany
4 Siemens Corporation, Corporate Research and Technology, Princeton, NJ, USA  Nothing is mentioned",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
03/10/2022 09.33.27,41,2012,Yes,It was accurately labelled,"We test our method on a
collection of 42 lymphocyte sequences and present results on i) discrimination of
normal and abnormal cellular morphology, ii) local statistical differences between
abnormal and normal shape sequences, and iii) classification of shape sequences
based on the dynamic cellular shape changes from one time point to another.","In this paper we propose a dynamic framework for quantitative analysis of
lymphocyte morphological changes in 2D+t image sequences","Morphological analysis of cells features prominently in a wide range of applications
including digital pathology and is essential for improving our understanding
of the basic physiological processes of organisms.",Learning Vector Quantization,"Specificity, Accuracy, Sensitivity",No,"But mentions the data is segmented prior to use: We represent the segmented cell boundary by 2D
closed, continuously parameterized curve [5–7] c ⊂ R2 given by c(s) : [0, 2π) →
R2.",No,"Our data consists of 42 lymphocyte image sequences (20∼30 seconds) of mice
undergoing back skin transplantation (age: 6-8 weeks, weight 20-22 g) observed
with phase contrast microscopy (Olympus BX51, 0.3 μ resolution, 16 × 1000
magnification). The first group consisted of 21 healthy Balb/C mice as hosts
and 21 healthy Balb/C mice as donors, whereas the second group consisted of
21 healthy Balb/C mice as hosts and 21 healthy C57BL/6 mice as donors. The
lymphocytes were obtained from the blood samples of the 42 hosts collected
from the tail 7 days after the skin transplant.","Our data consists of 42 lymphocyte image sequences (20∼30 seconds) of mice
undergoing back skin transplantation (age: 6-8 weeks, weight 20-22 g) observed
with phase contrast microscopy (Olympus BX51, 0.3 μ resolution, 16 × 1000
magnification). The first group consisted of 21 healthy Balb/C mice as hosts
and 21 healthy Balb/C mice as donors, whereas the second group consisted of
21 healthy Balb/C mice as hosts and 21 healthy C57BL/6 mice as donors. The
lymphocytes were obtained from the blood samples of the 42 hosts collected
from the tail 7 days after the skin transplant.",Private,"Our data consists of 42 lymphocyte image sequences (20∼30 seconds) of mice
undergoing back skin transplantation (age: 6-8 weeks, weight 20-22 g) observed
with phase contrast microscopy (Olympus BX51, 0.3 μ resolution, 16 × 1000
magnification). The first group consisted of 21 healthy Balb/C mice as hosts
and 21 healthy Balb/C mice as donors, whereas the second group consisted of
21 healthy Balb/C mice as hosts and 21 healthy C57BL/6 mice as donors. The
lymphocytes were obtained from the blood samples of the 42 hosts collected
from the tail 7 days after the skin transplant.",No,"Our data consists of 42 lymphocyte image sequences (20∼30 seconds) of mice
undergoing back skin transplantation (age: 6-8 weeks, weight 20-22 g) observed
with phase contrast microscopy (Olympus BX51, 0.3 μ resolution, 16 × 1000
magnification). The first group consisted of 21 healthy Balb/C mice as hosts
and 21 healthy Balb/C mice as donors, whereas the second group consisted of
21 healthy Balb/C mice as hosts and 21 healthy C57BL/6 mice as donors. The
lymphocytes were obtained from the blood samples of the 42 hosts collected
from the tail 7 days after the skin transplant.",No,"Also not relevant, tested on mice",Yes,For the article,Yes,"1 School of Information and Electronics, Beijing Inst. of Tech., Beijing, China
2 School of Computer Sci. and Engineering, Arizona State University, Tempe, USA
3 Department of General Surgery, Beijing You’An Hospital, Beijing, China
4 Laboratory of Neuro Imaging, UCLA School of Medicine, Los Angeles, CA, USA   This work is sponsored by the National Natural Science Foundation of China
(60971133) and the China Scholarship Council. anxing@bit.edu.cn",No,"Not relevant, mice testing",No,Nothing is mentioned,No,"Not relevant, mice testing",No,Nothing mentioned,
30/09/2022 11.17.42,38,2012,Yes,It was accurately labelled,"In the context of a blind source separation
of the underlying colour, we arrive at intrinsic melanin and hemoglobin
images, whose properties are then used in supervised learning to achieve excellent
malignant vs. benign skin lesion classification.","In this paper we propose a new log-chromaticity 2-D colour space,
an extension of previous approaches, which succeeds in removing confounding
factors from dermoscopic images: (i) the effects of the particular camera characteristics
for the camera system used in forming RGB images; (ii) the colour
of the light used in the dermoscope; (iii) shading induced by imaging non-flat
skin surfaces; (iv) and light intensity, removing the effect of light-intensity falloff
toward the edges of the dermoscopic image. In the context of a blind source separation
of the underlying colour, we arrive at intrinsic melanin and hemoglobin
images, whose properties are then used in supervised learning to achieve excellent
malignant vs. benign skin lesion classification.","The three most common malignant skin cancers are basal cell carcinoma (BCC), squamous
cell carcinoma (SCC), and melanoma, among which melanoma is the most deadly
with a high increasing rate in most parts of the world. Melanoma is often treatable if
detected in the early stage, particularly before the metastasis phase. Therefore, there is
an increasing demand for computer-aided diagnostic systems to catch early melanomas.
Colour has played a crucial role in the diagnosis of skin lesions by experts in most
clinical methods (see e.g. [1]). For instance, the presence of multiple colours with an
irregular distribution can signal malignancy.
Few studies have investigated the use of colour features representing biological properties
of skin lesions.",Logistic classifier,"AUC, Precision, Recall, F1 score",Yes,"For automatic segmentation of lesions, we found that
using the geometric-mean μ is as good as or better than the state of the art [12] for these
dermoscopic images, in amuch simpler algorithm.",No,"We applied a Logistic classifier to a set of 500 images, with two classes consisting of
malignant (melanoma and BCC) vs. all benign lesions","We applied a Logistic classifier to a set of 500 images, with two classes consisting of
malignant (melanoma and BCC) vs. all benign lesions",Private,"(This is the ONLY description of the dataset...) We applied a Logistic classifier to a set of 500 images, with two classes consisting of
malignant (melanoma and BCC) vs. all benign lesions",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,"School of Computing Science
Simon Fraser University
amadooei@cs.sfu.ca
http://www.cs.sfu.ca/∼amadooei  Nothing is mentioned",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,This article probably has the shortest description of a dataset I've seen so far!
30/09/2022 11.12.41,37,2012,Yes,It was accurately labelled,"Our experimental results show that the proposed method
can effectively identify the location of the foveola, facilitating diagnosis
around this important landmark.","We develop an automated method to determine the foveola
location in macular 3D-OCT images in either healthy or pathological
conditions.","The foveola is an important anatomical landmark for retinal image analysis [1]. It
is located in the center of the macula, responsible for sharp central vision. Several
clinically-relevant indices are measured with respect to the foveola location, such
as the retina’s average thickness, or drusen size within concentric circles around
the foveola [1, 2]. In addition, many macular diseases are best observed around
the foveola, such as macular hole, and age-related macular degeneration [3].
Therefore, the localization of the foveola in retinal images is an important first
step for diagnosis and longitudinal data analysis.",SVM,"mean, median, standard deviation",No,nothing is mentioned,No,"We collected a large sample of 3D SD-OCT macular scans (200x200x1024 or
512x128x1024 protocol, 6x6x2 mm; Cirrus HD-OCT; Carl Zeiss Meditec). Each
scan is then normalized to be 200x200x200 in x, y, z. For each scan, two ophthalmologists
labeled the (x, y) location of the foveola independently. We then
included a total of 170 scans from 170 eyes/126 subjects in which all scans have
good expert labeling agreement (distance ≤ 8 pixels)","We collected a large sample of 3D SD-OCT macular scans (200x200x1024 or
512x128x1024 protocol, 6x6x2 mm; Cirrus HD-OCT; Carl Zeiss Meditec). Each
scan is then normalized to be 200x200x200 in x, y, z. For each scan, two ophthalmologists
labeled the (x, y) location of the foveola independently. We then
included a total of 170 scans from 170 eyes/126 subjects in which all scans have
good expert labeling agreement (distance ≤ 8 pixels)",Private,"We collected a large sample of 3D SD-OCT macular scans (200x200x1024 or
512x128x1024 protocol, 6x6x2 mm; Cirrus HD-OCT; Carl Zeiss Meditec). Each
scan is then normalized to be 200x200x200 in x, y, z. For each scan, two ophthalmologists
labeled the (x, y) location of the foveola independently. We then
included a total of 170 scans from 170 eyes/126 subjects in which all scans have
good expert labeling agreement (distance ≤ 8 pixels)",No,"We collected a large sample of 3D SD-OCT macular scans (200x200x1024 or
512x128x1024 protocol, 6x6x2 mm; Cirrus HD-OCT; Carl Zeiss Meditec). Each
scan is then normalized to be 200x200x200 in x, y, z. For each scan, two ophthalmologists
labeled the (x, y) location of the foveola independently. We then
included a total of 170 scans from 170 eyes/126 subjects in which all scans have
good expert labeling agreement (distance ≤ 8 pixels)",No,Nothing is mentioned,Yes,For the purpose of this article,Yes,"1 College of Computing, Georgia Institute of Technology, Atlanta, GA
2 UPMC Eye Center, University of Pittsburgh School of Medicine, Pittsburgh, PA
3 Department of Bioengineering, University of Pittsburgh, Pittsburgh, PA
4 Intel Science and Technology Center on Embedded Computing, Pittsburgh, PA   This research is supported in part by National Institutes of
Health contracts R01-EY013178 and P30-EY008098, The Eye and Ear Foundation
(Pittsburgh, PA), unrestricted grants from Research to Prevent Blindness,
Inc. (New York, NY), and grants from Intel Labs Pittsburgh (Pittsburgh, PA).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"There is psuedocode included for their algorithm, so partial transparency?","This one is perhaps also unclear if it is classification or not, the performance measures especially seem to indicate perhaps not, more localisation than classification, though they do do classification"
28/09/2022 11.40.00,25,2012,Yes,It was accurately labelled,"Classification of Ambiguous Nerve Fiber
Orientations in 3D Polarized Light Imaging","3D Polarized Light Imaging (3D-PLI) has been shown to
measure the orientation of nerve fibers in post mortem human brains at
ultra high resolution. The 3D orientation in each voxel is obtained as a
pair of angles, the direction angle and the inclination angle with unknown
sign. The sign ambiguity is a major problem for the correct interpretation
of fiber orientation. Measurements from a tiltable specimen stage, that
are highly sensitive to noise, extract information, which allows drawing
conclusions about the true inclination sign. In order to reduce noise, we
propose a global classification of the inclination sign, which combines
measurements with spatial coherence constraints.","Fiber tracts are composed of axons, which connect nerve cells between each
other, and thus transmit information between brain areas. The exact courses
of fiber tracts are still far from being fully understood.",Graph analysis,"Sensitivity, RMSD",No,Nothing is mentioned (but this article is insanely convoluted!),No,"We evaluate our approach on synthetic and human brain
data.","Human Brain Data. Regions in histological sections of three post mortem
brains without pathological findings were selected to demonstrate the different
behavior of all approaches (Fig. 4(b)–(d)).
Synthetic Data. A synthetic data set consisting of a direction image ˜ ϕ and an
inclination image ˜α was created. The structure consists of rounded and crossing
fiber tracts (Fig. 4(a))",Private,The synthetic data they construct themselves and no mention is made of the human brain data,Don't know,"Not sure if someone who understands this could recreate the synthetic data, the human data no",No,Nothing is mentioned,Yes,At least the synthetic data is created for this article,No,"1 Institute of Neuroscience and Medicine (INM-1, INM-4),
Research Center J¨ulich,Germany
2 Department of Physics, University of Wuppertal, Germany
3 Department of Psychiatry, Psychotherapy and Psychosomatics,
RWTH Aachen University, Germany  Nothing is mentioned",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
28/09/2022 10.21.37,19,2012,Yes,It was accurately labelled,"One important contribution of the method is
the provision of an interpretability layer, which is able to explain a particular
classification by visually mapping the most important visual patterns associated
with such classification.","A method for automatic analysis and interpretation of histopathology
images is presented. The method uses a representation of the image data
set based on bag of features histograms built from visual dictionary of Haarbased
patches and a novel visual latent semantic strategy for characterizing the
visual content of a set of images.","This paper presents a new method, ViSAI, for automatic analysis and interpretation of
histopathological images.",probabilistic classification model,"Specificity, Accuracy, Sensitivity",No,Nothing is mentioned,No,"The dataset comprises 10 labeled histopathological cases from St. Jude Children’s Research
Hospital, which 5 are anaplastic and 5 are non-anaplastic","Each slide is a whole
virtual slide of 80000×80000 pixels with one or more cancerous regions with a large
tumoral variability, manually annotated by a neuro-pathologist. For every slide, 750
individual images of 200×200 pixels non-overlapping where extracted uniformly at
random from these cancerous regions, resulting in a database of 7500 different images:
half of them anaplastic.",Private,"The dataset comprises 10 labeled histopathological cases from St. Jude Children’s Research
Hospital, which 5 are anaplastic and 5 are non-anaplastic",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,"BioIngenium Research Group, Universidad Nacional de Colombia, Bogot´a, Colombia
2 Rutgers, Department of Biomedical Engineering, Piscataway, NJ, USA
3 Children Hospital of L.A., Department of Pathology Lab Medicine, Los Angeles, CA, USA
4 St. Jude Children’s Research Hospital from Memphis, TN, USA
5 Penn State College of Medicine, Department of Pathology, Hershey, PA, USA  Nothing is mentioned",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
26/09/2022 10.18.37,14,2012,Yes,It was accurately labelled,"A novel gland segmentation and classification scheme applied
to an H&E histology image of the prostate tissue is proposed. For
gland segmentation, we associate appropriate nuclei objects with each
lumen object to create a gland segment. We further extract 22 features
to describe the structural information and contextual information for
each segment. These features are used to classify a gland segment into
one of the three classes: artifact, normal gland and cancer gland.","A novel gland segmentation and classification scheme applied
to an H&E histology image of the prostate tissue is proposed.","In detecting prostate cancer on a digitized tissue slide, the pathologist relies
on: (i) structural information; glands in a cancer region (cancer glands) appear
to have structural properties (e.g. nuclei abundance, lumen size) different from
glands in a normal region (normal glands) and (ii) contextual information; cancer
glands typically cluster into groups and are of similar shape and size1, while
shape and size of normal glands vary widely. These two sources of information
can be observed in Fig. 1b. Hence, a reasonable approach to assist a pathologist
in finding cancer regions includes segmenting out glandular regions, examining
their structural and contextual information and finally classifying them.",SVM,Accuracy,Yes,"For
gland segmentation, we associate appropriate nuclei objects with each
lumen object to create a gland segment.",No,"The dataset includes 48 images at 5× magnification (average image
size is 900 × 1,500 pixels), which come from 20 patients. Glands in images of
the same patient still have very large variability in structures. Given the pathologist’s
annotation on each image, we manually label 525 artifacts, 931 normal
glands and 1,375 cancer glands to form the (ground truth) gland dataset.","The dataset includes 48 images at 5× magnification (average image
size is 900 × 1,500 pixels), which come from 20 patients. Glands in images of
the same patient still have very large variability in structures. Given the pathologist’s
annotation on each image, we manually label 525 artifacts, 931 normal
glands and 1,375 cancer glands to form the (ground truth) gland dataset.",Private,The article contains no mention of where the dataset comes from,No,No mention in article,No,No mention in article,No,No mention in article,No,"1 Michigan State Unversity, East Lansing, MI 48824, USA
2 Ventana Medical Systems, Inc., Sunnyvale, CA 94085, USA
{nguye231,jain}@cse.msu.edu, anindya.sarkar@ventana.roche.com  No mention in article",No,No mention in article,No,No mention in article,No,No mention in article,Yes,"The article includes a link to a github repo, containing at least some of the code
The article also includes stats for running time and details about the machine it has been run on",
26/09/2022 09.45.32,13,2012,Yes,It was accurately labelled,"We
utilize a fuzzy multi-class modeling using a stochastic expectation maximization
(SEM) algorithm to fit a finite mixture model (FMM) to the
PET image. We then propose a direct estimation formula for TLA and
SUVmean from this multi-class statistical model.","We
utilize a fuzzy multi-class modeling using a stochastic expectation maximization
(SEM) algorithm to fit a finite mixture model (FMM) to the
PET image. We then propose a direct estimation formula for TLA and
SUVmean from this multi-class statistical model.","The aforementioned functional markers computed from the PET image are
corrupted by partial volume effects and acquisition blur. Nonetheless, we recently
proposed a direct statistical estimation method, statistical lesion activity
computation (SLAC), in [6] for computing TLA, in the presence of blur.",Unsupervised learning,Accuracy,No,Not mentioned in the article,No,"To evaluate the performance, an NCAT phantom [11] with a hot liver lesion was
simulated.","To evaluate the performance, an NCAT phantom [11] with a hot liver lesion was
simulated. Realistic FDG uptake values were assigned to the various organs and
tissues of the NCAT phantom. A non spherical tumor (27.67mL) was inserted in
the liver (see Fig. 1). In the tumor, the activity was set to 18.2kBq/cc. The activity
in the liver, spleen, lungs and body was 6.3kBq/cc, 5.5kBq/cc, 0.9kBq/cc
and 2.5kBq/cc respectively. The voxel size used to generate the phantom was
1mm×1mm×1mm. 30 3min scans of the NCAT phantom were simulated using
a Monte Carlo simulator (PET-SORTEO [10]) which models among others the
spatially variant point spread function (PSF) of the ECAT Exact HR+ scanner.
Attenuation and scatter were also modeled. During reconstruction of both
datasets, the system PSF resolution was recovered by modeling as an isotropic
Gaussian with 5mm FWHM. The projection data were reconstructed using the
maximum likelihood expectation maximization (MLEM) algorithm [9] with ordered
subsets. As in clinical routine, 4 iterations over 16 subsets were performed.
The reconstruction voxel size was set to 2mm× 2mm× 2mm. The images were
post-smoothed with 5mm Gaussian FWHM.",Private,It is simulated data generated for this article of a liver lesion with a tumor inserted,Yes,"To evaluate the performance, an NCAT phantom [11] with a hot liver lesion was
simulated. Realistic FDG uptake values were assigned to the various organs and
tissues of the NCAT phantom. A non spherical tumor (27.67mL) was inserted in
the liver (see Fig. 1). In the tumor, the activity was set to 18.2kBq/cc. The activity
in the liver, spleen, lungs and body was 6.3kBq/cc, 5.5kBq/cc, 0.9kBq/cc
and 2.5kBq/cc respectively. The voxel size used to generate the phantom was
1mm×1mm×1mm. 30 3min scans of the NCAT phantom were simulated using
a Monte Carlo simulator (PET-SORTEO [10]) which models among others the
spatially variant point spread function (PSF) of the ECAT Exact HR+ scanner.
Attenuation and scatter were also modeled. During reconstruction of both
datasets, the system PSF resolution was recovered by modeling as an isotropic
Gaussian with 5mm FWHM. The projection data were reconstructed using the
maximum likelihood expectation maximization (MLEM) algorithm [9] with ordered
subsets. As in clinical routine, 4 iterations over 16 subsets were performed.
The reconstruction voxel size was set to 2mm× 2mm× 2mm. The images were
post-smoothed with 5mm Gaussian FWHM.",No,No patients were included as the data was simulated,Yes,It was created for this article,Yes,"1 Medical Imaging Research Center, UZ Leuven, Belgium
2 IBBT-KU Leuven Future Health Department, Belgium
3 Medical Image Computing (ESAT/PSI/MIC)
4 Nuclear Medicine
5 Gastroenterology
3,4,5 KU, Leuven, Belgium
6icoMetrix NV, Leuven, Belgium  The authors gratefully acknowledge the financial support
by KU Leuven’s Concerted Research Action GOA/11/006, IWT - TBM project
070717 and Research Foundation - Flanders (FWO).",No,Because no persons were involved,No,No persons involved,No,No persons involved,No,No persons involved but a very long perhaps useful theory section if one can recreate it from that?,
26/09/2022 09.18.32,9,2012,Yes,It was accurately labelled,"Automatic detection of lung tumors and abnormal lymph
nodes are useful in assisting lung cancer staging. This paper presents a
novel detection method, by first identifying all abnormalities, then differentiating
between lung tumors and abnormal lymph nodes based on their
degree of overlap with the lung field and mediastinum.","our aim of this study is to develop a computerized method to detect
the lung tumors and abnormal lymph nodes from PET-CT thoracic images automatically.","In this work, we propose a new and intuitive idea to the detection problem –
after attempting to detect all abnormalities, if we can identify the actual lung
field (tumors inclusive), then we can differentiate lung tumors and abnormal
lymph nodes based on the degree of overlap between the detected abnormality
and the lung field.",Graph analysis,"Accuracy, Precision, Recall",No,"The PET-CT thoracic images are first preprocessed to remove the background
and soft tissues outside of the lung and mediastinum with morphological operations.
All images are then aligned based on the carina of tracheae, and rescaled
to the same size [4]. Next, the abnormalities are detected by classification of
lung field (L), mediastinum (M) or abnormalities (O) (Fig. 1c), based on PET
uptake values and CT densities.",No,"The experiment is performed on 50 sets of 3D PET-CT thoracic
images from patients with non-small cell lung cancer (NSCLC), provided by
the Royal Prince Alfred Hospital, Sydney.","A total of 54 lung tumors and 35
abnormal lymph nodes are annotated as the ground truth. For each data set,
the contour of lung field is also roughly delineated. Five images representing
the typical cases are selected manually as the training set for both structure
labeling and classification between tumors and lymph nodes. The data sets are
then randomly divided into five sets; and within each set, each image is used as
the testing image, with the other nine as the reference images.",Private,"The experiment is performed on 50 sets of 3D PET-CT thoracic
images from patients with non-small cell lung cancer (NSCLC), provided by
the Royal Prince Alfred Hospital, Sydney.",No,"The experiment is performed on 50 sets of 3D PET-CT thoracic
images from patients with non-small cell lung cancer (NSCLC), provided by
the Royal Prince Alfred Hospital, Sydney.",No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,"Biomedical and Multimedia Information Technology (BMIT) Research Group,
School of Information Technologies, University of Sydney, Australia
2 The Russell H. Morgan Department of Radiology and Radiological Science,
Johns Hopkins University School of Medicine  Nothing is mentioned in the article",No,Nothing is mentioned in the article,No,"They only mention: Lung cancer is currently the leading cause of cancer deaths; and staging plays
a critical role in defining the prognosis and the best treatment approaches",No,Nothing is mentioned on the choice of subjects,No,"There is a lengthy theory section, but no mention of accessing the code or the dataset",
22/09/2022 13.28.08,4,2012,Yes,It was accurately labelled,we propose and evaluate three approaches to surgical gesture classification from video.,surgical gesture analysis,"Most of the prior work on surgical gesture recognition (see, e.g., [4-6]) uses hidden Markov models (HMMs) to analyze kinematic data stored by the robot (…) Overall, our main conclusion is that methods based on video data perform equally well as methods based n kinematic data for a typical surgical training setup."," linear dynamical system, bag of features,  multiple kernel learning",Accuracy,No,"There is no mention of segmentation, but the article does write: ""We assume that each video is segmented into video surgemes"", so the segmentation has been done",Yes,"""For our tests we used the California dataset [3].""","""The dataset consists of three different tasks: suturing (SU, 39 trials), needle passing (NP, 26 trials) and knot tying (KT, 36 trials). Each task is performed by 8 surgeons with different skill levels.""",Private,"There is no mention of a public dataset, and the article writes ""The authors thank Intuitive Surgical and Carol Reiley for providing the dataset""",No,searching for the dataset yields no results,No,"no mention of demographics, though the article does specify that ""Each task is performed by 8 surgeons with different skill levels""",No,the article contains no mention of intent of data collection,Yes,"Center for Imaging Science, Johns Hopkins University  This work was funded by NSF grants 0931805 and 0941362, and by the Talentia Fellowships Programme of the Andalusian Regional Ministry of Economy, Innovation and Science",No, the article contains no information about the surgeons in the dataset or the patients they were presumably operating on,No,"the focus of the article is to promote a new method, mode of analysis, and does not mention any risk or benefit other than the usefulness of this new method",No,there is no mention of this,No,there is no mention of this,
03/10/2022 09.37.26,42,2012,Yes,It was accurately labelled,"In this paper we propose a learning-based method that is general enough
to perform well across different microscopy modalities. Rather than invoking
computationally-intensive segmentation frameworks [1,9], or classifying all image
patches in a sliding-window manner [15], it uses a highly-efficient MSER region
detector [8] to find a broad number of candidate regions to be scored with a
learning-based measure. The non-overlaping subset of those regions with high
similarity to the class of interest can then be selected via dynamic programming,
while the learning can be done within the structured output framework [12].","In this paper we propose a learning-based method that is general enough
to perform well across different microscopy modalities. Rather than invoking
computationally-intensive segmentation frameworks [1,9], or classifying all image
patches in a sliding-window manner [15], it uses a highly-efficient MSER region
detector [8] to find a broad number of candidate regions to be scored with a
learning-based measure. The non-overlaping subset of those regions with high
similarity to the class of interest can then be selected via dynamic programming,
while the learning can be done within the structured output framework [12].","Automatic cell detection is a subject of interest in a wide range of cell-based
studies, as it is the basis of many automatic methods for cell counting, segmentation
and tracking. The broad diversity of cell lines and microscopy imaging
techniques require that cell detection algorithms adapt well to different scenarios.
The difficulty of the problem also increases when the cell density of the
sample is high, as in this case the cell size can vary and cell clumping is usual.
Moreover, in some applications different cell types or other similar structures
can be present in the same image, and in this case the algorithm is required to
detect only the cells of interest, posing a barrier hard to overcome with classical
image processing techniques.",SVM,"Precision, Recall",Yes,"Although the algorithm produces a set of regions, our
aim is to optimize the detection accuracy (and not the segmentation) w.r.t. the
ground truth provided in the form of dots.",No,"Three data sets for cell detection have been used to validate the method
(Figure 1). Firstly, the ICPR 2010 Histopathology Images contest [4], which
consists of 20 images of stained breast cancer tissue. It is required to detect
lymphocyte nuclei, while discriminating them from breast cancer nuclei having
very similar appearance. The second data set comes from [1] and contains 12
fluorescence microscopy images of human embryonic kidney (HEK) cells, where
the detection task is challenging due to the significant intensity variation between
cells across the image, fading boundaries, and frequent cell clumping. The third
data set contains 22 phase-contrast images of cervical cancer cell colonies of the
HeLa cell line, which presents a high variability in cell shapes and sizes.
Three variations of our method are evaluated: (I) direct classification","Three data sets for cell detection have been used to validate the method
(Figure 1). Firstly, the ICPR 2010 Histopathology Images contest [4], which
consists of 20 images of stained breast cancer tissue. It is required to detect
lymphocyte nuclei, while discriminating them from breast cancer nuclei having
very similar appearance. The second data set comes from [1] and contains 12
fluorescence microscopy images of human embryonic kidney (HEK) cells, where
the detection task is challenging due to the significant intensity variation between
cells across the image, fading boundaries, and frequent cell clumping. The third
data set contains 22 phase-contrast images of cervical cancer cell colonies of the
HeLa cell line, which presents a high variability in cell shapes and sizes.
","Private, Public","Three data sets for cell detection have been used to validate the method (Figure 1). Firstly, the ICPR 2010 Histopathology Images contest [4], which consists of 20 images of stained breast cancer tissue. It is required to detect lymphocyte nuclei, while discriminating them from breast cancer nuclei having very similar appearance. The second data set comes from [1] and contains 12 fluorescence microscopy images of human embryonic kidney (HEK) cells, where the detection task is challenging due to the significant intensity variation between cells across the image, fading boundaries, and frequent cell clumping. The third data set contains 22 phase-contrast images of cervical cancer cell colonies of the HeLa cell line, which presents a high variability in cell shapes and sizes. Three variations of our method are evaluated: (I) direct classification",No,"Three data sets for cell detection have been used to validate the method
(Figure 1). Firstly, the ICPR 2010 Histopathology Images contest [4], which
consists of 20 images of stained breast cancer tissue. It is required to detect
lymphocyte nuclei, while discriminating them from breast cancer nuclei having
very similar appearance. The second data set comes from [1] and contains 12
fluorescence microscopy images of human embryonic kidney (HEK) cells, where
the detection task is challenging due to the significant intensity variation between
cells across the image, fading boundaries, and frequent cell clumping. The third
data set contains 22 phase-contrast images of cervical cancer cell colonies of the
HeLa cell line, which presents a high variability in cell shapes and sizes.
Three variations of our method are evaluated: (I) direct classification",No,Nothing is mentioned,No,Nothing is mentioned,Yes,"1 Department of Engineering Science, University of Oxford, U.K.
2 Yandex, Moscow, Russia   We are grateful to Dr. N. Rajpoot, Dr. E. Bernadis, Dr.
B. Vojnovic and Dr. G. Flaccavento for providing cell data sets. Financial support
was provided by the RCUK Centre for Doctoral Training in Healthcare
Innovation (EP/G036861/1) and ERC grant VisRec no. 228180.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
30/09/2022 11.06.10,36,2012,Yes,It was accurately labelled,"There is an increasing demand for automated detection and analysis of
dermoscopy structures and malignancy clues such as streaks in dermoscopy images,
for computer-aided early diagnosis of deadly melanoma. This paper presents
a novel approach for streak detection and visualization on dermoscopic images.
We tackle the detection of streaks by means of ridge and valley estimation. Orientation
estimation and correction is applied to detect low contrast and fuzzy streaks
lines, and candidate streaks are used to classify dermoscopy images into streaks
Absent or Present with the AUC of 90.5% on 300 dermoscopy images.","This paper presents
a novel approach for streak detection and visualization on dermoscopic images.","Melanoma is the most deadly form of skin cancer, yet treatable via excision if detected
early. There is, therefore, a demand to develop computer-aided diagnostic systems to facilitate
the early detection of melanoma.",Logistic classifier,"AUC, Accuracy, Precision, Recall, F1 score",Yes,First the lesion is segmented using Wighton et al.’s method,No,"we evaluated our proposed approach on streak detection
on a set of 300 dermoscopy images, including 105 absent and 195 present. 250
images are chosen randomly from two atlases of dermoscopy [10,1], and 50 images
are taken from experts’ archives with permission.","we evaluated our proposed approach on streak detection
on a set of 300 dermoscopy images, including 105 absent and 195 present. 250
images are chosen randomly from two atlases of dermoscopy [10,1], and 50 images
are taken from experts’ archives with permission.","Private, Public","we evaluated our proposed approach on streak detection
on a set of 300 dermoscopy images, including 105 absent and 195 present. 250
images are chosen randomly from two atlases of dermoscopy [10,1], and 50 images
are taken from experts’ archives with permission. - So I suppose that means mixed, the atlases may be public?",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"1 School of Computing Science, Simon Fraser University, Canada
msa68@sfu.ca
2 Department of Dermatology and Skin Science, University of British Columbia, Canada
3 Cancer Control Research Program, BC Cancer Research Center, Canada   This work was funded by the Canadian NSERC, CIHR-Skin Research
Training Center and a grant from the Canadian Health Research Project (CHRP).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
16/10/2022 09.59.07,215,2012,Yes,It was accurately labelled,"In low-resource areas, the most common method of tuberculosis
(TB) diagnosis is visual identification of rod-shaped TB bacilli
in microscopic images of sputum smears. We present an algorithm for
automated TB detection using images from digital microscopes such as
CellScope [2], a novel, portable device capable of brightfield and fluorescence
microscopy. Automated processing on such platforms could
save lives by bringing healthcare to rural areas with limited access to
laboratory-based diagnostics. Our algorithm applies morphological operations
and template matching with a Gaussian kernel to identify candidate
TB-objects. We characterize these objects using Hu moments,
geometric and photometric features, and histograms of oriented gradients
and then perform support vector machine classification.","e present an algorithm for
automated TB detection using images from digital microscopes such as
CellScope [2], a novel, portable device capable of brightfield and fluorescence
microscopy","Automated processing on such platforms could
save lives by bringing healthcare to rural areas with limited access to
laboratory-based diagnostics",SVM,"Specificity, Precision, Recall, Sensitivity",No,"We propose a TB detection algorithm for FM with three stages: (1) candidate
TB-object identification, (2) feature representation, and (3) discriminative classification.",No,"Our dataset consists of sputum smear slides collected at clinics in Uganda. Fluorescence
images of these smears were taken using CellScope, which has a 0.4NA
objective and an 8-bit monochrome CMOS camera. CellScope gives a Rayleigh
resolution of 0.76μm and is capable of effective magnifications of 2000-3000x.
The CellScope images are 1944x2592 pixels and cover a 640x490μm field of view
at the smear-referenced plane. We use 594 CellScope images (296 TB-positive,
298 TB-negative), which correspond to 290 patients (143 TB-positive, 147 TBnegative).
We have slide-level human reader and culture classification results for
all 290 slides.","Our dataset consists of sputum smear slides collected at clinics in Uganda. Fluorescence
images of these smears were taken using CellScope, which has a 0.4NA
objective and an 8-bit monochrome CMOS camera. CellScope gives a Rayleigh
resolution of 0.76μm and is capable of effective magnifications of 2000-3000x.
The CellScope images are 1944x2592 pixels and cover a 640x490μm field of view
at the smear-referenced plane. We use 594 CellScope images (296 TB-positive,
298 TB-negative), which correspond to 290 patients (143 TB-positive, 147 TBnegative).
We have slide-level human reader and culture classification results for
all 290 slides.",Public,Our dataset and human annotations will be publicly available.,Yes,"Our dataset consists of sputum smear slides collected at clinics in Uganda. Fluorescence
images of these smears were taken using CellScope, which has a 0.4NA
objective and an 8-bit monochrome CMOS camera. CellScope gives a Rayleigh
resolution of 0.76μm and is capable of effective magnifications of 2000-3000x.
The CellScope images are 1944x2592 pixels and cover a 640x490μm field of view
at the smear-referenced plane. We use 594 CellScope images (296 TB-positive,
298 TB-negative), which correspond to 290 patients (143 TB-positive, 147 TBnegative).
We have slide-level human reader and culture classification results for
all 290 slides.",No,Nothing is mentioned,No,"Perhaps for this article: We would like to thank our collaborators at the Mulago
Hospital of Kampala, Uganda, who provided the sputum smears used in this
study.",Yes,"1 UC Berkeley Department of Electrical Engineering and Computer Sciences
2 UC Berkeley Department of Bioengineering
3 UC San Francisco Medical School and San Francisco General Hospital

We would like to thank our collaborators at the Mulago
Hospital of Kampala, Uganda, who provided the sputum smears used in this
study.",No,Nothing is mentioned,Yes,"Though tuberculosis (TB) receives relatively little attention in high-income countries,
it remains the second leading cause of death from infectious disease worldwide
(second only to HIV/AIDS)

Hence, with the advent of low-cost digital microscopy, automated TB diagnosis presents a ready opportunity
for the application of modern computer vision techniques to a real-world,
high-impact problem.

Sputum smear microscopy continues to be by far the
most widely used method of TB diagnosis, suggesting that enhancements to
microscopy-based screening methods could provide significant benefit to large
numbers of TB-burdened communities across the globe.",No,Nothing is mentioned,Yes,"The dataset and code is made available - though no link to where: We will release
our dataset, annotations, and code, which we hope will provide helpful
insights for future approaches to quantitative TB diagnosis.
",
11/10/2022 12.25.55,202,2012,Yes,It was accurately labelled,"we
investigate a tree-guided sparse coding method to identify grouped imaging
features in the brain regions for guiding disease classification and interpretation.","we
investigate a tree-guided sparse coding method to identify grouped imaging
features in the brain regions for guiding disease classification and interpretation.","Neuroimage analysis based on machine learning technologies has
been widely employed to assist the diagnosis of brain diseases such as
Alzheimer's disease and its prodromal stage - mild cognitive impairment. One
of the major problems in brain image analysis involves learning the most
relevant features from a huge set of raw imaging features, which are far more
numerous than the training samples. This makes the tasks of both disease
classification and interpretation extremely challenging. Sparse coding via L1
regularization, such as Lasso, can provide an effective way to select the most
relevant features for alleviating the curse of dimensionality and achieving more
accurate classification. However, the selected features may distribute randomly
throughout the whole brain, although in reality disease-induced abnormal
changes often happen in a few contiguous regions.",SVM,Accuracy,Yes,"Then, each image was segmented
into three brain tissues, i.e., gray matter (GM), white matter (WM), and cerebrospinal
fluid (CSF),",Yes,"We evaluate the proposed classification algorithm with the T1-weighted baseline MR
brain images of 643 subjects, which include 196 AD patients, 220 MCI subjects, and
227 normal controls (NC), randomly selected from Alzheimer's Disease
Neuroimaging Initiative (ADNI) database.","We evaluate the proposed classification algorithm with the T1-weighted baseline MR
brain images of 643 subjects, which include 196 AD patients, 220 MCI subjects, and
227 normal controls (NC), randomly selected from Alzheimer's Disease
Neuroimaging Initiative (ADNI) database.",Public,ADNI,No,Nothing is mentioned,Yes,"Table 1 provides a summary of the
demographic characteristics of the studied subjects (denoted as mean ± standard
deviation). (and shows age and gender as the only demographic information)",No,Nothing is mentioned,Yes,"1 IDEA Lab, Department of Radiology and BRIC,
University of North Carolina at Chapel Hill, USA
2 Department of Instrument Science and Technology,
Shanghai Jiao Tong University, China
3 Department of Computer Science and Engineering,
Nanjing University of Aeronautics and Astronautics, China

This work was partially supported by NIH grants EB006733, EB008374, EB009634, AG041721,
and MH088520, Medical and Engineering Foundation of Shanghai Jiao Tong University (No.
YG2010MS74), and NSFC grants (No. 61005024 and 60875030).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
07/10/2022 11.28.33,62,2012,Yes,It was accurately labelled,"to find discriminative regions
of 3D brain images in the classification of neurodegenerative disease.","We present a novel method of Hierarchical Manifold Learning
which aims to automatically discover regional variations within images.","We present a novel method of Hierarchical Manifold Learning
which aims to automatically discover regional variations within images.","Graph analysis, Hierarchical Manifold Learning",Accuracy,No,Nothing is mentioned,Yes,ADNI,"We have
applied HML to the Alzheimer’s Disease Neuroimaging Initiative (ADNI) [1]
dataset of 429 subjects of size 160 × 192 × 160 mm. This consists of 231 normal
control subjects and 198 subjects with Alzheimer’s disease.",Public,"We have
applied HML to the Alzheimer’s Disease Neuroimaging Initiative (ADNI) [1]
dataset of 429 subjects of size 160 × 192 × 160 mm. This consists of 231 normal
control subjects and 198 subjects with Alzheimer’s disease.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Biomedical Image Analysis Group,
Department of Computing, Imperial College London, London, UK
2 Division of Imaging Sciences and Biomedical Engineering,
King’s College London, London, UK

We thank Marc Modat and M. Jorge Cardoso from the Centre for Medical Image
Computing, University College London, for their advice and assistance. The work is
partially funded under the 7th Framework Programme by the European Commission
(http://cordis.europa.eu/ist/).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
30/09/2022 11.23.46,40,2012,Yes,It was accurately labelled,"We applied a linear Support Vector Machine (SVM) to classify candidate patch
sequences.","we present an image analysis method to
detect apoptosis in time-lapse phase-contrast microscopy, which is nondestructive
imaging.","There have been little-to-no reports of apoptosis detection in phase-contrast
microscopy. To the best of our knowledge, cell death event detection has only
been implicitly performed as a byproduct of cell tracking; i.e., if the trajectory of
a cell terminates during cell tracking, the cell is considered dead. However, this
simple heuristic often yields poor results because many cell traject",SVM,"Precision, Recall",No,Nothing is mentioned,No,"After C2C12 myoblastic stem cells were cultured for one day, Mitomycin C was
added to induce apoptosis. Afterward, three populations were imaged every 5
minutes over 45 hours, resulting in three sets of 540 image frames. As shown in
Fig. 4, most of cells were dead at the last frame. We manually annotated apoptosis
by marking the center of each apoptotic cell after it shrinks and becomes
bright, obtaining 1154 cases in total. The image sequences and ground truths
are available on the first author’s web page (www.cs.cmu.edu/∼seungilh).","After C2C12 myoblastic stem cells were cultured for one day, Mitomycin C was
added to induce apoptosis. Afterward, three populations were imaged every 5
minutes over 45 hours, resulting in three sets of 540 image frames. As shown in
Fig. 4, most of cells were dead at the last frame. We manually annotated apoptosis
by marking the center of each apoptotic cell after it shrinks and becomes
bright, obtaining 1154 cases in total. The image sequences and ground truths
are available on the first author’s web page (www.cs.cmu.edu/∼seungilh).",Public,"The image sequences and ground truths
are available on the first author’s web page (www.cs.cmu.edu/∼seungilh).",Yes,"After C2C12 myoblastic stem cells were cultured for one day, Mitomycin C was
added to induce apoptosis. Afterward, three populations were imaged every 5
minutes over 45 hours, resulting in three sets of 540 image frames. As shown in
Fig. 4, most of cells were dead at the last frame. We manually annotated apoptosis
by marking the center of each apoptotic cell after it shrinks and becomes
bright, obtaining 1154 cases in total. The image sequences and ground truths
are available on the first author’s web page (www.cs.cmu.edu/∼seungilh).",No,"Not relevant as such, uses stem cells (though no mention is made of which and whose stem cells are used of course)",Yes,For the purpose of this article,No,"Robotics Institute, Carnegie Mellon University
{seungilh,hangs,tk}@cs.cmu.edu
2 Department of Orthopedic Surgery, Stanford University
elmerker@stanford.edu  Nothing is mentioned",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,Pseudo code included,
28/09/2022 10.03.14,16,2012,Yes,It was accurately labelled,"With the advent of advanced imaging techniques, genotyping,
and methods to assess clinical and biological progression, there is
a growing need for a unified framework that could exploit information
available from multiple sources to aid diagnosis and the identification
of early signs of Alzheimer’s disease (AD).","We propose a modeling
strategy using supervised feature extraction to optimally combine highdimensional
imaging modalities with several other low-dimensional disease
risk factors. The motivation is to discover new imaging biomarkers
and use them in conjunction with other known biomarkers for prognosis
of individuals at high risk of developing AD. Our framework also has the
ability to assess the relative importance of imaging modalities for predicting
AD conversion.","Mild cognitive impairment (MCI) is an intermediate stage between healthy aging
and dementia. Patients diagnosed with MCI are at high risk of developing
Alzheimer’s disease (AD), but not everyone with MCI will convert. Accurate
prognosis for MCI patients is an important prerequisite for providing the optimal
treatment and management of the disease.",SVM,"AUC, Specificity, Accuracy, Sensitivity",Yes,"Tissue-wise intensity normalization
for white matter, gray matter, and cerebrospinal fluid was performed using the expectation maximization based segmentation followed by the piecewise polynomial
histogram matching algorithm",Yes,"All the baseline and screening T1 weighted, bias-fieldcorrected
and N3 scaled structuralMagnetic Resonance Images were downloaded
from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database.","The baseline subjects that had all the clinical, APOE genotyping, FDG-PET
imaging and MRI imaging data from the ADNI database comprised of a total
of 242 individuals.",Public,ADNI is available upon request,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"University of Utah, Salt Lake City, UT    Data collection and sharing for this project was funded by
the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (NIH Grant U01
AG024904).The research in this paper was supported byNIHgrant 5R01EB007688,
the University of California, San Francisco (NIH grantP41 RR023953),NSF grant
CNS-0751152), and NSF CAREER Grant 1054057.",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
26/09/2022 09.25.02,10,2012,Yes,It was accurately labelled,"In this paper, we propose a novel domain-transfer learning method
for MCI conversion prediction. Different from most existing methods, we
classify MCI-C and MCI-NC with aid from the domain knowledge learned with
AD and NC subjects as auxiliary domain to further improve the classification
performance.","Different from most existing methods, we
classify MCI-C and MCI-NC with aid from the domain knowledge learned with
AD and NC subjects as auxiliary domain to further improve the classification
performance.","Alzheimer’s disease (AD) is the most common form of dementia in elderly people
worldwide. Early diagnosis of AD is very important for possible delay of the disease.
Mild cognitive impairment (MCI) is a prodromal stage of AD, which can be further
categorized into MCI converters (MCI-C) and MCI non-converters (MCI-NC). The
former will convert into AD in follow-up time, while the latter will not convert. Thus,
accurate diagnosis of MCI converters is of great importance.",SVM,"AUC, Specificity, Accuracy, Sensitivity",Yes,"Then, we use the FSL package to segment
each structural MR image into three different tissue types: gray matter (GM), white
matter (WM), and cerebrospinal fluid (CSF).",Yes,"we evaluate the effectiveness of our proposed DTSVM method on
multimodal data, including MRI, PET and CSF, from the AlzheimerÊs disease
Neuroimaging Initiative (ADNI) database.","the baseline ADNI subjects with all corresponding MRI, PET,
and CSF data are included, which leads to a total of 202 subjects (including 51 AD
patients, 99 MCI patients, and 52 normal controls (NC)). For 99 MCI patients, it
includes 43 MCI converters and 56 MCI non-converters. We use 51 AD and 52 NC
subjects as auxiliary domains, and 99 MCI subjects as target domains.",Public,Googling ADNI leads to a webpage where the dataset is available upon request for research,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,Yes,"1 Dept. of Computer Science and Engineering,
Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China
2 Dept. of Radiology and BRIC, University of North Carolina at Chapel Hill, NC 27599  This work was partially supported by NIH grants (EB006733,
EB008374, EB009634, AG041721 and MH088520), NSFC grant (60875030), and
CQKJ (KJ121111).",No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,
22/09/2022 13.43.26,7,2012,Yes,It was accurately labelled,"Second, the classifier learning process does
not rely on pre-labeled training samples, but rather the training samples are extracted
from the test image itself using structural priors on relative cup and disc
positions. Third, we present a classification refinement scheme that utilizes both
structural priors and local context.",We present a superpixel based learning framework based on retinal structure priors for glaucoma diagnosis.,It is critical to detect this degeneration of the optic nerve as early as possible in order to stall its progression,SVM,accuracy,Yes,"In this work, we utilize the stateof-the-art SLIC (Simple Linear Iterative Clustering) algorithm [12] to segment the fundus disc image into compact and nearly uniform superpixels.",Yes,using a large clinical dataset called ORIGA−light,"For testing we use the ORIGA−light dataset, comprised of 168 glaucoma and 482 normal images.",Public,"Is searchable, available upon request",No,"But the dataset is available upon request, so perhaps this information is also",No,only size is mentioned,No,no mention of intent in article,Yes,"1 Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore
2 Microsoft Research Asia, P.R. China
3 School of Computer Engineering, Nanyang Technological University, Singapore
4 Singapore Eye Research Institute, Singapore
5 Department of Ophthalmology, National University of Singapore, Singapore  This work is funded by Singapore A*STAR SERC Grant (092-148-00731)",No,article contains no mention of patients ,No,"no direct mention, simply mentions the method proposed gets a level of ""accuracy is comparable to or higher than the state-of-the-art technique [1], with a speedup factor of tens or hundreds.""",No,no mention of patients,Yes,"no mention of law/transparency - though they do mention ""The settings in [1] are also adopted in this work to facilitate comparisons"", so based on what that says, maybe",
17/10/2022 08.45.32,236,2012,Yes,It was accurately labelled,"The proposed approach is compared with state–of–the–art
texture attributes and shows significant improvement in classification
performance with an average area under receiver operating characteristic
curves of 0.94 for five lung tissue classes.","Abstract. Texture–based computerized analysis of high–resolution
computed tomography images from patients with interstitial lung diseases
is introduced to assist radiologists in image interpretation. The
cornerstone of our approach is to learn lung texture signatures using
a linear combination of N–th order Riesz templates at multiple scales.
The weights of the linear combination are derived from one–versus–all
support vector machines. Steerability and multiscale properties of Riesz
wavelets allow for scale and rotation covariance of the texture descriptors
with infinitesimal precision. Orientations are normalized among texture
instances by locally aligning the Riesz templates, which is carried out
analytically. The proposed approach is compared with state–of–the–art
texture attributes and shows significant improvement in classification
performance with an average area under receiver operating characteristic
curves of 0.94 for five lung tissue classes. The derived lung texture
signatures illustrate optimal class–wise discriminative properties.","Abstract. Texture–based computerized analysis of high–resolution
computed tomography images from patients with interstitial lung diseases
is introduced to assist radiologists in image interpretation. ",SVM,"AUC, confusion matrix",No,Nothing is mentioned,No,"A publicly available dataset of 85 ILD cases with annotated HRCT images is used
to evaluate our approach [12]. Expert annotations were carried out in collaboration
by two radiologists with 15 and 20 years of experience in CT imaging. The
slice thickness is 1mm and the inter–slice distance is 10mm. The images were acquired
with two imaging devices at the Radiology Service of the University Hospitals
of Geneva: a Philips Mx8000 IDT 16 CT Scanner and a General Electric
HiSpeed CT. The five lung tissue classes encountered in most ILDs were chosen
as lung texture classes: healthy (H), emphysema (E), ground glass (G), fibrosis
(F) and micronodules (M). In each annotated slice, 2D hand–drawn regions of
interests (ROIs) are divided into 32×32 square blocks. The visual appearance of
the lung texture classes and their distribution are detailed in Fig. 4.","A publicly available dataset of 85 ILD cases with annotated HRCT images is used
to evaluate our approach [12]. Expert annotations were carried out in collaboration
by two radiologists with 15 and 20 years of experience in CT imaging. The
slice thickness is 1mm and the inter–slice distance is 10mm. The images were acquired
with two imaging devices at the Radiology Service of the University Hospitals
of Geneva: a Philips Mx8000 IDT 16 CT Scanner and a General Electric
HiSpeed CT. The five lung tissue classes encountered in most ILDs were chosen
as lung texture classes: healthy (H), emphysema (E), ground glass (G), fibrosis
(F) and micronodules (M). In each annotated slice, 2D hand–drawn regions of
interests (ROIs) are divided into 32×32 square blocks. The visual appearance of
the lung texture classes and their distribution are detailed in Fig. 4.",Public,"A publicly available dataset of 85 ILD cases with annotated HRCT images is used
to evaluate our approach [12]. Expert annotations were carried out in collaboration
by two radiologists with 15 and 20 years of experience in CT imaging. The
slice thickness is 1mm and the inter–slice distance is 10mm. The images were acquired
with two imaging devices at the Radiology Service of the University Hospitals
of Geneva: a Philips Mx8000 IDT 16 CT Scanner and a General Electric
HiSpeed CT. The five lung tissue classes encountered in most ILDs were chosen
as lung texture classes: healthy (H), emphysema (E), ground glass (G), fibrosis
(F) and micronodules (M). In each annotated slice, 2D hand–drawn regions of
interests (ROIs) are divided into 32×32 square blocks. The visual appearance of
the lung texture classes and their distribution are detailed in Fig. 4.",Yes,"A publicly available dataset of 85 ILD cases with annotated HRCT images is used
to evaluate our approach [12]. Expert annotations were carried out in collaboration
by two radiologists with 15 and 20 years of experience in CT imaging. The
slice thickness is 1mm and the inter–slice distance is 10mm. The images were acquired
with two imaging devices at the Radiology Service of the University Hospitals
of Geneva: a Philips Mx8000 IDT 16 CT Scanner and a General Electric
HiSpeed CT. The five lung tissue classes encountered in most ILDs were chosen
as lung texture classes: healthy (H), emphysema (E), ground glass (G), fibrosis
(F) and micronodules (M). In each annotated slice, 2D hand–drawn regions of
interests (ROIs) are divided into 32×32 square blocks. The visual appearance of
the lung texture classes and their distribution are detailed in Fig. 4.",No,"A publicly available dataset of 85 ILD cases with annotated HRCT images is used
to evaluate our approach [12]. Expert annotations were carried out in collaboration
by two radiologists with 15 and 20 years of experience in CT imaging. The
slice thickness is 1mm and the inter–slice distance is 10mm. The images were acquired
with two imaging devices at the Radiology Service of the University Hospitals
of Geneva: a Philips Mx8000 IDT 16 CT Scanner and a General Electric
HiSpeed CT. The five lung tissue classes encountered in most ILDs were chosen
as lung texture classes: healthy (H), emphysema (E), ground glass (G), fibrosis
(F) and micronodules (M). In each annotated slice, 2D hand–drawn regions of
interests (ROIs) are divided into 32×32 square blocks. The visual appearance of
the lung texture classes and their distribution are detailed in Fig. 4.",No,Nothing is mentioned,Yes,"1 University of Applied Sciences Western Switzerland (HES–SO)
2 University and University Hospitals of Geneva (HUG), Switzerland
3 Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Switzerland

This work was supported by the Swiss National Science
Foundation (grants 205321–130046 and PP00P2–123438), the CIBM, and the
EU in the context of Khresmoi (257528).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,
17/10/2022 09.11.26,247,2012,No,Other medical imaging task,"Abstract. Analyzing geometry of sulcal curves on the human cortical
surface requires a shape representation invariant to Euclidean motion.
We present a novel shape representation that characterizes the shape of
a curve in terms of a coordinate system based on the eigensystem of the
anisotropic Helmholtz equation. This representation has many desirable
properties: stability, uniqueness and invariance to scaling and isometric
transformation. Under this representation, we can find a point-wise shape
distance between curves as well as a bijective smooth point-to-point correspondence.
When the curves are sampled irregularly, we also present
a fast and accurate computational method for solving the eigensystem
using a finite element formulation. This shape representation is used
to find symmetries between corresponding sulcal shapes between cortical
hemispheres. For this purpose, we automatically generate 26 sulcal
curves for 24 subject brains and then compute their invariant shape representation.
Left-right sulcal shape symmetry as measured by the shape
representation’s metric demonstrates the utility of the presented invariant
representation for shape analysis of the cortical folding pattern.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
17/10/2022 09.02.58,242,2012,No,Other medical imaging task,"Abstract. Time-resolved imaging of the thorax or abdominal area is
affected by respiratory motion. Nowadays, one-dimensional respiratory
surrogates are used to estimate the current state of the lung during its
cycle, but with rather poor results. This paper presents a framework to
predict the 3D lung motion based on a patient-specific finite element
model of respiratory mechanics estimated from two CT images at end
of inspiration (EI) and end of expiration (EE). We first segment the
lung, thorax and sub-diaphragm organs automatically using a machinelearning
algorithm. Then, a biomechanical model of the lung, thorax and
sub-diaphragm is employed to compute the 3D respiratory motion. Our
model is driven by thoracic pressures, estimated automatically from the
EE and EI images using a trust-region approach. Finally, lung motion
is predicted by modulating the thoracic pressures. The effectiveness of
our approach is evaluated by predicting lung deformation during exhale
on five DIR-Lab datasets. Several personalization strategies are tested,
showing that an average error of 3.88 ± 1.54mm in predicted landmark
positions can be achieved. Since our approach is generative, it may constitute
a 3D surrogate information for more accurate medical image reconstruction
and patient respiratory analysis.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
17/10/2022 09.01.28,241,2012,No,Other medical imaging task,"Abstract. Dynamic PET imaging provides important spatial-temporal
information for metabolism analysis of organs and tissues, and generates
a great reference for clinical diagnosis and pharmacokinetic analysis.
Due to poor statistical properties of the measurement data in low
count dynamic PET acquisition and disturbances from surrounding tissues,
identifying small lesions inside the human body is still a challenging
issue. The uncertainties in estimating the arterial input function will also
limit the accuracy and reliability of the metabolism analysis of lesions.
Furthermore, the sizes of the patients and the motions during PET acquisition
will yield mismatch against general purpose reconstruction system
matrix, this will also affect the quantitative accuracy of metabolism
analyses of lesions. In this paper, we present a dynamic PET metabolism
analysis framework by defining a patient adaptive system matrix to improve
the lesion metabolism analysis. Both patient size information and
potential small lesions are incorporated by simulations of phantoms of
different sizes and individual point source responses. The new framework
improves the quantitative accuracy of lesion metabolism analysis,
and makes the lesion identification more precisely. The requirement of
accurate input functions is also reduced. Experiments are conducted on
Monte Carlo simulated data set for quantitative analysis and validation,
and on real patient scans for assessment of clinical potential.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
17/10/2022 08.46.32,237,2012,No,Other medical imaging task,"Abstract. In this paper we present a hybrid 0D-3D modeling method to
investigate the hepatic flow in a virtual right lobe hepatectomy (RLH),
the surgical procedure for adult-to-adult living donor liver transplanation
(LDLT). The 3D method is employed to simulate complex 3D flow in
the portal vein, and the 0D model is used to study the systemic hepatic
circulation. In particular, we quantify the flow velocity and wall shear
stress (WSS) in the left portal vein which increase dramatically post-
RLH, and also simulate the essential hepatic distribution features in a
healthy adult pre- and post-procedure. We further predict the arterial
flow in the remnant left liver, which would decrease due to a hepatic
arterial buffer response (HABR) effect. Finally we discuss the physiological
significance of these phenomena, and the potential of this hybrid
modeling approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
17/10/2022 08.29.42,231,2012,No,Other medical imaging task,"Abstract. Diffusion imaging, through the study of water diffusion, allows
for the characterization of brain white matter, both at the population
and individual level. In recent years, it has been employed to detect
brain abnormalities in patients suffering from a disease, e.g. from multiple
sclerosis (MS). State-of-the-art methods usually utilize a database
of matched (age, sex, ...) controls, registered onto a template, to test for
differences in the patient white matter. Such approaches however suffer
from two main drawbacks. First, registration algorithms are prone
to local errors, thereby degrading the comparison results. Second, the
database needs to be large enough to obtain reliable results. However,
in medical imaging, such large databases are hardly available. In this
paper, we propose a new method that addresses these two issues. It relies
on the search for samples in a local neighborhood of each pixel to
increase the size of the database. Then, we propose a new test based
on these samples to perform a voxelwise comparison of a patient image
with respect to a population of controls. We demonstrate on simulated
and real MS patient data how such a framework allows for an improved
detection power and a better robustness and reproducibility, even with
a small database.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
16/10/2022 10.06.50,228,2012,No,Segmentation,"Accurate segmentation of prostate in CT images is important
in image-guided radiotherapy. However, it is difficult to localize the
prostate in CT images due to low image contrast, unpredicted motion
and large appearance variations across different treatment days. To address
these issues, we propose a sparse representation based classification
method to accurately segment the prostate.

So it does use classification, but main goal is segmentation",,,,,,,,,,,,,,,,,,,,,,,,,,,,
16/10/2022 10.00.59,226,2012,No,Other medical imaging task,"The recently proposed Sparse Shape Composition (SSC)
opens a new avenue for shape prior modeling. Instead of assuming any
parametric model of shape statistics, SSC incorporates shape priors onthe-
fly by approximating a shape instance (usually derived from appearance
cues) by a sparse combination of shapes in a training repository.
Theoretically, one can increase the modeling capability of SSC by including
as many training shapes in the repository. However, this strategy confronts
two limitations in practice. First, since SSC involves an iterative
sparse optimization at run-time, the more shape instances contained in
the repository, the less run-time efficiency SSC has. Therefore, a compact
and informative shape dictionary is preferred to a large shape repository.
Second, in medical imaging applications, training shapes seldom come in
one batch. It is very time consuming and sometimes infeasible to reconstruct
the shape dictionary every time new training shapes appear.
In this paper, we propose an online learning method to address these
two limitations. Our method starts from constructing an initial shape
dictionary using the K-SVD algorithm. When new training shapes come,
instead of re-constructing the dictionary from the ground up, we update
the existing one using a block-coordinates descent approach. Using the
dynamically updated dictionary, sparse shape composition can be gracefully
scaled up to model shape priors from a large number of training
shapes without sacrificing run-time efficiency. Our method is validated
on lung localization in X-Ray and cardiac segmentation in MRI time
series. Compared to the original SSC, it shows comparable performance
while being significantly more efficient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
16/10/2022 10.00.01,217,2012,No,Segmentation,"We provide a fully automatic method of segmenting vertebrae
in DXA images.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
16/10/2022 09.44.52,208,2012,No,Other medical imaging task,"Almost classification, they research a new possible biomarker for Alzheimers prediction, but do not directly do classification, more some form of exploration 

Cortical thinning is a widely used and powerful biomarker
for measuring disease progression in Alzheimer’s disease (AD). However,
there has been little work on the effect of atrophy on the cortical folding
patterns. In this study, we examined whether the cortical folding could
be used as a biomarker of AD. Cortical folding metrics were computed
on 678 patients from the Alzheimer’s Disease Neuroimaging Initiative
(ADNI) cohort. For each subject, the boundary between grey matter and
white matter was extracted using a level set technique. At each point on
the boundary two metrics characterising folding, curvedness and shape
index, were generated. Joint histograms using these metrics were calculated
for five regions of interest (ROIs): frontal, temporal, occipital, and
parietal lobes as well as the cingulum. Pixelwise statistical maps were
generated from the joint histograms using permutations tests. In each
ROI, a significant reduction was observed between controls and AD in
areas associated with the sulcal folds, suggesting a sulcal opening associated
with neurodegeneration. When comparing to MCI patients, the
regions of significance were smaller but overlapping with those regions
found comparing controls to AD. It indicates that the differences in cortical
folding are progressive and can be detected before formal diagnosis
of AD. Our preliminary analysis showed a viable signal in the cortical
folding patterns for Alzheimer’s disease that should be explored further.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
16/10/2022 09.37.47,205,2012,No,Other medical imaging task,"A bit in doubt, but none of the expected performance measures were there

Sparse learning has recently received increasing attentions in
neuroimaging research such as brain disease diagnosis and progression. Most
existing studies focus on cross-sectional analysis, i.e., learning a sparse model
based on single time-point of data. However, in some brain imaging
applications, multiple time-points of data are often available, thus longitudinal
analysis can be performed to better uncover the underlying disease progression
patterns. In this paper, we propose a novel temporally-constrained group sparse
learning method aiming for longitudinal analysis with multiple time-points of
data. Specifically, for each time-point, we train a sparse linear regression model
by using the imaging data and the corresponding responses, and further use the
group regularization to group the weights corresponding to the same brain
region across different time-points together. Moreover, to reflect the smooth
changes between adjacent time-points of data, we also include two smoothness
regularization terms into the objective function, i.e., one fused smoothness term
which requires the differences between two successive weight vectors from
adjacent time-points should be small, and another output smoothness term
which requires the differences between outputs of two successive models from
adjacent time-points should also be small. We develop an efficient algorithm to
solve the new objective function with both group-sparsity and smoothness
regularizations. We validate our method through estimation of clinical cognitive
scores using imaging data at multiple time-points which are available in the
Alzheimer’s Disease Neuroimaging Initiative (ADNI) database.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 12.20.57,197,2012,No,Other medical imaging task,"We propose a metamorphic geodesic regression approach approximating
spatial transformations for image time-series while simultaneously
accounting for intensity changes. Such changes occur for example
in magnetic resonance imaging (MRI) studies of the developing brain due
to myelination. To simplify computations we propose an approximate
metamorphic geodesic regression formulation that only requires pairwise
computations of image metamorphoses. The approximated solution is an
appropriately weighted average of initial momenta. To obtain initial momenta
reliably, we develop a shooting method for image metamorphosis.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 11.00.34,189,2012,No,Other medical imaging task,"We propose a method for deformable registration based on
learning the manifolds of individual brain regions. Recent publications
on registration of medical images advocate the use of manifold learning
in order to confine the search space to anatomically plausible deformations.
Existing methods construct manifolds based on a single metric over
the entire image domain thus frequently miss regional brain variations.
We address this issue by first learning manifolds for specific regions and
then computing region-specific deformations from these manifolds. We
then determine deformations for the entire image domain by learning the
global manifold in such a way that it preserves the region-specific deformations.
We evaluate the accuracy of our method by applying it to the
LPBA40 dataset and measuring the overlap of the deformed segmentations.
The result shows significant improvement in registration accuracy
on cortex regions compared to other state of the art methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.58.07,182,2012,No,Other medical imaging task,"Methods that leverage neighbourhood structures in highdimensional
image spaces have recently attracted attention. These approaches
extract information from a new image using its “neighbours” in
the image space equipped with an application-specific distance. Finding
the neighbourhood of a given image is challenging due to large dataset
sizes and costly distance evaluations. Furthermore, automatic neighbourhood
search for a new image is currently not possible when the distance
is based on ground truth annotations. In this article we present a general
and efficient solution to these problems. “Neighbourhood Approximation
Forests” (NAF) is a supervised learning algorithm that approximates the
neighbourhood structure resulting from an arbitrary distance. As NAF
uses only image intensities to infer neighbours it can also be applied
to distances based on ground truth annotations. We demonstrate NAF
in two scenarios: i) choosing neighbours with respect to a deformationbased
distance, and ii) age prediction from brain MRI. The experiments
show NAF’s approximation quality, computational advantages and use
in different contexts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.55.47,177,2012,No,Other medical imaging task,"The geometry of white matter tracts is of increased interest
for a variety of neuroscientific investigations, as it is a feature reflective of
normal neurodevelopment and disease factors that may affect it. In this
paper, we introduce a novel method for computing multi-scale fibre tract
shape and geometry based on the differential geometry of curve sets. By
measuring the variation of a curve’s tangent vector at a given point in all
directions orthogonal to the curve, we obtain a 2D “dispersion distribution
function” at that point. That is, we compute a function on the unit
circle which describes fibre dispersion, or fanning, along each direction
on the circle. Our formulation is then easily incorporated into a continuous
scale-space framework. We illustrate our method on different fibre
tracts and apply it to a population study on hemispheric lateralization
in healthy controls. We conclude with directions for future work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.53.56,174,2012,No,Other medical imaging task,"In this work, we propose an original and efficient approach
to exploit the ability of Compressed Sensing (CS) to recover Diffusion
MRI (dMRI) signals from a limited number of samples while efficiently
recovering important diffusion features such as the Ensemble Average
Propagator (EAP) and the Orientation Distribution Function (ODF).
Some attempts to sparsely represent the diffusion signal have already
been performed. However and contrarly to what has been presented in
CS dMRI, in this work we propose and advocate the use of a well adapted
learned dictionary and show that it leads to a sparser signal estimation
as well as to an efficient reconstruction of very important diffusion
features. We first propose to learn and design a sparse and parametric
dictionary from a set of training diffusion data. Then, we propose a
framework to analytically estimate in closed form two important diffusion
features : the EAP and the ODF. Various experiments on synthetic,
phantom and human brain data have been carried out and promising
results with reduced number of atoms have been obtained on diffusion
signal reconstruction, thus illustrating the added value of our method
over state-of-the-art SHORE and SPF based approaches.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.50.08,169,2012,No,Other medical imaging task,"Image registration is an important tool for imaging validation
studies investigating the effect of underlying focal disease on the
imaging signal. The strength of the conclusions drawn from these analyses
is limited by statistical power. Based on the observation that in
this context, statistical power depends in part on uncertainty arising
from registration error, we derive a power calculation formula relating
registration error, sample size, and the minimum detectable difference
between normal and pathologic regions on imaging. Statistical mappings
between target registration error and fractional overlap metrics are also
derived, and Monte Carlo simulations are used to evaluate the derived
models and test the strength of their assumptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.40.44,162,2012,No,Other medical imaging task,"We propose novel methods for (a) detection of a catheter
in fluoroscopic images and (b) reconstruction of this catheter from two
views. The novelty of (a) is a reduced user interaction and a higher
accuracy. It requires only a single seed point on the catheter in the fluoroscopic
image. Using this starting point, possible parts of the catheter
are detected using a graph search. An evaluation of the detection using
66 clinical fluoroscopic images yielded an average error of 0.7 mm ± 2.0
mm. The novelty of (b) is a better ability to deal with highly curved
objects as it selects an optimal set of point correspondences from two
point sequences describing the catheters in two fluoroscopic images. The
selected correspondences are then used for computation of the 3-D reconstruction.
The evaluation on 33 clinical biplane images yielded an
average backprojection error of 0.4 mm ± 0.6 mm.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.40.07,157,2012,No,Other medical imaging task,"New minimal-invasive interventions such as transcatheter valve procedures
exploit multiple imaging modalities to guide tools (fluoroscopy) and visualize
soft tissue (transesophageal echocardiography (TEE)). Currently, these
complementary modalities are visualized in separate coordinate systems and on
separate monitors creating a challenging clinical workflow. This paper proposes
a novel framework for fusing TEE and fluoroscopy by detecting the pose of the
TEE probe in the fluoroscopic image. Probe pose detection is challenging in
fluoroscopy and conventional computer vision techniques are not well suited.
Current research requires manual initialization or the addition of fiducials. The
main contribution of this paper is autonomous six DoF pose detection by combining
discriminative learning techniques with a fast binary template library.
The pose estimation problem is reformulated to incrementally detect pose parameters
by exploiting natural invariances in the image. The theoretical contribution
of this paper is validated on synthetic, phantom and in vivo data. The
practical application of this technique is supported by accurate results (< 5 mm
in-plane error) and computation time of 0.5s",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.32.54,154,2012,No,Other medical imaging task,"Am unsure about this one - looks almost like classification of water/fat, but doesn't have performance measures that match anything",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.30.28,153,2012,No,Segmentation,"Prior-Based Automatic Segmentation of the
Carotid Artery Lumen in TOF MRA (PASCAL)",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.24.45,150,2012,No,Other medical imaging task,"Coarctation of the aorta (CoA), is a congenital defect
characterized by a severe narrowing of the aorta, usually distal to the
aortic arch. The treatment options include surgical repair, stent implantation,
and balloon angioplasty. In order to evaluate the physiological
significance of the pre-operative coarctation and to assess the
post-operative results, the hemodynamic analysis is usually performed
by measuring the pressure gradient (P) across the coarctation site via
invasive cardiac catheterization.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.13.21,130,2012,No,Other medical imaging task,"Current methods in high angular resolution diffusion imaging
(HARDI) estimate the probability density function of water diffusion
as a continuous-valued orientation distribution function (ODF) on
the sphere. However, such methods could produce an ODF with negative
values, because they enforce non-negativity only at finitely many
directions. In this paper, we propose to enforce non-negativity on the
continuous domain by enforcing the positive semi-definiteness of Toeplitzlike
matrices constructed from the spherical harmonic representation of
the ODF. We study the distribution of the eigenvalues of these
matrices and use it to derive an iterative semi-definite program that
enforces non-negativity on the continuous domain. We illustrate the performance
of our method and compare it",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.12.09,128,2012,No,Other medical imaging task,"Diffusion MRI measures micron scale displacement of water
molecules, providing unique insight into microstructural tissue architecture.
However, current practical image resolution is in the millimeter
scale, and thus diffusivities from many tissue compartments are averaged
in each voxel, reducing the sensitivity and specificity of the measurement
to subtle pathologies. Recent studies have pointed out that eliminating
the contribution of extracellular water increases the sensitivity of the
diffusion measures to tissue architecture. Moreover, in brain imaging,
estimation of the extracellular volume appears to indicate pathological
processes such as atrophy, edema and neuroinflammation. Here we study
the free-water method, which assumes a bi-tensor model. We add low bvalue
shells to a regular DTI acquisition and present methods to improve
the estimation of the model parameters using the extra information. In
addition, we define a Laplace-Beltrami regularization operator that further
stabilizes the multi-shell estimation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.10.48,126,2012,No,Other medical imaging task,"Diffusion spectrum imaging (DSI) from multiple diffusionweighted
images (DWI) allows to image the complex geometry of water
diffusion in biological tissue. To capture the structure of DSI data, we
propose to use sparse coding constrained by physical properties of the
signal, namely symmetry and positivity, to learn a dictionary of diffusion
profiles. Given this estimated model of the signal, we can extract
better estimates of the signal from noisy measurements and also speed
up acquisition by reducing the number of acquired DWI while giving
access to high resolution DSI data. The method learns jointly for all the
acquired DWI and scales to full brain data. Working with two sets of
515 DWI images acquired on two different subjects we show that using
just half of the data (258 DWI) we can better predict the other 257 DWI
than the classic symmetry procedure. The observation holds even if the
diffusion profiles are estimated on a different subject dataset from an
undersampled q-space of 40 measurements.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
11/10/2022 10.09.05,123,2012,No,Other medical imaging task,"In this work we explore a new
framework where these sources of information can be propagated to morphologically
dissimilar images by diffusing and mapping the information
through intermediate steps. The spatially variant data embedding uses
the local morphology and intensity similarity between images to diffuse
the information only between locally similar images. This framework can
thus be used to propagate any information from any group of subject
to every other subject in a database with great accuracy. Comparison
to state-of-the-art propagation methods showed highly statistically significant
(p < 10−4) improvements in accuracy when propagating both
structural parcelations and brain segmentations geodesically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 12.09.34,121,2012,No,Other medical imaging task,Something to improve frmi datasets sensitivity/specificity,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 12.07.36,119,2012,No,Other medical imaging task,No methods/perf measures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 12.06.32,118,2012,No,Other medical imaging task,None of the methods/perf measures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.56.15,106,2012,No,Other medical imaging task,"Analytic Regularization of Uniform Cubic
B-spline Deformation Fields",,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.55.19,105,2012,No,Other medical imaging task,A Novel Approach for Global Lung Registration,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.53.57,103,2012,No,Other medical imaging task,None of the obvious methods/perf measures,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.47.45,96,2012,No,Other medical imaging task,"Cardiac Mechanical Parameter Calibration
Based on the Unscented Transform",,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.41.18,90,2012,No,Other medical imaging task,"Definitely classification adjacent, but contains no methods/performance measures I would expect from classification",,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.39.17,89,2012,No,Other medical imaging task,"Analysis of Longitudinal Shape Variability
via Subject Specific Growth Modeling",,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.29.32,82,2012,No,Other medical imaging task,"Localization of Sparse Transmural Excitation
Stimuli from Surface Mapping",,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.22.16,61,2012,No,Segmentation,"Combining CRF and Multi-hypothesis Detection
for Accurate Lesion Segmentation
in Breast Sonograms",,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.21.16,59,2012,No,Other medical imaging task,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07/10/2022 11.19.36,55,2012,No,Other medical imaging task,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
03/10/2022 09.40.04,52,2012,No,Other medical imaging task,"Towards Intra-operative PET for Head
and Neck Cancer: Lymph Node Localization
Using High-Energy Probes",,,,,,,,,,,,,,,,,,,,,,,,,,,,
03/10/2022 09.39.15,51,2012,No,Other medical imaging task,"However, our target applications for this model are registration and reconstruction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
30/09/2022 10.58.09,33,2012,No,I don't know,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Unsure about this one, doesn't have the performance measures I would expect, but also don't understand what they are really doing and how.."
28/09/2022 11.44.17,31,2012,No,Other medical imaging task,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"One I understand the basics of - trying to sample from MRI's to get better data faster, but not directly anything to do with classification I would say"
28/09/2022 11.41.48,28,2012,No,Other medical imaging task,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28/09/2022 11.32.22,20,2012,No,I don't know,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No clue what this one is about :D
28/09/2022 10.14.36,17,2012,No,Other medical imaging task,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Detection not classification
28/09/2022 09.54.15,15,2012,No,I don't know,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Don't know what this is, not classification I think...."
26/09/2022 09.33.42,12,2012,No,Other medical imaging task,"It does not use the same performance measures, and aims to predict the outcome of facial deformation post surgery, so not really a diagnosis or a clear classification problem either.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,
25/10/2022 13.40.13,113,2012,No,Other medical imaging task,"Abstract. We present a new method for the estimation of the next brain
MR scan in a longitudinal tumor follow-up study. Our method effectively
incorporates information of the past scans in the time series to predict
the future scan of the patient. Its advantages are that it requires no user
intervention and does not assume any particular tumor growth model.
Instead, the patient-specific tumor growth parameters are estimated individually
from the past patient scans. To validate our method, we conducted
an experimental study on four patients with Optic Path Gliomas
(OPGs) and four patients with glioblastomas multiforma (GBM), each
scanned at five time points. The tumor volumes in the predicted and actual
future scans, both segmented by an expert radiologist, yield a mean
volume overlap difference of 13.65% for the OPG patients, and 34.23%
for the GBM patients.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
25/10/2022 13.40.49,116,2012,No,Other medical imaging task,"Abstract. In this paper, anatomical development is modeled as a collection
of distinctive image patterns localized in space and time. A Bayesian
posterior probability is defined over a random variable of subject age,
conditioned on data in the form of scale-invariant image features. The
model is automatically learned from a large set of images exhibiting significant
variation, used to discover anatomical structure related to age
and development, and fit to new images to predict age. The model is
applied to a set of 230 infant structural MRIs of 92 subjects acquired at
multiple sites over an age range of 8-590 days. Experiments demonstrate
that the model can be used to identify age-related anatomical structure,
and to predict the age of new subjects with an average error of 72 days.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
25/10/2022 13.41.47,239,2012,No,Other medical imaging task,"Abstract. Arterial Spin Labeling (ASL) enables measuring cerebral
blood flow in MRI without injection of a contrast agent. Perfusion measured
by ASL carries relevant information for patients suffering from
pathologies associated with singular perfusion patterns. However, to date,
individual identification of abnormal perfusion patterns in ASL usually
relies on visual inspection or manual delineation of regions of interest.
In this paper, we introduce a new framework to automatically outline
patterns of abnormal perfusion in individual patients by means of
an ASL template. We compare two models of normal perfusion and assess
the quality of detections comparing an a contrario approach to the
Generalized Linear Model (GLM).",,,,,,,,,,,,,,,,,,,,,,,,,,,,