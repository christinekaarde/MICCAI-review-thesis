{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac42d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf259c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab29b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to find lines in html document with DOI and titles of articles\n",
    "def has_doi(href):\n",
    "    return href and re.compile(\"chapter/\").search(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08810aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main mining function returning the initial dataframe \n",
    "\n",
    "def mining(html_doc, year, current_page, all_pages, part):\n",
    "    #opening the html document (copy pasted and saved as a .doc file)\n",
    "    doc = open(html_doc, \"r\", encoding = \"utf-8\") \n",
    "    soup = BeautifulSoup(doc, 'html.parser' )\n",
    "\n",
    "    list_of_doi = soup.find_all(href=has_doi)\n",
    "        \n",
    "    #getting the titles and the doi's from list generated helper function\n",
    "\n",
    "    titles = []\n",
    "    doi_str = []\n",
    "\n",
    "    for element in list_of_doi:\n",
    "        titles.append(element.get_text()) #returns the titles as the only text in the list\n",
    "        string = str(element)\n",
    "        first_substring = '/chapter'\n",
    "        second_substring ='\">'\n",
    "        #separates out the DOIS (added the +9 to remove /chapter/ from the beginning of all DOIS)\n",
    "        doi_str.append(string[(string.find(first_substring)+9):string.find(second_substring)]) \n",
    "            \n",
    "    ## now the lines containing author are found\n",
    "    authors = soup.find_all(\"li\", class_=\"c-author-list__item\")\n",
    "        \n",
    "    #keeping only the author names\n",
    "    authors_str = []\n",
    "    for element in authors:\n",
    "        string = str(element)\n",
    "        first_substring = 'item\">'\n",
    "        second_substring ='</li>'\n",
    "        authors_str.append(string[(string.find(first_substring)+6):string.find(second_substring)])\n",
    "            \n",
    "    #now the lines containing page numbers are found\n",
    "    page_numbers= soup.find_all('div', class_ = \"c-meta\")\n",
    "        \n",
    "    #keeping only the page numbers\n",
    "    page_numbers_str = []\n",
    "\n",
    "    for element in page_numbers:\n",
    "        string = element.get_text()[6:-1] #removes the white spaces and \"Pages \"\n",
    "        both = string.split(\"-\")\n",
    "        if 'x' not in string: #filtering out front matters\n",
    "            try: \n",
    "                if int(both[1])-int(both[0]) > 1: \n",
    "                    page_numbers_str.append(string)\n",
    "            except:\n",
    "                if \"C\" in string or \"E\" in string: #including corrections and erratum, are removed later\n",
    "                    page_numbers_str.append(string)\n",
    "    #filtering out back matters, only an issue in 2021 \n",
    "    if year == 2021 and int(current_page) == int(all_pages):\n",
    "        page_numbers_str = page_numbers_str[:-1]\n",
    "                \n",
    "            \n",
    "    #need to create a list of the year of publication to add to dataframe \n",
    "    year_of_pub = []\n",
    "    for element in titles:\n",
    "        year_of_pub.append(year)\n",
    "        \n",
    "    #will add the part of the publication to the dataframe as well\n",
    "    part_of_pub = []\n",
    "    for element in titles:\n",
    "        part_of_pub.append(part)\n",
    "    \n",
    "    #creating the column names and content for the dataframe        \n",
    "    data = {'Title': titles,\n",
    "        'Authors': authors_str,\n",
    "        'Page numbers' : page_numbers_str,\n",
    "        'DOI': doi_str,\n",
    "        'Year of publication' : year_of_pub,\n",
    "        'Part of publication' : part_of_pub\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768f75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to combine all df together\n",
    "def data_together(data, year):\n",
    "    combined_frame = pd.concat(data, ignore_index = True, sort = False)\n",
    "    if year == 2012:\n",
    "        combined_frame.drop(combined_frame[combined_frame[\"Title\"].str.contains(\"Erratum\")].index, inplace = True)\n",
    "    elif year == 2021:\n",
    "        combined_frame.drop(combined_frame[combined_frame[\"Title\"].str.contains(\"Correction to\")].index, inplace=True)\n",
    "    combined_frame.to_csv('database_miccai_'+ str(year) +'.csv')\n",
    "    return combined_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54e99a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving all 2012 together as one with the helper function above\n",
    "miccai =['miccai 2012 part 1 page 1 of 5.doc', 'miccai 2012 part 1 page 2 of 5.doc', \n",
    "         'miccai 2012 part 1 page 3 of 5.doc' , 'miccai 2012 part 1 page 4 of 5.doc', \n",
    "         'miccai 2012 part 1 page 5 of 5.doc', \n",
    "         \n",
    "         'miccai 2012 part 2 page 1 of 5.doc', \n",
    "         'miccai 2012 part 2 page 2 of 5.doc', 'miccai 2012 part 2 page 3 of 5.doc' ,\n",
    "         'miccai 2012 part 2 page 4 of 5.doc', 'miccai 2012 part 2 page 5 of 5.doc', \n",
    "         \n",
    "         \n",
    "         'miccai 2012 part 3 page 1 of 5.doc', 'miccai 2012 part 3 page 2 of 5.doc', \n",
    "         'miccai 2012 part 3 page 3 of 5.doc' , 'miccai 2012 part 3 page 4 of 5.doc', \n",
    "         'miccai 2012 part 3 page 5 of 5.doc']\n",
    "\n",
    "data = []\n",
    "for element in miccai:  \n",
    "    data.append(mining(element, 2012, element[24:25], element[29:30], element[17:18]))\n",
    "\n",
    "data_together(data, 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bacf7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving all 2021 together as one with the helper function above\n",
    "miccai =['miccai 2021 part 1 page 1 of 4.doc', 'miccai 2021 part 1 page 2 of 4.doc', \n",
    "         'miccai 2021 part 1 page 3 of 4.doc' , 'miccai 2021 part 1 page 4 of 4.doc', \n",
    "         \n",
    "         'miccai 2021 part 2 page 1 of 4.doc', 'miccai 2021 part 2 page 2 of 4.doc', \n",
    "         'miccai 2021 part 2 page 3 of 4.doc' , 'miccai 2021 part 2 page 4 of 4.doc',\n",
    "         \n",
    "         'miccai 2021 part 3 page 1 of 4.doc', 'miccai 2021 part 3 page 2 of 4.doc', \n",
    "         'miccai 2021 part 3 page 3 of 4.doc' , 'miccai 2021 part 3 page 4 of 4.doc',\n",
    "         \n",
    "        'miccai 2021 part 4 page 1 of 4.doc', 'miccai 2021 part 4 page 2 of 4.doc', \n",
    "         'miccai 2021 part 4 page 3 of 4.doc' , 'miccai 2021 part 4 page 4 of 4.doc',\n",
    "         \n",
    "        'miccai 2021 part 5 page 1 of 5.doc', 'miccai 2021 part 5 page 2 of 5.doc', \n",
    "         'miccai 2021 part 5 page 3 of 5.doc' , 'miccai 2021 part 5 page 4 of 5.doc', \n",
    "         'miccai 2021 part 5 page 5 of 5.doc',\n",
    "        \n",
    "        'miccai 2021 part 6 page 1 of 4.doc', 'miccai 2021 part 6 page 2 of 4.doc', \n",
    "         'miccai 2021 part 6 page 3 of 4.doc' , 'miccai 2021 part 6 page 4 of 4.doc',\n",
    "        \n",
    "        'miccai 2021 part 7 page 1 of 5.doc', 'miccai 2021 part 7 page 2 of 5.doc', \n",
    "         'miccai 2021 part 7 page 3 of 5.doc' , 'miccai 2021 part 7 page 4 of 5.doc', \n",
    "         'miccai 2021 part 7 page 5 of 5.doc', \n",
    "        \n",
    "        'miccai 2021 part 8 page 1 of 4.doc', 'miccai 2021 part 8 page 2 of 4.doc',\n",
    "        'miccai 2021 part 8 page 3 of 4.doc', 'miccai 2021 part 8 page 4 of 4.doc']\n",
    "\n",
    "data = []\n",
    "for element in miccai:  \n",
    "    data.append(mining(element, 2021, element[24:25], element[29:30], element[17:18]))\n",
    "data_together(data, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dc2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
