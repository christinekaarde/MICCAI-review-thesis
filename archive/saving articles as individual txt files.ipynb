{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec186a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad072bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1-9'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOT USED\n",
    "df_2012 = pd.read_csv(r\"C:\\Users\\chris\\Desktop\\Value-Analysis-Thesis\\mining test\\miccai_2012_first_wave.csv\")\n",
    "\n",
    "page_number_list = df_2012[\"Page numbers\"]\n",
    "\n",
    "page_number_list = page_number_list.to_list()\n",
    "\n",
    "page_number_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65410836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT USED\n",
    "def save_articles(page_number_list):\n",
    "    counter = 0\n",
    "    for element in page_number_list:\n",
    "        pages = element.split(\"-\")\n",
    "        start_page = int(pages[0])+50\n",
    "        end_page = int(pages[1])+50\n",
    "        article = \"\"\n",
    "        for i in range(end_page-start_page):\n",
    "            if start_page < 100:\n",
    "                page = open(r\"C:\\Users\\chris\\Dropbox\\ITU\\Thesis\\book\\miccia_2012_img-0\" + str(start_page) + \".txt\", \"r\", encoding='utf-8')\n",
    "            else:\n",
    "                page = open(r\"C:\\Users\\chris\\Dropbox\\ITU\\Thesis\\book\\miccia_2012_img-\" + str(start_page) + \".txt\", \"r\", encoding='utf-8')\n",
    "            \n",
    "            article = article + page.read() #I'm hoping this adds next page to it\n",
    "            start_page = start_page + 1\n",
    "        \n",
    "        with open(¨'2012_'+str(counter)+'.txt', 'w') as f:\n",
    "            f.write(article)\n",
    "\n",
    "        counter = counter + 1\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4932711a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Limited Angle C-Arm tomography and Segmentation of the LA 639\\n\\nwith the same angular range as the test image. Thereby, for 150° reconstruc-\\ntions, a segmentation error of about 2mm is obtained. Third, these findings hold\\nfor the whole LAPV plus trachea mesh, but also for the individual anatomical\\nstructures of the mesh, as shown in Table Fourth, if the reconstruction angle\\nis further reduced to 130° or 110°, a segmentation error of about 3mm is ob-\\ntained in matched conditions. Models trained on low reconstruction angles are\\ngenerally more robust in mismatched test conditions, while not being optimal\\nfor large reconstruction angles.\\n\\n_ 6 —— : r\\nE training on\\n& reconstructions from\\n5 200° —+—\\n5 170° —<—\\n5 150° —K—\\n24 130° —E—\\ne 110° Os\\n5\\nBot\\n8\\n3\\n2 2+ _—_.\\noy\\n$\\n® yo n n n\\n200 170 150 130 110\\n\\nangular range used for test reconstruction [degree]\\n\\nFig. 1. Average segmentation error émean of the whole mesh for various training and\\ntest angular ranges\\n\\nTable 2. Average segmentation error of the individual mesh parts in [mm] for various\\ntraining and test angular ranges\\n\\nTest on 200° Test on 170° Test on 150°\\n\\n: Train: Train: Train: Train: : Train:\\n\\n170° 150° 170° 150° 150°\\n\\nWhole mesh 151 1.48 1.59 1.59 1.66 1.98\\nLeft atrium 158 1.49 1.61 1.63 1.76 2.04\\nInf. Left PV 1.87 1.88 2.00 1.83 1.88 2.20\\nInf. Right PV | 1.87 1.81 1.94 1.85 1.77 2.31\\nSup. Left PV 141 1.38 1.65 1.41 1.70 2.08\\nSup. Right PV] 1.67 1.65 1.68 1.61 1.63 | 1.99 1.84\\nTrachea 1.35 136 1.46 1.51 1.52 | 3.00 1.86\\n\\nThese findings are also confirmed by visual inspection of the mesh outline as\\noverlay on angiograms (Fig. 2) and slices of the reconstructed images (Fig. B).\\nThe decreasing depth resolution in anterior-posterior direction can be observed\\nin the transaxial slices for A = 130°,110° resulting in mesh deformations and\\nlarger segmentation errors.\\n640 D. Schafer et al.\\n\\nFig. 2. Oblique view on the joint LAPV and trachea model as wire rendering (left),\\nangiogram of the case shown in Fig. [3] post-processed by local contrast enhancement.\\n(middle), and overlay of corresponding adapted model trained on 200° (right)\\n\\n200° |.0.694mm, 170° |,0.69.mm 150° [,0.82imm\\n\\nFig. 3. Example overlay of segmentation results on the corresponding test data recon-\\nstructions using the trained model from 150° with 10-fold cross-correlation. Transaxial\\nslices (top), sagittal (middle), coronal (bottom). In the top row the angular range A\\nand the error ¢mean is given for the corresponding column.\\n\\n4 Summary and Conclusions\\n\\nA reduction of the angular range from 200° to 170° has only a minimal in-\\nfluence on the segmentation performance (from 1.5mm to 1.6mm). If smaller\\nangular ranges are used in the test images, the decrease in segmentation per-\\nformance can be partly compensated by using a segmentation model trained in\\nLimited Angle C-Arm tomography and Segmentation of the LA 641\\n\\nmatched conditions. Thereby, for 150° reconstructions, a segmentation error of\\nabout 2mm is obtained. Hence, accepting this minor decrease in segmentation\\nperformance, the clinical workflow can be substantially simplified by avoiding\\nlarge C-arm angulations between 60° RAO and 100° RAO.\\n\\nReferences\\n\\n1\\n\\n10.\\n\\n11\\n\\n. Delaney, A.H., Bresler, Y.: Global\\n\\nManzke, R., Meyer, C., Ecabert, O., Peters, J., Noordhock, N.J., Thiagalingam, A.,\\nReddy, V-Y., Chan, R.C., Weese, J.: Automatic Segmentation of Rotational X-Ray\\nImages for Anatomic Intra-Procedural Surface Generation in Atrial Fibrillation\\nAblation Procedures. IEEE Trans. Med. Imag. 29(2), 260-272 (2010)\\n\\nly Convergent Edge-Preserving Regularized Re-\\nconstruction: An Application to Limited-Angle Tomography. IEEE Trans. Imag.\\nProc. 7(2), 204-221 (1998)\\n\\nPersson, M., Bone, D., Elmqvist, H.: Total variation norm for three-dimensional\\niterative reconstruction in limited view angle tomography. Phys. Med. Biol. 46,\\n853-866 (2001)\\n\\nSnyder, D.L., O\\'Sullivan, J.A., Murphy, R.J., Politte, D.G., Whiting, B.R.,\\nWilliamson, J.F.: Image reconstruction for transmission tomography when pro-\\njection data are incomplete. Phys. Med. Biol. 51, 5603-5619 (2006)\\n\\nBachar, G., Siewerdsen, J.H., Daly, M.J., Jaffray, D.A., Irish, J.C.: Image quality\\nand localization accuracy in C-arm tomosynthesis-guided head and neck surgery.\\nMed. Phys. 34, 4664-4677 (2007)\\n\\nFeldkamp, L., Davis, L., Kress, J.: Practical cone-beam algorithm. J. Opt. Soc\\nAm. A 1, 612-619 (1984)\\n\\nEcabert, O., Peters, J., Schramm, H., Lorenz, C., von Berg, J., Walker, M., Vembar,\\nM., Olszewski, M., Subramanyan, K., Lavi, G., Weese, J.: Automatic model-based\\nsegmentation of the heart in CT images. IEEE Trans. Med. Imag. 27(9), 1189-1201\\n(2008)\\n\\n. Li, J.H., Haim, M., Movassaghi, B., Mendel, J.B., Chaudhry, G.M., Haffajee, CL,\\n\\nOrlov, M.V.: Segmentation and registration of three-dimensional rotational an-\\ngiogram on live fluoroscopy to guide atrial fibrillation ablation: A new online imag-\\ning tool. Heart Rhythm 6(2), 231-237 (2009)\\n\\n. Saalbach, A., Wachter-Stehle, I., Kneser, R., Mollus, S., Peters, J., Weese, J.:\\n\\nOptimizing GHT-Based Heart Localization in an Automatic Segmentation Chain.\\nIn: Fichtinger, G., Martel, A., Peters, T. (eds.) MIGCAI 2011, Part III, LNCS,\\nvol. 6893, pp. 463-470. Springer, Heidelberg (2011)\\n\\nPeters, J., Ecabert, O., Meyer, C., Kneser, R., Weese, J.: Optimizing boundary de-\\ntection via Simulated Search with applications to multi-modal heart segmentation\\nMedical Image Analysis 14(1), 70-84 (2010)\\n\\nMeyer, C., Manzke, R., Peters, J., Ecabert, O., Kneser, R., Reddy, V.Y.,\\nChan, R.C., Weese, J.: Automatic Intra-operative Generation of Geometric Left\\nAtrium/Pulmonary Vein Models from Rotational X-Ray Angiography. In: Metaxas,\\nD., Axel, L., Fichtinger, G., Székely, G. (eds.) MICCAI 2008, Part II. LNCS,\\nvol. 5242, pp. 61-69. Springer, Heidelberg (2008)\\n\\nZheng, Y., Wang, T., John, M., Zhou, $.K., Boese, J., Comaniciu, D.: Multi-part\\nLeft Atrium Modeling and Segmentation in C-Arm CT Volumes for Atrial Fib-\\nrillation Ablation. In: Fichtinger, G., Martel, A., Peters, T. (eds.) MICCAI 2011,\\nPart III. LNCS, vol. 6893, pp. 487-495. Springer, Heidelberg (2011)\\n\\nAutomatic Non-rigid Temporal Alignment\\nof IVUS Sequences\\n\\nMarina Alberti!*, Simone Balocco!?, Xavier Carrillo®,\\nand Petia Radeval:?*\\n\\n\\' Dept. of Applied Mathematics and Analysis, University of Barcelona, Spain\\n2 Computer Vision Center, Campus UAB, Bellaterra, Barcelona, Spain\\n% University Hospital “Germans Trias i Pujol”, Badalona, Spain\\nmalberti@cvc.uab.es\\n\\nAbstract. Clinical studies on atherosclerosis regression /progression per-\\nformed by Intravascular Ultrasound analysis require the alignment of\\npullbacks of the same patient before and after clinical interventions.\\nIn this paper, a methodology for the automatic alignment of IVUS se-\\nquences based on the Dynamic Time Warping technique is proposed.\\n‘The method is adapted to the specific IVUS alignment task by apply-\\ning the non-rigid alignment technique to multidimensional morphological\\nsignals, and by introducing a sliding window approach together with a\\nregularization term. To show the effectiveness of our method, an exten-\\nsive validation is performed both on synthetic data and in-vivo TVUS\\nsequences. The proposed method is robust to stent deployment and post\\ndilation surgery and reaches an alignment error of approximately 0.7 mm\\nfor in-vivo data, which is comparable to the inter-observer variability.\\n\\n1 Introduction\\n\\nIntravascular Ultrasound (IVUS) is a catheter-based imaging technique used for\\ndiagnostic purposes and for planning and validation of Percutaneous Coronary\\nIntervention (PCI). IVUS sequences are acquired by dragging an ultrasound\\nemitter carried by a catheter, at constant speed, inside the arterial vessel, from\\nthe distal to the proximal position (pullbacks). The image alignment of the same\\nvessel from different IVUS pullbacks is important from a clinical viewpoint. Af-\\nter performing PCI, frame alignment allows physicians to assess the intervention\\noutcome (i.e., evaluate final lumen dimensions and blood flow restoration, de-\\ntect stent malapposition and inspect side-branch occlusion by deployed stent).\\nAt follow-up, it is crucial to monitor restenosis and the evolution of plaque com-\\nposition. Currently, in plaque regression studies, the longitudinal correspondence\\nof coronary artery segments is manually determined by identifying common land-\\nmarks, such as bifurcations {1 [3]. To our knowledge, no method for the automatic\\nalignment of IVUS sequences has been published in literature.\\n\\n* This work is supported by TIN2009-14404-C02 and CONSOLIDER INGENIO CSD\\n2007-00018.\\n\\nN. Ayache et al. (Eds.): MICCAT 2012, Part I, LNCS 7510, pp. 642-650, 2012\\n© Springer-Verlag Berlin Heidelberg 2012\\nAutomatic Non-rigid Temporal Alignment of IVUS Sequences 643\\n\\nPullback 1\\n\\nFests pagar\\n\\n‘Aiea(n)\\n\\nCC) a a aC?\\nTine mr of one ames). Tine (tebe of aa ramen.\\n\\nPullback 2\\n\\nfeed ghenn\\n\\na a a\\nTine arb ated ames) Tee fuer of ated ames.\\n\\n(a) (b) (}\\n\\nFig. 1. Pair of IVUS sequences of the same vessel: (a) longitudinal views, (b) corre-\\nsponding frames, (c) temporal signals describing the pullbacks\\n\\nDespite the constant catheter speed, an automatic alignment is hampered\\neral obstacles: (1) the heart beating causes a non-rigid deformation of\\nthe vessel, (2) the catheter and the heart motion generate acquisition artifacts,\\nsuch as the catheter longitudinal swinging and the roto-translation of successive\\nframes (see Figure [(b)), (3) the catheter can follow different trajectories inside\\nthe vessel, therefore the acquired image sections might not be orthogonal to the\\nvessel walls, (4) the probe can remain stuck in the vessel for some time and\\nthen accelerate, (5) different initial and end spatial positions of the pullbacks\\ncause partial overlapping (see Figure [[a)) and (6) the vessel morphology can\\nsignificantly change after the intervention or evolve at follow-up. Consequently,\\nthere is no one-to-one correspondence between frames of the two pullbacks, thus\\nmaking image-based alignment approaches inaccurate. Hence, instead of using an\\nimage-based description, in this study the morphological content: of the artery is\\nexploited. IVUS sequences\\ni.e., side-branch location, vi\\n\\nby\\n\\nn be described by temporal morphological signals,\\nsel, lumen and plaque areas (see Figure [c)). Thu\\nthe IVUS alignment task is addressed as a temporal alignment problem,\\n\\nIn different applications, such as speech recognition, activity recognition,\\nshape matching, several methods have been developed for non-rigid signal align-\\nment, like Dynamic Time Warping (DTW), Canonical Time Warping (CTW)\\nand Correlation Optimized Warping (COW) (345). DTW [3] minimizes the Eu-\\nclidean distance of corresponding points of the signals. Canonical Time Warping\\n(CTW) [4] improves DTW by combining it with Canonical Correlation Analysis\\n(CCA), thus adding a feature weighting mechanism. The optimization process\\nrequires the application of CCA at suc > iterations. Correlation Optimized\\nWarping (COW) [5] is a piecewise data alignment method which allows limited\\nchanges in segment lengths and performs a segment-wise correlation optimiza-\\ntion by means of dynamic programming\\n\\n644 M. Alberti et al.\\n\\nTo address the non-rigid correspondence between frames, in this paper a\\nDTW-based framework is adapted to the specific clinical task and compared\\nvs. the CTW and COW techniques. To tackle the partial overlapping problem,\\nthe alignment algorithms are integrated into a sliding window approach. More-\\nover, a regularization term is introduced to penalize significant differences in the\\nglobal temporal expansion/compression of IVUS sequences. Two validations are\\npresented, on synthetic data and on 13 pairs of in-vivo sequences (26 pullbacks)\\n\\n2 Method for IVUS Sequences Alignment\\n\\nThe signal alignment framework is based on the DTW technique. To align two\\nsequences X = [21,22,...%,] and Y = [yi,y2,..-Yny], DTW builds a ma-\\ntrix din, xn,)> Where dl (1 <i <ny;1 <j < ny) represents a dissimilarity\\nmeasure between X(é) and Y(7) (3). In the classical DTW formulation, d(i, 3) is\\ncomputed as the Euclidean distance dg (i, j). Successively, the algorithm finds the\\nwarping path (i.e., a mapping of the time axes of X and Y on a common time axis,\\nf = ([i(k), j(kK)] |k =1,...,)) with the minimum cumulative distance (MCD).\\nThe MCD of the path leading to the entry (i,j) is computed by dynamic pro-\\ngramming as: D (i,j) = d (i,j) + min(D (i — 1,3), D(é—1,j — 1), D (i,j - 1).\\nFinally, the matching cost is calculated as ® (X,Y) = argmin DL, d (i(k), j(k))-\\n£\\n\\n2.1 Multidimensional Profiles Extraction\\n\\nProfile Framework: A pair of corresponding IVUS sequences is described by\\ntemporal morphological profiles (i.\\nphological measurements along the vessel) and defined as a pair of time series\\nX €R™\" and Y € R™*\"», of length nz, ny and dimensionality m. The profiles\\nare invariant to frame rotation, thus making the method independent from the\\n\\nsignals describing the evolution of mor-\\n\\ncatheter torsion. The use of multiple features is aimed to increase the robustness\\nwith respect to 1-D alignment. The following morphological measurements are\\nproposed in this study: (1) vessel area, defined as the area inside the media-\\nadventitia border [i], (2) lumen area [7], (3) area of calcified plaque [§], (4) area\\nof fibro-lipidic plaque [8], (5) angular extension of bifurcations [9]. It is worth\\nnoticing that the framework could potentially provide similar results using a\\ndifferent set of profiles and it is independent from the technique employed for\\nthe measurements.\\n\\nGating Preprocessing: To limit the morphological variations due to vessel\\npulsation and catheter swinging effect, an image-based gating algorithm {14|\\nis applied to the pullbacks in order to select the frames belonging to the end-\\ndiastolic phase. The selected images provide coherent morphological measures,\\nsince the arterial tissues are subject to the same blood pressure. Moreover, the\\n\\npreprocessing assures that the frames are consecutive in the direction of the\\ncatheter movement\\nAutomatic Non-rigid Temporal Alignment of IVUS Sequences 645,\\n\\n2.2 IVUS Alignment Framework\\n\\nSliding Window (SW) Approach: The DTW approach suffers from a lim-\\nitation: it attempts to compute a global matching between all the frames of\\nthe two sequences, even in presence of partial overlapping (see Figure [IKa)).\\nHence, the alignment algorithm is integrated into a sliding window approach.\\nThe two sequences are iteratively slided one along the other and for each step\\nthe alignment between the overlapping subsegments is identified. The optimal\\niteration is selected by minimizing a matching cost ®vorm (Xiter, Yiter)\\n\\n® (Xiter, Yiter) /liter, where Xiter and Yiter are the overlapping subsegments and\\nliter is the length of the overlapping window at iteration iter. In order to de-\\ncrease the computational cost, the allowed overlap ranges from ns — Welast to\\nng, where ng is the length of the shortest sequence.\\n\\nRegularization Cost (RC): In the classical version of DTW [8], an arbitrary\\nband constraint is employed to guide the warping path by limiting its acceptable\\ndomain to a band around the diagonal of the dissimilarity matrix d. In this pa-\\nper, a regularization term, described by a continuous formulation, is introduced\\nto favor warping paths closer to the diagonal. Such improvement automatically\\npenalizes an excessive and non-physiological compression/expansion of the two\\nsequences. In the DTW+5W framework, the dissimilarity measure d(i, j) is com-\\nputed as: d(i,j) = dg(i,j) + wr: dr(i,j), where dr(i,j) — j| is the reg-\\nularization cost (see Figure 2(d)) and we represents the penalization weight.\\nThe regularization is aimed to reduce the alignment error when the profiles are\\n\\naffected by noise corruption, as shown in Figure B{a)-(c).\\n\\nPtbock {ote rare rarer)\\n\\nrout ont egition cn ont rion cost\\n\\nee 8 &\\n\\nPutts 2st tame rambo\\n\\nanton ee ee\\nTe (ged tame abet) Time ged tame surbe)\\n\\nTime ged tame surbe)\\n\\n@) () ©) (a)\\n\\nFig. 2. Example of alignment of noise-corrupted signals. In (b) incorrect alignment\\nby classical DTW, in (c) correct alignment by applying RC, in (d) regularization cost\\nmatrix dr, where the red arrows indicate the penalization increase. In (a), (b), (c), blue\\nlines represent the signals and red lines represent correspondences between frames.\\n\\n3 Experimental Results\\n\\nOur dataset consists of 26 in-vivo TVUS sequences from human coronary arteries.\\nThe acquisitions have been performed both preoperatively and after interven-\\ntion (stent deployment, post dilation), by using an iLab IVUS Imaging Syste:\\n(Boston Scientific). The performance of DTW is fairly compared with two other\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOT USED\n",
    "save_articles(page_number_list) #technically worked - can open them on my computer fine, but not on jupyter.. odd!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb30d76",
   "metadata": {},
   "source": [
    "Nope, something is wrong. either I missed something or something from Dad is gibberish - probably me though!\n",
    "The txt I am looking at only has part 1 while the numbers list is for the entire 2012 proceedings on the test I did - so this is why I get 251 txt files and the last ones are gibberish, it starts over on part 1 with page numbers from part 2 and 3. Probably more issues as well, but this is a big one! Have abandoned this approach for now and am continiuing with the one below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b575730",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012 = pd.read_csv(r\"C:\\Users\\chris\\Desktop\\Value-Analysis-Thesis\\mining test\\miccai_2012_first_wave.csv\")\n",
    "\n",
    "title_list = df_2012[\"Title\"]\n",
    "author_list = df_2012[\"Authors\"]\n",
    "\n",
    "author_list = author_list.to_list()\n",
    "title_list = title_list.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bac48938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_from_big_file(place, title_list, author_list):\n",
    "    for i in range(len(title_list)-240): #should be -1 to get all of them, now just want to test\n",
    "        with open(place, \"r\",encoding='utf-8') as part:\n",
    "            article = part.read()\n",
    "            author_first_article = author_list[i].split(\", \")\n",
    "            first_author = author_first_article[0] + \"[0-9]\"  \n",
    "            current_article_author = re.search(first_author, article) #this works and finds the instance \n",
    "            #print(current_article_author.span())\n",
    "            \n",
    "            author_next_article = author_list[i+1].split(\", \")\n",
    "            next_first_author = author_next_article[0] + \"[0-9]\"\n",
    "            next_article_author = re.search(next_first_author, article)\n",
    "            #print(next_article_author.span())\n",
    "            \n",
    "            try:\n",
    "                article = article[current_article_author.span()[0]:next_article_author.span()[0]] \n",
    "        #need to fix title placement!\n",
    "        \n",
    "            #will this search from author to author with numbers? \n",
    "            #yes, works - but only works for those where both authors have numbers that the regex can find, \n",
    "            #otherwise the regex.span() returns nothing for one of the authors and it doens't work of course\n",
    "\n",
    "            except:\n",
    "                print(i)\n",
    "                author_first_article = author_list[i].split(\", \")\n",
    "                author_next_article = author_list[i+1].split(\", \")\n",
    "                if current_article_author is None:\n",
    "                    print(\"first is wrong\")\n",
    "                    article = article[article.find(author_first_article[0]):next_article_author.span()[0]] \n",
    "                    print(len(article))\n",
    "                    \n",
    "                    \n",
    "                if next_article_author is None:\n",
    "                    print(\"second is wrong\") \n",
    "               \n",
    "                    #all instances of the title - need to find the one that is after the beginning of the \n",
    "                #current article\n",
    "                    \n",
    "                    #this was testing if I could find the next articles title instead and use that\n",
    "                    #but not all titles can be found in full\n",
    "                    #txt file has random page breaks\n",
    "                    #res = [i.start() for i in re.finditer(title_list[i+1], article)]\n",
    "                   #print(res)\n",
    "\n",
    "                    #correct_title_instance = next(x for x, val in enumerate(res)\n",
    "                                  #if val > current_article_author.span()[0])\n",
    "                        \n",
    "                    #will try to find next first author instead - with out a number or with a *\n",
    "                    \n",
    "                    #this now correctly cuts off at the beginning of the article, but cannot find the end point\n",
    "                    end_range = find_author(author_list, i+1, article, current_article_author.span()[0])\n",
    "                    article = article[current_article_author.span()[0]:end_range]\n",
    "                    #with the helper function finds nothing... YAY!\n",
    "                    print('start range')\n",
    "                    print(current_article_author.span()[0])\n",
    "                    print('end range')\n",
    "                    print(end_range)\n",
    "                    \n",
    "                    \n",
    "                    print('length of article')\n",
    "                    print(len(article))\n",
    "                    \n",
    "                if current_article_author is None and next_article_author is None:\n",
    "                    print(\"both are none\")\n",
    "                #adding this has meant that it does it correctly for those with numbers\n",
    "                #those without it just continues and writes the entire thing in the next line - so can add\n",
    "                #correct code here and that should mean titles and everything else still work\n",
    "        \n",
    "        with open('2012_'+str(i)+'.txt', 'w', encoding = 'utf-8') as f:\n",
    "            f.write(article)\n",
    "            \n",
    "        i = i+1\n",
    "        \n",
    "    \n",
    "    return article\n",
    "\n",
    "#returned 251 files but appear to be empty except for the first one which contains 6 lines\n",
    "#one issue is that these combined txt files contain the table of contents! Which is definitely the first thing this one finds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84f549af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to find the correct author (hopefully!)\n",
    "def find_author(author_list, index, article, start_range): \n",
    "    print(\"Entering the helper function\")\n",
    "    author = author_list[index].split(\", \")\n",
    "    print(author[0])\n",
    "    \n",
    "    #author with number\n",
    "    author_with_number = author[0] + \"[0-9]\"\n",
    "    author_found_num = re.findall(author_with_number, article) #changing them from search gives me issues\n",
    "    #author with *\n",
    "    star_author = author[0] + \"*\"\n",
    "    author_found_star = re.findall(star_author, article)\n",
    "    #first author with no added number or star\n",
    "    author_alone = author[0]\n",
    "    author_found_alone = re.findall(author_alone, article)\n",
    "    #author's first name, for those that are split stupidly \n",
    "    author_names = author_alone.split(\" \")\n",
    "    first_name_found = re.findall(author_names[-1], article)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if author_found_num is not None:\n",
    "        print(\"I found an author with number: \")\n",
    "        following_instance = next(x for x, val in enumerate(author_found_num)\n",
    "                                if val > start_range)\n",
    "        return author_found_num[following_instance]\n",
    "    \n",
    "    \n",
    "    elif author_found_star is not None:\n",
    "        print(\"I found an author with star\")\n",
    "        following_instance = next(x for x, val in enumerate(author_found_star)\n",
    "                                  if val > start_range)\n",
    "        return author_found_star[following_instance]\n",
    "\n",
    "\n",
    "    elif author_found_alone is not None:\n",
    "        print('I found the author alone')\n",
    "        following_instance = next(x for x, val in enumerate(author_found_alone)\n",
    "                                  if val > start_range)\n",
    "        return author_found_alone[following_instance]\n",
    "  \n",
    "    elif first_name_found is not None:\n",
    "        print(\"I found the last name\")\n",
    "        following_instance = next(x for x, val in enumerate(first_name_found)\n",
    "                                  if val > start_range)\n",
    "        return first_name_found[following_instance]\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        print(\"I found nothing\") #because even Benjamins name has been split oddly, and contains other signs?\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd9d3891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "second is wrong\n",
      "Entering the helper function\n",
      "BenjamÃ­n BÃ©jar Haro\n",
      "I found an author with number: \n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11220/993190809.py\u001b[0m in \u001b[0;36msave_articles_from_big_file\u001b[1;34m(place, title_list, author_list)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_article_author\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnext_article_author\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m#need to fix title placement!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'span'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11220/3826164889.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mattempt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_articles_from_big_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"miccai_2012_all_parts.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthor_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11220/993190809.py\u001b[0m in \u001b[0;36msave_articles_from_big_file\u001b[1;34m(place, title_list, author_list)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[1;31m#this now correctly cuts off at the beginning of the article, but cannot find the end point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                     \u001b[0mend_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_author\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthor_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_article_author\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                     \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_article_author\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_range\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                     \u001b[1;31m#with the helper function finds nothing... YAY!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11220/3641423677.py\u001b[0m in \u001b[0;36mfind_author\u001b[1;34m(author_list, index, article, start_range)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mauthor_found_num\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I found an author with number: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         following_instance = next(x for x, val in enumerate(author_found_num)\n\u001b[0m\u001b[0;32m     25\u001b[0m                                 if val > start_range)\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mauthor_found_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfollowing_instance\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "attempt = save_articles_from_big_file(\"miccai_2012_all_parts.txt\", title_list, author_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04aa34a",
   "metadata": {},
   "source": [
    "The article page numbers flip flop between\n",
    "page number then author (in short version - first author et al.)\n",
    "title (shorter version as well) then page number\n",
    "These have line breaks between them in the txt file\n",
    "\n",
    "If I just add in looking for title then authors (full names) I'm not sure I'll find the start of the articles, since they have numbers often denoting university affiliations - and my author list comes from the table of contents so doesn't have this.\n",
    "\n",
    "But they all seem to have it - so searching for the title plus author list with added numbers to all names should find beginning of each article?\n",
    "this should then work to deliniate all the articles? because if I go to beginning of next article its the same??\n",
    "\n",
    "Yikes, think I should start over and try from the beginning with titles - waaaay too complex right now\n",
    "\n",
    "Titles give me issues because of newlines in the txt version, so the full titles don't match where I expect them to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7c2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes the separate txt files of the part of the proceedings and adds them together, saving to a combined file\n",
    "def combine_txt(list_of_txt, title):\n",
    "    file = open(list_of_txt[0], \"r\",encoding='utf-8')\n",
    "    part = file.read() \n",
    "    \n",
    "    for i in range(len(list_of_txt)-1):\n",
    "        file = open(list_of_txt[i+1], \"r\",encoding='utf-8')\n",
    "        next_part = file.read() \n",
    "        part = part + \"\\n\" + next_part #adding each part together\n",
    "        i = i+1\n",
    "\n",
    "    with open(title + \".txt\", 'w', encoding='utf-8') as fp:\n",
    "            fp.write(part) #writing the whole thing to a file\n",
    "       \n",
    "    return part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb055694",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_txt=['miccai 2012 part 1.txt', 'miccai 2012 part 2.txt','miccai 2012 part 3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "329a8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = combine_txt(list_of_txt, \"miccai_2012_all_parts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6b6b5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_abstract(place):\n",
    "        with open(place, \"r\",encoding='utf-8') as part:\n",
    "            article = part.read()           \n",
    "            abstracts = [i.start() for i in re.finditer(\"Abstract\", article)]\n",
    "            introductions = [i.start() for i in re.finditer(\"Introduction\", article)]\n",
    "            #this finds 253 instances of \"Abstract\" - the word must appear some other places\n",
    "            #also finds 500 instances of introduction, so they may not line up precisely\n",
    "            #but perhaps reasonable to assume that if the first two digits match, and the difference is around 500-2000 \n",
    "            #characters, I have found the right spot?\n",
    "            \n",
    "        return abstracts, introductions             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e30b007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts, introductions = find_abstract(\"miccai_2012_all_parts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d95907ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(introductions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d27acb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "311514b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97849, 119206, 140413, 162878, 183007, 206404, 227986, 250810, 273759, 292718, 315578, 336100, 356129, 375030, 399054, 420164, 442254, 465656, 487615, 506433]\n"
     ]
    }
   ],
   "source": [
    "print(abstracts[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "d634aff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99297, 120216, 141361, 163887, 184221, 207258, 229355, 252131, 275114, 293515, 316947, 337331, 357061, 376228, 399991, 443309, 467250, 488623, 507555, 528857]\n"
     ]
    }
   ],
   "source": [
    "print(introductions[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "be5f070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448\n",
      "1010\n",
      "948\n",
      "1009\n",
      "1214\n",
      "854\n",
      "1369\n",
      "1321\n",
      "1355\n",
      "797\n",
      "1369\n",
      "1231\n",
      "932\n",
      "1198\n",
      "937\n",
      "1055\n",
      "1594\n",
      "1008\n",
      "1122\n",
      "908\n",
      "985\n",
      "976\n",
      "946\n",
      "832\n",
      "1008\n",
      "1028\n",
      "903\n",
      "1454\n",
      "1166\n",
      "1312\n",
      "873\n",
      "1222\n",
      "972\n",
      "1145\n",
      "926\n",
      "773\n",
      "1054\n",
      "958\n",
      "802\n",
      "1105\n",
      "624\n",
      "778\n",
      "697\n",
      "24854\n",
      "21181\n",
      "21872\n",
      "23734\n",
      "848\n",
      "20142\n",
      "23994\n",
      "22179\n",
      "22748\n",
      "3699\n",
      "24475\n",
      "43457\n",
      "42056\n",
      "24451\n",
      "21970\n",
      "44320\n",
      "45448\n",
      "46693\n",
      "20740\n",
      "22150\n",
      "43831\n",
      "49626\n",
      "23644\n",
      "20730\n",
      "43608\n",
      "41431\n",
      "38982\n",
      "23142\n",
      "18211\n",
      "39704\n",
      "44634\n",
      "45254\n",
      "22857\n",
      "22420\n",
      "42911\n",
      "42902\n",
      "23967\n",
      "20541\n",
      "41669\n",
      "43177\n",
      "44156\n",
      "23690\n",
      "21552\n",
      "43786\n",
      "63924\n",
      "39754\n",
      "68030\n",
      "47619\n",
      "47883\n",
      "-102861\n",
      "-125104\n",
      "-144629\n",
      "-164582\n",
      "-185800\n",
      "-204153\n",
      "-228522\n",
      "-250407\n",
      "-272842\n",
      "-293330\n",
      "-318100\n",
      "-341090\n",
      "-362209\n",
      "-381028\n",
      "-401042\n",
      "-419604\n",
      "-440965\n",
      "-461933\n",
      "-489681\n",
      "-512178\n",
      "-533462\n",
      "-553397\n",
      "-573558\n",
      "-594095\n",
      "-614233\n",
      "-635581\n",
      "-657571\n",
      "-677855\n",
      "-701676\n",
      "-723607\n",
      "-744823\n",
      "-766054\n",
      "-790331\n",
      "-810302\n",
      "-829646\n",
      "-850730\n",
      "-873425\n",
      "-894394\n",
      "-917339\n",
      "-940996\n",
      "-962251\n",
      "-983475\n",
      "-1005874\n",
      "-1026725\n",
      "-1047701\n",
      "-1068023\n",
      "-1089525\n",
      "-1106786\n",
      "-1130751\n",
      "-1151509\n",
      "-1171859\n",
      "-1193268\n",
      "-1214572\n",
      "-1238252\n",
      "-1258280\n",
      "-1280092\n",
      "-1298942\n",
      "-1321751\n",
      "-1340935\n",
      "-1363628\n",
      "-1386318\n",
      "-1409148\n",
      "-1428845\n",
      "-1449077\n",
      "-1468094\n",
      "-1489583\n",
      "-1511213\n",
      "-1534269\n",
      "-1556065\n",
      "-1576907\n",
      "-1597145\n",
      "-1618267\n",
      "-1641669\n",
      "-1663118\n",
      "-1683906\n",
      "-1700892\n",
      "-1725428\n",
      "-1748775\n",
      "-1770488\n",
      "-1793029\n",
      "-1815368\n",
      "-1941225\n",
      "-1962076\n",
      "-1985197\n",
      "-2002807\n",
      "-2024646\n",
      "-1987105\n",
      "-1987095\n",
      "-1986954\n",
      "-1988475\n",
      "-1989893\n",
      "-1992611\n",
      "-1994934\n",
      "-1992228\n",
      "-1989213\n",
      "-1989291\n",
      "-1991795\n",
      "-1987539\n",
      "-1988884\n",
      "-2007716\n",
      "-2009091\n",
      "-2010802\n",
      "-2012917\n",
      "-2016095\n",
      "-2018220\n",
      "-2017879\n",
      "-2010854\n",
      "-2010608\n",
      "-2010711\n",
      "-2012773\n",
      "-2013554\n",
      "-2014949\n",
      "-2019306\n",
      "-2018956\n",
      "-2019128\n",
      "-2018864\n",
      "-2014778\n",
      "-2015468\n",
      "-2015286\n",
      "-2017130\n",
      "-2013072\n",
      "-2015231\n",
      "-2019523\n",
      "-2018694\n",
      "-2016815\n",
      "-2017844\n",
      "-2017598\n",
      "-2016125\n",
      "-2015853\n",
      "-2013086\n",
      "-2014140\n",
      "-2012614\n",
      "-2012288\n",
      "-2012608\n",
      "-2012161\n",
      "-2017766\n",
      "-2016518\n",
      "-2017232\n",
      "-2018120\n",
      "-2018510\n",
      "-2018179\n",
      "-2013015\n",
      "-2014537\n",
      "-2012184\n",
      "-2012840\n",
      "-2010296\n",
      "-2006968\n",
      "-2007447\n",
      "-2005715\n",
      "-2004729\n",
      "-2004646\n",
      "-2005147\n",
      "-2011302\n",
      "-2009685\n",
      "-2012156\n",
      "-2007327\n",
      "-2006098\n",
      "-2004280\n",
      "-2004623\n",
      "-2004598\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(abstracts)-1):\n",
    "    if str(abstracts[i])[:2] == str(introductions[i])[:2]: #if the first two digits match\n",
    "        print(str(introductions[i]-abstracts[i]))\n",
    "    elif introductions[i]-abstracts[i] < 2000 and introductions[i]-abstracts[i] > 500: #if the difference matches expected size\n",
    "        print(str(introductions[i]-abstracts[i]))\n",
    "    else: \n",
    "        print(str(introductions[i]-abstracts[i+1])) #move to next abstract - could also be next introduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "f65c171f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reliable Assessment of Perfusivity and Diffusivity from Diffusion Imaging of the Body',\n",
       " 'Multi-organ Abdominal CT Segmentation Using Hierarchically Weighted Subject-Specific Atlases',\n",
       " 'Radiation-Free Drill Guidance in Interlocking of Intramedullary Nails',\n",
       " 'Developing Essential Rigid-Flexible Outer Sheath to Enable Novel Multi-piercing Surgery',\n",
       " 'Surgical Gesture Classification from Video Data',\n",
       " 'Remote Ultrasound Palpation for Robotic Interventions Using Absolute Elastography',\n",
       " 'Modeling and Real-Time Simulation of a Vascularized Liver Tissue',\n",
       " 'Efficient Optic Cup Detection from Intra-image Learning with Retinal Structure Priors',\n",
       " 'Population-Based Design of Mandibular Plates Based on Bone Quality and Morphology',\n",
       " 'Thoracic Abnormality Detection with Data Adaptive Structure Estimation',\n",
       " 'Domain Transfer Learning for MCI Conversion Prediction',\n",
       " 'Simulation of Pneumoperitoneum for Laparoscopic Surgery Planning',\n",
       " 'Incremental Kernel Ridge Regression for the Prediction of Soft Tissue Deformations',\n",
       " 'Fuzzy Multi-class Statistical Modeling for Efficient Total Lesion Metabolic Activity Estimation from Realistic PET Images',\n",
       " 'Structure and Context in Prostatic Gland Segmentation and Classification',\n",
       " 'Quantitative Characterization of Trabecular Bone Micro-architecture Using Tensor Scale and Multi-Detector CT Imaging',\n",
       " 'Genetic, Structural and Functional Imaging Biomarkers for Early Detection of Conversion from MCI to AD',\n",
       " 'Robust MR Spine Detection Using Hierarchical Learning and Local Articulated Model',\n",
       " 'Spatiotemporal Reconstruction of the Breathing Function',\n",
       " 'A Visual Latent Semantic Approach for Automatic Analysis and Interpretation of Anaplastic Medulloblastoma Virtual Slides']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218963c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
