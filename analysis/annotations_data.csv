Tidsstempel,What is the article's index?,Which year is the article from?,Is the article accurately labelled as classification?,"If not accurately labelled as classification, what would you label it as?",Please input the quote from which you infer the answer to the previous question (if possible),What is the aim or task of the article? (input quote),How does the article justify this aim or task? (input quote),Which method is used for classification?,Which performance measures are used? ,Does the article use segmentation as preprocessing?,Please input the quote from which you infer the answer to the previous question (if possible),Does the dataset used in the article have a title?,Please input the quote from which you infer the answer to the previous question (if possible),What is the size of the used dataset? (input quote),What type is the dataset?,Please input the quote from which you infer the answer to the previous question (if possible),Is the survey/method of how the dataset was obtained accessible?,Please input the quote from which you infer the answer to the previous question (if possible),Does the article mention the demographics of the patients/images included in the used dataset?,Please input the quote from which you infer the answer to the previous question (if possible),Does the article mention the intent for collecting the dataset? The intended task for the dataset?,Please input the quote from which you infer the answer to the previous question (if possible),Does the article disclose any affiliations?,Please input the quote from which you infer the answer to the previous question (if possible),"Does the article include anything about respect for persons (informed consent, voluntary participation) participating in the dataset? ",Please input the quote from which you infer the answer to the previous question (if possible),"Does the article have any mention of benefience, minimising risk/maximising benefit of work? ",Please input the quote from which you infer the answer to the previous question (if  possible),"Does the article have any mention of justice (equal treatment, fair selection of subjects)?",Please input the quote from which you infer the answer to the previous question (if  possible),"Does the article mention any respect for law/public interest (transparency in methods/results, accountability for actions)?",Please input the quote from which you infer the answer to the previous question (if  possible),Are there any other comments/interesting aspects?
22/09/2022 13.28.08,4,2012,Yes,It was accurately labelled,we propose and evaluate three approaches to surgical gesture classification from video.,surgical gesture analysis,"Most of the prior work on surgical gesture recognition (see, e.g., [4-6]) uses hidden Markov models (HMMs) to analyze kinematic data stored by the robot (…) Overall, our main conclusion is that methods based on video data perform equally well as methods based n kinematic data for a typical surgical training setup.","""In the first one, we model each video clip from each surgical gesture as the output of a linear dynamical system (LDS) and use metrics in the space of LDSs to classify new video clips. In the second one, we use spatio-temporal features extracted from each video clip to learn a dictionary of spatio-temporal words and use a bag-of-features (BoF) approach to classify new video clips. In the third approach, we use multiple kernel learning to combine the LDS and BoF approaches.""",Accuracy,No,"There is no mention of segmentation, but the article does write: ""We assume that each video is segmented into video surgemes"", so the segmentation has been done",Yes,"""For our tests we used the California dataset [3].""","""The dataset consists of three different tasks: suturing (SU, 39 trials), needle passing (NP, 26 trials) and knot tying (KT, 36 trials). Each task is performed by 8 surgeons with different skill levels.""",Private,"There is no mention of a public dataset, and the article writes ""The authors thank Intuitive Surgical and Carol Reiley for providing the dataset""",No,searching for the dataset yields no results,No,"no mention of demographics, though the article does specify that ""Each task is performed by 8 surgeons with different skill levels""",No,the article contains no mention of intent of data collection,Yes,"This work was funded by NSF grants 0931805 and 0941362, and by the Talentia Fellowships Programme of the Andalusian Regional Ministry of Economy, Innovation and Science",No, the article contains no information about the surgeons in the dataset or the patients they were presumably operating on,No,"the focus of the article is to promote a new method, mode of analysis, and does not mention any risk or benefit other than the usefulness of this new method",No,there is no mention of this,No,there is no mention of this,
22/09/2022 13.43.26,7,2012,Yes,It was accurately labelled,"Second, the classifier learning process does
not rely on pre-labeled training samples, but rather the training samples are extracted
from the test image itself using structural priors on relative cup and disc
positions. Third, we present a classification refinement scheme that utilizes both
structural priors and local context.",We present a superpixel based learning framework based on retinal structure priors for glaucoma diagnosis.,It is critical to detect this degeneration of the optic nerve as early as possible in order to stall its progression,SVM,"we use the same three evaluation criteria as in [1] to measure cup detection accuracy, namely non-overlap ratio (m1), relative absolute area difference (m2) and absolute CDR error (δ),",Yes,"In this work, we utilize the stateof-the-art SLIC (Simple Linear Iterative Clustering) algorithm [12] to segment the fundus disc image into compact and nearly uniform superpixels.",Yes,using a large clinical dataset called ORIGA−light,"For testing we use the ORIGA−light dataset, comprised of 168 glaucoma and 482 normal images.",Public,"Is searchable, available upon request",No,"But the dataset is available upon request, so perhaps this information is also",No,only size is mentioned,No,no mention of intent in article,Yes,This work is funded by Singapore A*STAR SERC Grant (092-148-00731),No,article contains no mention of patients ,No,"no direct mention, simply mentions the method proposed gets a level of ""accuracy is comparable to or higher than the state-of-the-art technique [1], with a speedup factor of tens or hundreds.""",No,no mention of patients,Don't know,"no mention of law/transparency - though they do mention ""The settings in [1] are also adopted in this work to facilitate comparisons"", so based on what that says, maybe",
26/09/2022 09.18.32,9,2012,Yes,It was accurately labelled,"Automatic detection of lung tumors and abnormal lymph
nodes are useful in assisting lung cancer staging. This paper presents a
novel detection method, by first identifying all abnormalities, then differentiating
between lung tumors and abnormal lymph nodes based on their
degree of overlap with the lung field and mediastinum.","our aim of this study is to develop a computerized method to detect
the lung tumors and abnormal lymph nodes from PET-CT thoracic images automatically.","In this work, we propose a new and intuitive idea to the detection problem –
after attempting to detect all abnormalities, if we can identify the actual lung
field (tumors inclusive), then we can differentiate lung tumors and abnormal
lymph nodes based on the degree of overlap between the detected abnormality
and the lung field.","Graph analysis, Regression-based appearance model and graph-based structure labeling are designed to estimate the actual lung field and mediastinum from the pathology-affected thoracic images adaptively.","Accuracy, Precision, Recall",No,"The PET-CT thoracic images are first preprocessed to remove the background
and soft tissues outside of the lung and mediastinum with morphological operations.
All images are then aligned based on the carina of tracheae, and rescaled
to the same size [4]. Next, the abnormalities are detected by classification of
lung field (L), mediastinum (M) or abnormalities (O) (Fig. 1c), based on PET
uptake values and CT densities.",No,"The experiment is performed on 50 sets of 3D PET-CT thoracic
images from patients with non-small cell lung cancer (NSCLC), provided by
the Royal Prince Alfred Hospital, Sydney.","A total of 54 lung tumors and 35
abnormal lymph nodes are annotated as the ground truth. For each data set,
the contour of lung field is also roughly delineated. Five images representing
the typical cases are selected manually as the training set for both structure
labeling and classification between tumors and lymph nodes. The data sets are
then randomly divided into five sets; and within each set, each image is used as
the testing image, with the other nine as the reference images.",Private,"The experiment is performed on 50 sets of 3D PET-CT thoracic
images from patients with non-small cell lung cancer (NSCLC), provided by
the Royal Prince Alfred Hospital, Sydney.",No,"The experiment is performed on 50 sets of 3D PET-CT thoracic
images from patients with non-small cell lung cancer (NSCLC), provided by
the Royal Prince Alfred Hospital, Sydney.",No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,"They only mention: Lung cancer is currently the leading cause of cancer deaths; and staging plays
a critical role in defining the prognosis and the best treatment approaches",No,Nothing is mentioned on the choice of subjects,No,"There is a lengthy theory section, but no mention of accessing the code or the dataset",
26/09/2022 09.25.02,10,2012,Yes,It was accurately labelled,"In this paper, we propose a novel domain-transfer learning method
for MCI conversion prediction. Different from most existing methods, we
classify MCI-C and MCI-NC with aid from the domain knowledge learned with
AD and NC subjects as auxiliary domain to further improve the classification
performance.","Different from most existing methods, we
classify MCI-C and MCI-NC with aid from the domain knowledge learned with
AD and NC subjects as auxiliary domain to further improve the classification
performance.","Alzheimer’s disease (AD) is the most common form of dementia in elderly people
worldwide. Early diagnosis of AD is very important for possible delay of the disease.
Mild cognitive impairment (MCI) is a prodromal stage of AD, which can be further
categorized into MCI converters (MCI-C) and MCI non-converters (MCI-NC). The
former will convert into AD in follow-up time, while the latter will not convert. Thus,
accurate diagnosis of MCI converters is of great importance.",SVM,"AUC, Specificity, Accuracy, Sensitivity",Yes,"Then, we use the FSL package to segment
each structural MR image into three different tissue types: gray matter (GM), white
matter (WM), and cerebrospinal fluid (CSF).",Yes,"we evaluate the effectiveness of our proposed DTSVM method on
multimodal data, including MRI, PET and CSF, from the AlzheimerÊs disease
Neuroimaging Initiative (ADNI) database.","the baseline ADNI subjects with all corresponding MRI, PET,
and CSF data are included, which leads to a total of 202 subjects (including 51 AD
patients, 99 MCI patients, and 52 normal controls (NC)). For 99 MCI patients, it
includes 43 MCI converters and 56 MCI non-converters. We use 51 AD and 52 NC
subjects as auxiliary domains, and 99 MCI subjects as target domains.",Public,Googling ADNI leads to a webpage where the dataset is available upon request for research,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,Yes,"This work was partially supported by NIH grants (EB006733,
EB008374, EB009634, AG041721 and MH088520), NSFC grant (60875030), and
CQKJ (KJ121111).",No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,No,Nothing is mentioned in the article,
26/09/2022 09.33.42,12,2012,No,Other medical imaging task,"It does not use the same performance measures, and aims to predict the outcome of facial deformation post surgery, so not really a diagnosis or a clear classification problem either.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 09.45.32,13,2012,Yes,It was accurately labelled,"We
utilize a fuzzy multi-class modeling using a stochastic expectation maximization
(SEM) algorithm to fit a finite mixture model (FMM) to the
PET image. We then propose a direct estimation formula for TLA and
SUVmean from this multi-class statistical model.","We
utilize a fuzzy multi-class modeling using a stochastic expectation maximization
(SEM) algorithm to fit a finite mixture model (FMM) to the
PET image. We then propose a direct estimation formula for TLA and
SUVmean from this multi-class statistical model.","The aforementioned functional markers computed from the PET image are
corrupted by partial volume effects and acquisition blur. Nonetheless, we recently
proposed a direct statistical estimation method, statistical lesion activity
computation (SLAC), in [6] for computing TLA, in the presence of blur.","Unsupervised learning, The main contributions of this paper include a fuzzy multi-class statistical modeling of the PET data with fuzzy mixing of all ‘hard’ classes involved in the model (in order to handle lesion as well as background heterogeneities) along with a direct estimation formula for TLA and SUVmean from the statistical model parameters.","Accuracy, At least I think it is accuracy, they speak of ground truth and deviating from that I believe",No,Not mentioned in the article,No,"To evaluate the performance, an NCAT phantom [11] with a hot liver lesion was
simulated.","To evaluate the performance, an NCAT phantom [11] with a hot liver lesion was
simulated. Realistic FDG uptake values were assigned to the various organs and
tissues of the NCAT phantom. A non spherical tumor (27.67mL) was inserted in
the liver (see Fig. 1). In the tumor, the activity was set to 18.2kBq/cc. The activity
in the liver, spleen, lungs and body was 6.3kBq/cc, 5.5kBq/cc, 0.9kBq/cc
and 2.5kBq/cc respectively. The voxel size used to generate the phantom was
1mm×1mm×1mm. 30 3min scans of the NCAT phantom were simulated using
a Monte Carlo simulator (PET-SORTEO [10]) which models among others the
spatially variant point spread function (PSF) of the ECAT Exact HR+ scanner.
Attenuation and scatter were also modeled. During reconstruction of both
datasets, the system PSF resolution was recovered by modeling as an isotropic
Gaussian with 5mm FWHM. The projection data were reconstructed using the
maximum likelihood expectation maximization (MLEM) algorithm [9] with ordered
subsets. As in clinical routine, 4 iterations over 16 subsets were performed.
The reconstruction voxel size was set to 2mm× 2mm× 2mm. The images were
post-smoothed with 5mm Gaussian FWHM.",Private,It is simulated data generated for this article of a liver lesion with a tumor inserted,Yes,"To evaluate the performance, an NCAT phantom [11] with a hot liver lesion was
simulated. Realistic FDG uptake values were assigned to the various organs and
tissues of the NCAT phantom. A non spherical tumor (27.67mL) was inserted in
the liver (see Fig. 1). In the tumor, the activity was set to 18.2kBq/cc. The activity
in the liver, spleen, lungs and body was 6.3kBq/cc, 5.5kBq/cc, 0.9kBq/cc
and 2.5kBq/cc respectively. The voxel size used to generate the phantom was
1mm×1mm×1mm. 30 3min scans of the NCAT phantom were simulated using
a Monte Carlo simulator (PET-SORTEO [10]) which models among others the
spatially variant point spread function (PSF) of the ECAT Exact HR+ scanner.
Attenuation and scatter were also modeled. During reconstruction of both
datasets, the system PSF resolution was recovered by modeling as an isotropic
Gaussian with 5mm FWHM. The projection data were reconstructed using the
maximum likelihood expectation maximization (MLEM) algorithm [9] with ordered
subsets. As in clinical routine, 4 iterations over 16 subsets were performed.
The reconstruction voxel size was set to 2mm× 2mm× 2mm. The images were
post-smoothed with 5mm Gaussian FWHM.",No,No patients were included as the data was simulated,Yes,It was created for this article,Yes,"The authors gratefully acknowledge the financial support
by KU Leuven’s Concerted Research Action GOA/11/006, IWT - TBM project
070717 and Research Foundation - Flanders (FWO).",No,Because no persons were involved,No,No persons involved,No,No persons involved,No,No persons involved but a very long perhaps useful theory section if one can recreate it from that?,
26/09/2022 10.18.37,14,2012,Yes,It was accurately labelled,"A novel gland segmentation and classification scheme applied
to an H&E histology image of the prostate tissue is proposed. For
gland segmentation, we associate appropriate nuclei objects with each
lumen object to create a gland segment. We further extract 22 features
to describe the structural information and contextual information for
each segment. These features are used to classify a gland segment into
one of the three classes: artifact, normal gland and cancer gland.","A novel gland segmentation and classification scheme applied
to an H&E histology image of the prostate tissue is proposed.","In detecting prostate cancer on a digitized tissue slide, the pathologist relies
on: (i) structural information; glands in a cancer region (cancer glands) appear
to have structural properties (e.g. nuclei abundance, lumen size) different from
glands in a normal region (normal glands) and (ii) contextual information; cancer
glands typically cluster into groups and are of similar shape and size1, while
shape and size of normal glands vary widely. These two sources of information
can be observed in Fig. 1b. Hence, a reasonable approach to assist a pathologist
in finding cancer regions includes segmenting out glandular regions, examining
their structural and contextual information and finally classifying them.",SVM,Accuracy,Yes,"For
gland segmentation, we associate appropriate nuclei objects with each
lumen object to create a gland segment.",No,"The dataset includes 48 images at 5× magnification (average image
size is 900 × 1,500 pixels), which come from 20 patients. Glands in images of
the same patient still have very large variability in structures. Given the pathologist’s
annotation on each image, we manually label 525 artifacts, 931 normal
glands and 1,375 cancer glands to form the (ground truth) gland dataset.","The dataset includes 48 images at 5× magnification (average image
size is 900 × 1,500 pixels), which come from 20 patients. Glands in images of
the same patient still have very large variability in structures. Given the pathologist’s
annotation on each image, we manually label 525 artifacts, 931 normal
glands and 1,375 cancer glands to form the (ground truth) gland dataset.",Private,The article contains no mention of where the dataset comes from,No,No mention in article,No,No mention in article,No,No mention in article,No,No mention in article,No,No mention in article,No,No mention in article,No,No mention in article,Yes,"The article includes a link to a github repo, containing at least some of the code
The article also includes stats for running time and details about the machine it has been run on",
26/09/2022 10.25.31,8,2021,No,Segmentation,Consistent Segmentation of Longitudinal Brain MR Images with Spatio-Temporal Constrained Networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 10.27.16,17,2021,No,Segmentation,Improved Brain Lesion Segmentation with Anatomical Priors from Healthy Subjects,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 10.28.27,18,2021,No,Segmentation,CarveMix: A Simple Data Augmentation Method for Brain Lesion Segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 10.30.02,30,2021,No,,Partially-Supervised Learning for Vessel Segmentation in Ocular Images,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26/09/2022 10.31.32,30,2021,No,Segmentation,Multi-compound Transformer for Accurate Biomedical Image Segmentation,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/09/2022 11.14.02,255,2021,Yes,It was accurately labelled,"In the early diagnosis of lung cancer, an important step is classifying malignancy/benignity for each lung nodule.","Recently, the contextual features attract increasing attention, due to the complementary information they provide. Clinically, such contextual features refer to the features of nodule’s surrounding structures, such that (together with nodule’s features) they can expose discriminate patterns for the malignant/benign, such as vascular convergence and fissural attachment. To leverage such contextual features, we propose a Context Attention Network (CA-Net) which extracts both nodule’s and contextual features and then effectively fuses them during malignancy/benignity classification. To accurately identify the contextual features that contain structures distorted/attached by the nodule, we take the nodule’s features as a reference via an attention mechanism. Further, we propose a feature fusion module that can adaptively adjust the weights of nodule’s and contextual features across nodules.","Recently, the contextual features attract increasing attention, due to the complementary information they provide. Clinically, such contextual features refer to the features of nodule’s surrounding structures, such that (together with nodule’s features) they can expose discriminate patterns for the malignant/benign, such as vascular convergence and fissural attachment. To leverage such contextual features, we propose a Context Attention Network (CA-Net) which extracts both nodule’s and contextual features and then effectively fuses them during malignancy/benignity classification",Neural network,"AUC, Accuracy, Log Loss",Yes,"The whole pipeline of our method, namely Context Attention Network (CA-Net), is illustrated in Fig. 2. As shown, it is the sequential of three stages: (i) Nodule Detection that detects all nodules from the CT image;",Yes,DSB2017 dataset,"There are 1397, 198, and 506 patients in the training, validation, and test set, respectively.",Public,"Experimentally, our CA-Net outperforms the 1st place by a noticeable margin on Kaggle DSB2017 dataset.",No,Nothing is mentioned in the article,No,This is the only description in the article: This dataset provides pathologically proven lung cancer label for each patient.,No,Nothing is mentioned in the article,Yes,"This work was supported by MOST-2018AAA0102004, NSFC-61625201, and the Beijing Municipal Science and Technology Planning Project (Grant No. Z201100005620008).",No,Nothing is mentioned in the article,Yes,"Experimentally, our CA-Net outperforms the 1st place by a noticeable margin on Kaggle DSB2017 dataset.",No,Nothing is mentioned in the article,Yes,"To some degree perhaps - though who knows what is actually in this supplementary material?
Also no mention is made of the repo/code for reproducibility... 
Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​3) contains supplementary material, which is available to authorized users.",
27/09/2022 11.21.06,257,2021,Yes,It was accurately labelled,"Learning disease-related representations plays a critical role in image-based cancer diagnosis, due to its trustworthy, interpretable and good generalization power.","Learning disease-related representations plays a critical role in image-based cancer diagnosis, due to its trustworthy, interpretable and good generalization power. A good representation should not only be disentangled from the disease-irrelevant features, but also incorporate the information of lesion’s attributes (e.g., shape, margin) that are often identified first during cancer diagnosis clinically. To learn such a representation, we propose a Disentangle Auto-Encoder with Graph Convolutional Network (DAE-GCN), which adopts a disentangling mechanism with the guidance of a GCN model in the AE-based framework. ","For better representation learning, the disentanglement mechanism has been proved to be an effective way [1, 3, 12], since such a mechanism prompts different independent latent units to encode different independent ground truth generation factors that vary in the data [1]. Based on the above, to capture the disease-related features without mixing other irrelevant information, in this paper we propose a Disentangle Auto-Encoder with Graph Convolutional Network (DAE-GCN), which incorporates a disentangling mechanism into an AE framework, equipped with attribution data during training stage (the attributes are not provided during the test). ","Graph analysis, Supervised learning, Neural network","AUC, Accuracy",No,"Our dataset contains $$\{x_i,A_i,y_i\}_{i \in \{1,...,n\}}$$, in which x, A, y respectively denote the patch-level mass image, attributes (e.g., circumscribed-margin, round-shape, irregular-shape), and the binary disease label. ",Yes," DDSM 
But also uses 3 ""in house"" datasets","We consider both the public dataset DDSM [2] and three in-house datasets: Inhouse1, Inhouse2 and Inhouse3. For each dataset, the region of interests (ROIs) (malignant/benign masses) are cropped based on the annotations of radiologists the same as [9]1. For all datasets, we randomly2 divide the whole set into training, validation and testing as 8:1:1 in patient-wise. ",Don't know,"We consider both the public dataset DDSM [2] and three in-house datasets: Inhouse1, Inhouse2 and Inhouse3.",No,"To evaluate the effectiveness of our DAE-GCN, we verify it on the patch-level mammogram mass benign/malignant classification. We consider both the public dataset DDSM [2] and three in-house datasets: Inhouse1, Inhouse2 and Inhouse3. For each dataset, the region of interests (ROIs) (malignant/benign masses) are cropped based on the annotations of radiologists the same as [9]1. For all datasets, we randomly2 divide the whole set into training, validation and testing as 8:1:1 in patient-wise.",No,Nothing is mentioend,No,Nothing is mentioned,Yes,"This work was supported by MOST-2018AAA0102004, NSFC-61625201 and ZheJiang Province Key Research & Development Program (No. 2020C03073).",No,Nothing is mentioned,Yes,"From the introduction: 
For image-based disease benign/malignant diagnosis, it is crucial to learn the disease-related representation for prediction, due to the necessity of trustworthy (to patients), explainable (to clinicians) and good generalization ability in healthcare.",No,Nothing is mentioned,Yes,"To provide convenience for latter works, we publish our spitted test set of DDSM [2] in supplementary.",
27/09/2022 11.27.05,258,2021,Yes,It was accurately labelled,"Ultrasound (US) imaging is a fundamental modality for detecting and diagnosing breast lesions, while shear-wave elastography (SWE) serves as a crucial complementary counterpart. Although an automated breast lesion classification system is desired, training of such a system is constrained by data scarcity and modality imbalance problems due to the lack of SWE devices in rural hospitals. ","To enhance the diagnosis with only US available, in this work, we propose a knowledge-guided data augmentation framework, which consists of a modal translater and a semantic inverter, achieving cross-modal and semantic data augmentation simultaneously. ","Breast cancer, the most commonly diagnosed cancer, is the fifth leading cause of cancer death all over the world [24].",Neural network,"AUC, Accuracy, Precision, Recall, F1 score",No,Nothing is mentioned,No,"From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. ","From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. ",Private,"From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. ",Yes,"The Super Linear SL-15-4 probe of ultrafast ultrasound device Aixplorer (Super Sonic Imagine, Aix-en-Provence, France) was used for imaging data collection. The maximum stiffness scale of SWE images was selected as 180 Kilopascal (kPa). ",No,Nothing is mentioned,Yes,"From September 2020 to January 2021, a total of 2,008 images of benign lesions and 1,466 images of malignant lesions from 593 patients were collected as the dataset used in this paper. ",Yes,"This work was supported in part by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), 111 project (BP0719010), Shanghai Science and Technology Committee (18DZ2270700) and Shanghai Jiao Tong University Science and Technology Innovation Special Fund (ZH2018ZDA17).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​6) contains supplementary material, which is available to authorized users.",
27/09/2022 11.29.27,259,2021,No,Other medical imaging task,"Transfer learning is an important step to extract meaningful features and overcome the data limitation in the medical Visual Question Answering (VQA) task. However, most of the existing medical VQA methods rely on external data for transfer learning, while the meta-data within the dataset is not fully utilized. In this paper, we present a new multiple meta-model quantifying method that effectively learns meta-annotation and leverages meaningful features to the medical VQA task. Our proposed method is designed to increase meta-data by auto-annotation, deal with noisy labels, and output meta-models which provide robust features for medical VQA tasks. Extensively experimental results on two public medical VQA datasets show that our approach achieves superior accuracy in comparison with other state-of-the-art methods, while does not require external data to train meta-models.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
27/09/2022 11.37.14,262,2021,Yes,It was accurately labelled,"In this paper, we propose a coherent cooperative learning framework based on transfer learning for unsupervised cross-domain classification.","In this paper, we propose a coherent cooperative learning framework based on transfer learning for unsupervised cross-domain classification.","In the practical application of medical image analysis, due to the different data distributions of source domain and target domain and the lack of the labels of target domain, domain adaptation for unsupervised cross-domain classification attracts widespread attention. However, current methods take knowledge transfer model and classification model as two separate training stages, which inadequately considers and utilizes the intrinsic information interaction between module","Unsupervised learning, Transfer learning, Neural network","Accuracy, Precision, Recall, F1 score",No,Nothing is mentioned,Yes,"We use three databases in the experiments, and their information is shown in Table 1. The Chest X-Ray1 is divided into training dataset and testing dataset [12]. Single lesion2 and Multiple lesions3 are the training datasets of two open lesion recognition competitions.","No quote, but there is Table 1 which includes the number of images of each used dataset",Public,"The Chest X-Ray1 is divided into training dataset and testing dataset [12]. Single lesion2 and Multiple lesions3 are the training datasets of two open lesion recognition competitions. (and I think the first one is also accessible, there is a link in the footnote)",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,No,"This work was supported in part by 2030 National Key Research and Development Program of China (2018AAA0100500), the National Nature Science Foundation of China (no. 61773166), Projects of International Cooperation of Shanghai Municipal Science and Technology Committee (14DZ2260800), the Fundamental Research Funds for the Central Universities, and the ECNU Academic Innovation Promotion Program for Excellent Doctoral Students (YBNLTS2021-040).",No,Nothing is mentioned,No,Nothing is mentioned,No,Nothing is mentioned,Yes,"Electronic supplementary material
The online version of this chapter (https://​doi.​org/​10.​1007/​978-3-030-87240-3_​10) contains supplementary material, which is available to authorized users.",